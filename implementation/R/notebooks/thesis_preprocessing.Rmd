---
title: "Thesis: Preprocessing"
output: html_notebook
---

# 0. Load Libraries and Corpus Files 
### Prerequisites
To open and use this notebook file you need to install the following:
```{r message=FALSE, warning=FALSE}
# intall necessary packages
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("tidyverse")
#install.packages("quanteda.textstats")
#install.packages("quanteda.textplots")
#install.packages("data.table")
#install.packages("stringr")
#install.packages("spacyr")
#install.packages("textcat")

# load libraries
library(quanteda)
library(readtext)
library(tidyverse)
library(quanteda.textplots)
library(quanteda.textstats)
library(spacyr)
library(stringr)
library(data.table)
library(textcat)

spacy_initialize(model = "de_core_news_sm")
```
### Load Corpus Files and List of Compounds
(Please make sure that the path is set to the current directory to run this code).
```{r}
# load corpus files 
pro2022 = readRDS("corpora/pro2022.rds")
contra2022 = readRDS("corpora/contra2022.rds")

# load list of final climate change compounds 
compounds <- scan("files/wordlist.txt", what="", sep="\n")

# load list of final climate change compounds and word forms
compound_df <- read.csv("files/compounds.csv", sep = ";", stringsAsFactors=FALSE)

# lower compounds and word forms                  
compounds <- tolower(compounds)
compound_df$original <- tolower(compound_df$original)
compound_df$compound_forms <- tolower(compound_df$compound_forms)
```

# 1. Preprocessing
To preprocess the corpora, we perform several steps: stop word removal, lemmatization, removal of hyphens  

## 1.1 Retrieve Stop Lists 
```{r}
# retrieve stoplists
de_stopwords <- stopwords::stopwords("de", source="snowball")
en_stopwords <- stopwords::stopwords("en", source="snowball" )

# add custom stoplist
custom_stopwords <- c("dass", "=", "the", "seit", "ab", "beim", "\n", "mal", "c", "\\|","|", "m", "kommentare", "neueste", "gepostet", "admin", "cookies", "inhalte", "inhalt", "newsletter", "posten", "zugriff", "passwort", "geschützt", "seite", "website", "webseite", "and", "0", "1", "2", "3","4","5","6","7","8","9", "mfg","w","t","wer","00", "30", ">", "anmelden", "\\+", "40", "81", "erneuerbarer",
"OWLIT", "et", "\\´", "\\^", "tppubtype", "pubstate", "z", "b", "d", "ct", "--", "[", "]", "{", "}")

# combine lists to a full version
full_stopwords <- c(de_stopwords,en_stopwords,custom_stopwords)
```

## 1.2 Lemmatization
Use the `spacy_parse` function to lemmatize the corpus data
```{r}
# parse the pro corpus with spacy function and retrieve lemma for each token
sp_pro2022 <- spacy_parse(pro2022, pos=FALSE, entity=FALSE, dependency=FALSE)
sp_contra2022 <- spacy_parse(contra2022, pos=FALSE, entity=FALSE, dependency=FALSE)
```

### Manually retrieve lemma form of compound words 
Since `spacyr` is not capable of performing a lemmatization of our compound words, we use our manually created list of the word forms for all compound words to retrieve the lemma form. 
```{r}
# function to preprocess compounds data frame 
unlist_forms = function(word){
  x <- unlist(strsplit(word, ","))
  return(gsub(" ","",x))}

# retrieve lemma forms for our compound words 
# for each compound word 
for (words in compound_df$compound_forms){
  word_forms = c(unlist_forms(words)) # put into correct format
  
  # each word form of the compound word 
  for (token in word_forms){
    
    # retrieve the lemma form
    lemma_form <- compound_df[compound_df$compound_forms %like% token, ]$original[[1]]
  
    # replace lemma form in pro and contra lemmatized data frame  
    # to make sure we get correct lemma form
    sp_pro2022$lemma[tolower(sp_pro2022$token) == token] <- lemma_form
    sp_contra2022$lemma[tolower(sp_contra2022$token) == token] <- lemma_form
    }
}
```

This is still part of spacyr's lemmatization procedure. 
```{r}
# replace token with lemma - Pro Corpus
sp_pro2022$token <- sp_pro2022$lemma

# replace token with lemma - Contra Corpus
sp_contra2022$token <- sp_contra2022$lemma
```

### Remove Hyphens from Words and Create Tokens Object 
Some words contain a hyphen which is being removed in the following step. E.g. "Klima-skeptiker" becomes "Klimaskeptiker".
```{r}
# remove hyphens from tokens
sp_p2022_tokens <- as.tokens(sp_pro2022)
toks_comp <- tokens_compound(sp_p2022_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_p2022_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))

sp_c2022_tokens <- as.tokens(sp_contra2022)
toks_comp <- tokens_compound(sp_c2022_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_c2022_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))
```

### Apply Preprocessing 
```{r}
# apply preprocessing to pro corpus tokens
sp_p2022_tokens <- sp_p2022_tokens %>% 
  tokens(remove_punct = FALSE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, 
         remove_separators = TRUE, split_hyphens=FALSE) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = full_stopwords, padding = FALSE)

# apply preprocessing to contra corpus tokens
sp_c2022_tokens <- sp_c2022_tokens %>% 
  tokens(remove_punct = FALSE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, 
         remove_separators = TRUE, split_hyphens=FALSE) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = full_stopwords, padding = FALSE)
```

## 1.3 Create DFM 
In a last step, we generate a Document-Feature-Matrix which we need for the upcoming corpus-based methods. 
```{r}
# create dfm
dfm_p2022_lemma <- dfm(sp_p2022_tokens)
dfm_c2022_lemma <- dfm(sp_c2022_tokens)
```

```{r}
dfm_c2022_lemma
```

```{r}
head(dfm_sort(dfm_p2022_lemma, decreasing=TRUE, "features"))
```
```{r}
head(dfm_sort(dfm_c2022_lemma, decreasing=TRUE, "features"))
```


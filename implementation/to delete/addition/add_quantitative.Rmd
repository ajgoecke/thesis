---
title: "Adding New Terms"
output: html_notebook
---
Please run `implementation/addition/add_corpus.Rmd` first to update the corpus (do not forget to change the corpus names in here too then.)

Please run `implementation/addition/addition.ipynb.` before this to make sure we have all information we need for the new compound.  

# Load existing corpora
```{r}
# load corpus files 
pro2022 = readRDS("corpora/pro2022.rds")
contra2022 = readRDS("corpora/contra2022.rds")

# load list of new climate change compounds 
compounds <- scan("../../wordlist_new.txt", what="", sep="\n")

# load list of new climate change compounds and word forms
compound_df <- read.csv("../../addition/compounds_prep.csv", sep = ";", stringsAsFactors=FALSE)

# lower compounds and word forms                  
compounds <- tolower(compounds)
compound_df$original <- tolower(compound_df$original)
compound_df$compound_forms <- tolower(compound_df$compound_forms)
```


# 1. Preprocessing
To preprocess the corpora, we perform several steps: stop word removal, lemmatization, removal of hyphens  

## 1.1 Retrieve Stop Lists 
```{r}
# retrieve stoplists
de_stopwords <- stopwords::stopwords("de", source="snowball")
en_stopwords <- stopwords::stopwords("en", source="snowball" )

# add custom stoplist
custom_stopwords <- c("dass", "=", "the", "seit", "ab", "beim", "\n", "mal", "c", "\\|","|", "m", "kommentare", "neueste", "gepostet", "admin", "cookies", "inhalte", "inhalt", "newsletter", "posten", "zugriff", "passwort", "geschützt", "seite", "website", "webseite", "and", "0", "1", "2", "3","4","5","6","7","8","9", "mfg","w","t","wer","00", "30", ">", "anmelden", "\\+", "40", "81", "erneuerbarer",
"OWLIT", "et", "\\´", "\\^", "tppubtype", "pubstate", "z", "b", "d", "ct", "--", "[", "]", "{", "}")

# combine lists to a full version
full_stopwords <- c(de_stopwords,en_stopwords,custom_stopwords)
```

## 1.2 Lemmatization
Use the `spacy_parse` function to lemmatize the corpus data
```{r}
# parse the pro corpus with spacy function and retrieve lemma for each token
sp_pro2022 <- spacy_parse(pro2022, pos=FALSE, entity=FALSE, dependency=FALSE)
sp_contra2022 <- spacy_parse(contra2022, pos=FALSE, entity=FALSE, dependency=FALSE)
```

### Manually retrieve lemma form of compound words 
Since `spacyr` is not capable of performing a lemmatization of our compound words, we use our manually created list of the word forms for all compound words to retrieve the lemma form. 
```{r}
# function to preprocess compounds data frame 
unlist_forms = function(word){
  x <- unlist(strsplit(word, ","))
  return(gsub(" ","",x))}

# retrieve lemma forms for our compound words 
# for each compound word 
for (words in compound_df$compound_forms){
  word_forms = c(unlist_forms(words)) # put into correct format
  
  # each word form of the compound word 
  for (token in word_forms){
    
    # retrieve the lemma form
    lemma_form <- compound_df[compound_df$compound_forms %like% token, ]$original[[1]]
  
    # replace lemma form in pro and contra lemmatized data frame  
    # to make sure we get correct lemma form
    sp_pro2022$lemma[tolower(sp_pro2022$token) == token] <- lemma_form
    sp_contra2022$lemma[tolower(sp_contra2022$token) == token] <- lemma_form
    }
}
```

This is still part of spacyr's lemmatization procedure. 
```{r}
# replace token with lemma - Pro Corpus
sp_pro2022$token <- sp_pro2022$lemma

# replace token with lemma - Contra Corpus
sp_contra2022$token <- sp_contra2022$lemma
```

### Remove Hyphens from Words and Create Tokens Object 
Some words contain a hyphen which is being removed in the following step. E.g. "Klima-skeptiker" becomes "Klimaskeptiker".
```{r}
# remove hyphens from tokens
sp_p2022_tokens <- as.tokens(sp_pro2022)
toks_comp <- tokens_compound(sp_p2022_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_p2022_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))

sp_c2022_tokens <- as.tokens(sp_contra2022)
toks_comp <- tokens_compound(sp_c2022_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_c2022_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))
```

### Apply Preprocessing 
```{r}
# apply preprocessing to pro corpus tokens
sp_p2022_tokens <- sp_p2022_tokens %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, 
         remove_separators = TRUE, split_hyphens=FALSE) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = full_stopwords, padding = FALSE) 

# apply preprocessing to contra corpus tokens
sp_c2022_tokens <- sp_c2022_tokens %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, 
         remove_separators = TRUE, split_hyphens=FALSE) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = full_stopwords, padding = FALSE) 
```

## 1.3 Create DFM 
In a last step, we generate a Document-Feature-Matrix which we need for the upcoming corpus-based methods. 
```{r}
# create dfm
dfm_p2022_lemma <- dfm(sp_p2022_tokens)
dfm_c2022_lemma <- dfm(sp_c2022_tokens)
```





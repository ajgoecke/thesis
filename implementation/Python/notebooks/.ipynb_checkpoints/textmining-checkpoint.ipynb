{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62848dc-66e2-4bc3-bfd2-b8154e373333",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION OF TEXT MINING METHODS \n",
    "The present notebook contains all code that was used to apply text mining methods on the glossary terms. This is the heart piece of the implementation. Please read the `README` first to make sure that the code will work on your device. \n",
    "\n",
    "**Important**: Due to the notation that was used in prior project regarding the glossary, we will in many cases use the term `pro` to refer to the corpus of climate supporters and the term `con(tra)` to refer to the corpus of climate skeptics. This notation is also used throughout the R notebooks. Accordingly, variable names containing `pro` or `con` are used to indicate that this variable exists for each of the corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded432ec-2e29-4044-898f-adc1e2cc1f6f",
   "metadata": {},
   "source": [
    "## Requirements to run this Notebook\n",
    "\n",
    "Please make sure that the following libraries are installed to be able to run all code of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "id": "f8cd4f02-03c6-4dd5-849e-6db8dd5697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install wn\n",
    "#!pip3 install spacy\n",
    "#!python -m spacy download de_core_news_md\n",
    "#!pip3 install networkx\n",
    "#!pip3 install tabulate\n",
    "#!pip3 install germansentiment\n",
    "#!pip3 install -U numpy\n",
    "#!pip3 install -U textblob-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5ba1c4-db3e-4cbd-b6b8-8b23c2551f68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "### IMPORT REQUIREMENTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from nltk.metrics import distance as dist\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "from germansentiment import SentimentModel\n",
    "model = SentimentModel()\n",
    "\n",
    "### IMPORT SPACY \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "\n",
    "### IMPORT WORDNET \n",
    "import wn\n",
    "import wn.taxonomy\n",
    "#wn.download('odenet')\n",
    "#wn.download('oewn:2021')\n",
    "\n",
    "# assign German and English WordNet source \n",
    "en = wn.Wordnet('oewn:2021')\n",
    "de = wn.Wordnet('odenet:1.4')\n",
    "\n",
    "from wn.similarity import path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff61f0-3ae7-410b-8d60-ff9c185fea6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Load Files\n",
    "\n",
    "The file `compounds.csv` is the output of the `preprocessing.ipynb` file. The file is loaded here into a pandas data frame and columns are preprocessed such that they have the correct format for the upcoming pieces of code. These are the unchanged files that were originally used to run the upcoming text mining methods. They only contain simplistic information and the output of the corpus-based methods from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "d4723891-a286-4d08-b8a4-51c9dc7bf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load compounds file (output of preprocessing.ipynb)\n",
    "compounds = pd.read_csv(\"../files/compounds_info.csv\")\n",
    "\n",
    "# load context files (i.e. output of corpus-based methods notebook)\n",
    "pro_context = pd.read_csv(\"../../R/output/pro_context.csv\")\n",
    "con_context = pd.read_csv(\"../../R/output/con_context.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c38dc-161a-4729-957b-a6b51fa92e76",
   "metadata": {},
   "source": [
    "Or to load the final files that are composed in the course of this notebook (to avoid re-running all code) please run the following code:\n",
    "- `knowledge_base` = the complete knowledge base containing all information about the compounds\n",
    "- `pro_info`= knowledge base of methods that were applied to context of the climate supporters corpus\n",
    "- `con_info`= knowledge base of methods that were applied to context of the climate skeptics corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89b68c8-1e68-41f3-a03d-6ab04f1da623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final knowledge base \n",
    "knowledge_base = pd.read_csv(\"../output/knowledge_base.csv\")\n",
    "\n",
    "# load context files (i.e. output of corpus-based methods notebook in R)\n",
    "pro_info = pd.read_csv(\"../output/pro_info.csv\")\n",
    "con_info = pd.read_csv(\"../output/con_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac04c34-faaf-4d9a-8b1b-40e2fb9599cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46345df8-147a-427f-a5e6-60a4f6297856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.1 Convert Literals\n",
    "For most of the `csv` files the literals are not evaluated correctly, i.e. columns containing a list of strings is evaluated as a string when loading them into Python using `pandas` (`read_csv`). This issue is addressed by applying the `literal_eval` function of the `ast` library to the columns for which we have the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b102c99-d93e-454f-9e28-22a65644015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds['noun_forms'] = compounds.noun_forms.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['compound_forms'] = compounds.compound_forms.apply(lambda x: literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0e8f1-4f9a-43db-aa80-51824fc543de",
   "metadata": {},
   "source": [
    "This may also be the case for the following columns if the final knowledge base was loaded into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9545cc2-1095-45aa-8a03-3d493c347e7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compounds['compound_forms'] = compounds.compound_forms.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['related_words'] = compounds.related_words.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['hypernyms'] = compounds.hypernyms.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['en_hypernyms'] = compounds.en_hypernyms.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['definition'] = compounds.definition.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['PERS_pro'] = compounds.PERS_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['PERS_con'] = compounds.PERS_con.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['ORG_pro'] = compounds.ORG_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['ORG_con'] = compounds.ORG_con.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['similar_words'] = compounds.similar_words.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['pro_mods'] = compounds.pro_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['con_mods'] = compounds.con_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds[\"PERS_pro\"] = compounds.PERS_pro.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"PERS_con\"] = compounds.PERS_con.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"ORG_pro\"] = compounds.ORG_pro.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"ORG_con\"] = compounds.ORG_con.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"pro_attr\"] = compounds.pro_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != 'nan') else x)\n",
    "compounds[\"con_attr\"] = compounds.con_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != 'nan') else x)\n",
    "compounds[\"pro_colls\"] = compounds.pro_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else list())\n",
    "compounds[\"con_colls\"] = compounds.con_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "147d8605-be7c-4934-b570-db3ff8fb77a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pro_context['entities'] = pro_context.entities.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['persons'] = pro_context.persons.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['organisations'] = pro_context.organisations.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['PERS'] = pro_context.PERS.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['ORG'] = pro_context.ORG.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['dependencies'] = pro_context.dependencies.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "pro_context['modifiers'] = pro_context.modifiers.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "\n",
    "con_context['entities'] = con_context.entities.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['persons'] = con_context.persons.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['organisations'] = con_context.organisations.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['PERS'] = con_context.PERS.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['ORG'] = con_context.ORG.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['dependencies'] = con_context.dependencies.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "con_context['modifiers'] = con_context.modifiers.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ec2d-5d8c-4763-b5cf-4ce45f776562",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Reduce Concordances\n",
    "The context that we retrieved via the concordances (`kwic`) in R unfortunately could only be performed on regex level. Accordingly also more complex forms of the compounds are contained in out data frame. E.g. for the compound *Klimagerechtigkeit* also concordances containing the key word *Klimagerechtigkeitspolitik* were included and for the compound *Klimaalarm* also key word phrases containin the adjective form *klimaalarmistisch* were retrieved. We want to get rid of the rows not containing **exact matches** of our compound word forms. \n",
    "\n",
    "Accordingly, we search for the rows that contain one of the compound forms and retrieve those columns for the data frames which we will then use for the upcoming analyses. \n",
    "\n",
    "To do this, we first get a list of the compound word forms from our `compounds` data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3118,
   "id": "29a9d078-086c-4ae9-bf77-e8c7e491bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_forms = compounds.compound_forms.tolist()\n",
    "compound_forms = [item for sublist in compound_forms for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66499c-9d4f-40b3-b259-10abc76ae983",
   "metadata": {},
   "source": [
    "Then we apply the following function from `pandas` to check whether a string is contained in the row and to retrieve the according rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3145,
   "id": "ae50249c-854a-4d2c-bcf6-1d4feba12098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all columns which contain a compound form\n",
    "pro_context = pro_context[pro_context.keyword.str.contains(\" |\".join(compound_forms), case=False).groupby(level=0).any()]\n",
    "con_context = con_context[con_context.keyword.str.contains(\" |\".join(compound_forms), case=False).groupby(level=0).any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770aae7-80d2-4e52-ae5d-31680a57b727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Working with WordNet\n",
    "In this section we will apply various function from the `WordNet` library to the `compounds` data frame, i.e. the list of compound words and word forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3a031-c4b4-481e-8669-01cf4b6af22e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.1 Exploring Hierarchical Structures (Hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fa0d8-ba73-4e83-854a-22c80fe028a6",
   "metadata": {},
   "source": [
    "We create multiple functions which we need to retrieve the synset and additional more information of the noun from the `WordNet` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a215763c-22a1-4903-aec5-0d640259c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the WordNet synsets of the input string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        The synsets of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        word = de.synsets(string, pos=\"n\") # retrieve synsets for the string\n",
    "        return word # return synsets\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def get_lemmas(string):\n",
    "    \"\"\"\n",
    "    Returns the WordNet lemmas of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of lemmas (i.e. related words) of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmas = [] # initiate empty list\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            lemmas.append(s.lemmas()) # retrieve lemmas and append to list\n",
    " \n",
    "        return list(set([x for l in lemmas for x in l])) # return flattened list of lemmas \n",
    "    \n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    \n",
    "def get_hypernyms(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the WordNet hypernyms of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of hypernyms of the noun if available, else empty list.\n",
    "    \"\"\"\n",
    "       \n",
    "    hypers = [] # initiate empty list\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            hypernyms = s.hypernyms() # retrieve hypernyms\n",
    "            \n",
    "            # for each hypernym\n",
    "            for el in hypernyms:\n",
    "                hypers.append(el.lemmas()) # retrieve lemmas\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return list(set([y for x in hypers for y in x])) # return flattened list of hypernyms \n",
    "      \n",
    "\n",
    "# for each compound save lemmas (as related words), hypernyms, definition to compound dataframe \n",
    "compounds['related_words'] = compounds.second_part.apply(get_lemmas)\n",
    "compounds['hypernyms'] = compounds.second_part.apply(get_hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98619b-8fa4-4a2f-9b37-8d170e58e85c",
   "metadata": {},
   "source": [
    "To visualize the hierarchical tree structure of the WordNet knowledge base, we will have a quick look at the following example. Here we see the hypernyms of the word \"Betrug\" (en: \"fraud\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "898ea735-32d8-4da6-a5f4-ef51627ab0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path 1\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      "  Synset('odenet-15937-n') Frevel\n",
      "   Synset('odenet-6999-n') Topf\n",
      "    Synset('odenet-9850-n') Vermögen\n",
      "     Synset('odenet-4667-n') Vermögen\n",
      "      Synset('odenet-10390-n') Liegenschaft\n",
      "\n",
      "Path 2\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      "  Synset('odenet-5502-n') Handlung\n",
      "\n",
      "Path 3\n",
      " Synset('odenet-8872-n') Rauheit\n",
      "  Synset('odenet-25840-n') Unglück\n"
     ]
    }
   ],
   "source": [
    "# retrieve synsets\n",
    "synsets = de.synsets('Betrug', pos='n')\n",
    "\n",
    "# for each synset\n",
    "p = 1\n",
    "for s in synsets: \n",
    "    \n",
    "    # for each hypernym path\n",
    "    for path in wn.taxonomy.hypernym_paths(s):\n",
    "        print(\"\\nPath\", p)\n",
    "        for i, ss in enumerate(path):\n",
    "            \n",
    "            # print synset ID and lemma\n",
    "            print(' ' * i, ss, ss.lemmas()[0]) \n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9dbe7-7af4-46e0-827a-af02ab9198ac",
   "metadata": {},
   "source": [
    "Additionally, to retrieve the hypernym which is closest to the root of the knowledge base, we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2640,
   "id": "de921c6c-4b50-44e2-9bfa-0a162ca72fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roots(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the German WordNet roots of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of root concepts of the noun if available, else empty list.\n",
    "    \"\"\"\n",
    "    \n",
    "    roots = [] # initiate empty list\n",
    "    synsets = get_synset(string) # retrieve synsets\n",
    "\n",
    "    # for each synset\n",
    "    for s in synsets: \n",
    "        \n",
    "        # for each hypernym path\n",
    "        for path in wn.taxonomy.hypernym_paths(s):\n",
    "            \n",
    "            # retrieve root (i.e. last element of path)\n",
    "            roots.append(path[-1].lemmas())\n",
    "            \n",
    "    # return flattened list of root hypernyms\n",
    "    return [y for x in roots for y in x]\n",
    "\n",
    "# apply function\n",
    "compounds['roots'] = compounds.second_part.apply(get_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574f5a7-295c-4e66-a462-404bdb9d0a8e",
   "metadata": {},
   "source": [
    "To be able to specify whether the concept of the compound describes an action or a person, we will retrieve the english hypernym paths for each compound word. For this, we translate the synset and retrieve the hypernym paths as we did before for the German WordNet lexicon. Instead of only retrieving the root concept (which is always `entity` for the English lexicon), we retrieve the complete path and check for key words that give us the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2641,
   "id": "a69031b5-157b-47b9-a918-7d50b4fe79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_hypernyms(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the English WordNet hypernym paths of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of hypernyms of the noun if available, else NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        roots = [] # initiate empty list\n",
    "        synsets = get_synset(string)[0].translate(lexicon='oewn:2021') # get English version of synset \n",
    "    \n",
    "        # for each synset\n",
    "        for s in synsets: \n",
    "            \n",
    "            # for each hypernym path\n",
    "            for path in wn.taxonomy.hypernym_paths(s):\n",
    "                \n",
    "                # retrieve lemmas of hypernyms and append to list \n",
    "                roots.append(x.lemmas() for x in path)\n",
    "                \n",
    "        # flatten list\n",
    "        roots = [z for x in roots for y in x for z in y]\n",
    "        \n",
    "        return roots # return list\n",
    "    \n",
    "    # except no translation is available\n",
    "    except:\n",
    "        \n",
    "        return np.nan # then return NaN\n",
    "\n",
    "# apply function\n",
    "compounds['en_hypernyms'] = compounds.second_part.apply(get_en_hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99946aac-a4c4-4c45-b88d-4cdfa413a169",
   "metadata": {},
   "source": [
    "For instance, let's retrieve the English hypernym paths for the following two words: \"Betrüger\" (person) and \"Betrug\" (action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2642,
   "id": "11021c26-b1af-423d-8d03-2390fcacc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betrüger - Hypernym Paths:\n",
      " Synset('oewn-09974494-n') chiseler\n",
      "  Synset('oewn-10017621-n') slicker\n",
      "   Synset('oewn-09657157-n') offender\n",
      "    Synset('oewn-09851208-n') bad person\n",
      "     Synset('oewn-00007846-n') soul\n",
      "      Synset('oewn-00004475-n') being\n",
      "       Synset('oewn-00004258-n') animate thing\n",
      "        Synset('oewn-00003553-n') unit\n",
      "         Synset('oewn-00002684-n') physical object\n",
      "          Synset('oewn-00001930-n') physical entity\n",
      "           Synset('oewn-00001740-n') entity\n",
      " Synset('oewn-09974494-n') chiseler\n",
      "  Synset('oewn-10017621-n') slicker\n",
      "   Synset('oewn-09657157-n') offender\n",
      "    Synset('oewn-09851208-n') bad person\n",
      "     Synset('oewn-00007846-n') soul\n",
      "      Synset('oewn-00004475-n') being\n",
      "       Synset('oewn-00007347-n') cause\n",
      "        Synset('oewn-00001930-n') physical entity\n",
      "         Synset('oewn-00001740-n') entity\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "\n",
      "Betrug - Hypernym Paths:\n",
      " Synset('oewn-00770581-n') fraud\n",
      "  Synset('oewn-00767761-n') criminal offence\n",
      "   Synset('oewn-00767587-n') offence\n",
      "    Synset('oewn-00746303-n') transgression\n",
      "     Synset('oewn-00734044-n') wrongdoing\n",
      "      Synset('oewn-00408356-n') activity\n",
      "       Synset('oewn-00030657-n') human action\n",
      "        Synset('oewn-00029677-n') event\n",
      "         Synset('oewn-00002137-n') abstraction\n",
      "          Synset('oewn-00001740-n') entity\n"
     ]
    }
   ],
   "source": [
    "word1 = \"Betrüger\"\n",
    "word2 = \"Betrug\"\n",
    "\n",
    "print(word1, \"- Hypernym Paths:\")\n",
    "# for each hypernym path of the synset\n",
    "for path in wn.taxonomy.hypernym_paths(get_synset(word1)[0].translate(lexicon='oewn:2021')[0]):\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synset and lemma \n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"_\"*50)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(word2, \"- Hypernym Paths:\")\n",
    "# for each hypernym path of the synset\n",
    "for path in wn.taxonomy.hypernym_paths(get_synset(word2)[0].translate(lexicon='oewn:2021')[0]):\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synset and lemma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c3d60-d103-49a5-b333-54266f4a116c",
   "metadata": {},
   "source": [
    "In the paths we can see the following key words for the two concepts:\n",
    "- **Betrüger**: person, soul\n",
    "- **Betrug**: activity, human action\n",
    "\n",
    "Also for other concepts we identify the following hypernyms:\n",
    "- **Vernunft**: psychological feature\n",
    "- **Staat**: people, grouping\n",
    "\n",
    "We finally decide for the *four* main categories which we want to use to specify the concepts by looking for the following key words:\n",
    "- Abstraction: rational motive, motive, state, psychological feature, attribute,\n",
    "                phenomenon, process, cause, physical object, abstract entity, artifact\n",
    "- Person: person, soul, image, spiritual being, ideal\n",
    "- Action: activity, human action, wrongdoing\n",
    "- Group: grouping, people\n",
    "- Location: location, area\n",
    "\n",
    "Accordingly, we will create lists of those key words and specify the concept in the upcoming lines and save the information to a new column `concept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3166,
   "id": "89c51187-0544-4515-9887-6c11339afc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate concepts\n",
    "abstraction = [\"rational motive\", \"motive\", \"state\", \"psychological feature\", \"attribute\",\n",
    "                \"phenomenon\", \"process\", \"cause\", \"physical object\", \"abstract entity\", \"artifact\"]\n",
    "person = [\"person\", \"soul\",  \"image\", \"spiritual being\", \"ideal\"] \n",
    "action = [\"activity\", \"human action\", \"wrongdoing\"]\n",
    "group = [\"grouping\", \"people\"]\n",
    "location = [\"location\", \"area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3167,
   "id": "8a522f7b-65d6-41f7-b3bb-ff94512df080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify_concept(hypernyms):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the concept label for each compound word.\n",
    "    Arg: \n",
    "        hypernyms: a list of hypernyms.\n",
    "    Returns: \n",
    "        A concept label (either \"person\" or \"action\") for the compound word, else NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of action key words\n",
    "        if len(set(location).intersection(set(hypernyms))) > 0:\n",
    "            return \"location\" # label as location\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of group key words    \n",
    "        elif len(set(group).intersection(set(hypernyms))) > 0:\n",
    "            return \"group\" # label as group\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of action key words\n",
    "        elif len(set(action).intersection(set(hypernyms))) > 0:\n",
    "            return \"action\" # label as action\n",
    "       \n",
    "        # if there is at least one common element in list of hypernyms and list of person key words    \n",
    "        elif len(set(person).intersection(set(hypernyms))) > 0:\n",
    "            return \"person\" # label as person    \n",
    "    \n",
    "        # if there is at least one common element in list of hypernyms and list of abstraction key words    \n",
    "        elif len(set(abstraction).intersection(set(hypernyms))) > 0:\n",
    "            return \"abstraction\" # label as abstraction\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            return(np.nan)\n",
    "        \n",
    "    # if there is no comparison available (e.g. bc. no hypernym list is available for the row)\n",
    "    except:\n",
    "        return np.nan # return NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3168,
   "id": "4642fd16-7a6b-4837-9a49-c14a0785db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to en_hypernyms column\n",
    "compounds['concept'] = compounds.en_hypernyms.apply(specify_concept)\n",
    "\n",
    "# manually change the value of \"Milliardär\" since \n",
    "compounds.loc[(compounds.second_part == 'milliardär'),'concept']='person'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018f5f7-59b9-4233-8f95-bd679a9bfed5",
   "metadata": {},
   "source": [
    "Next, we will save those columns for which we could not identify a concept to a csv file to manually add the concept of those compound words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3185,
   "id": "a5ad24f0-4435-4c7b-8961-351a520334d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gather all columns that did not receive a concept\n",
    "concept_manual = compounds[compounds['concept'].isna()][[\"original\", \"concept\"]]\n",
    "\n",
    "#concept_manual.to_csv(\"../evaluation/concept_manual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633065e-f8bf-4631-8c3e-5b671d37d01b",
   "metadata": {},
   "source": [
    "With the automatic method we could specify concepts for 80.65% of the compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3186,
   "id": "21096ef0-5f12-4119-8fd2-843a144297b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.24193548387098"
      ]
     },
     "execution_count": 3186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - (len(concept_manual)/248)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f75e9-7c2d-4e49-9180-ad3370f43f4d",
   "metadata": {},
   "source": [
    "After the manual annotation of the remaining 48 concepts, we load the table back into Python and merge the concepts to the final knowledeg base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3187,
   "id": "7de82579-c595-4a55-a3e5-56947ef3f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated table\n",
    "concept_annotated = pd.read_csv(\"../evaluation/concept_manual.csv\", sep =\";\")\n",
    "\n",
    "# reset index and update values\n",
    "compounds = compounds.set_index('original')\n",
    "concept_annotated = concept_annotated.set_index('original')\n",
    "compounds.update(concept_annotated)\n",
    "compounds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5abad9-e20d-458f-9f8a-b671741c4f7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.2 Definitions\n",
    "In the following piece of code we retrieve the definitions of the synsets of a word, retrieved from the German `OdeNet` distribution of `WordNet`. (Note: This information will not be used in the final definition phrasing part, please see the paper for an explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2655,
   "id": "e79c2466-68cf-4efd-ae8b-257ba7284b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definition(string):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the WordNet definitions of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of definitions of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    definition = [] # initiate empty list\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            definition.append(s.definition()) # retrieve definition and append to list\n",
    " \n",
    "        return definition # return list of definitions\n",
    "    \n",
    "    except:\n",
    "        return \n",
    "    \n",
    "# create and save definition to new column in data frame\n",
    "compounds['definition'] = compounds.second_part.apply(get_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf743c-7c9e-4c33-812d-d1aee1b2b168",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.3 Similarity Measures\n",
    "Via the `WordNet` library there are multiple options to compute the similarity of two input concepts. For our case we used the following two ways of retrieving a similarity score. \n",
    "Here, a score of 1 means that words are very similar and 0 indicating that words are not similar at all.\n",
    "\n",
    "In the following example we can see how the two options differ in scoring for the same combination of words. While the `PATH` similarity score for **Betrug** and **Verbrechen** is 0.5, the `WUP` score is 0.9 - indicating a higher relatedness of the two concepts than the first method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2656,
   "id": "8f4daf45-f123-4268-a2f1-af99e346d900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PATH similarity measure\n",
    "# 1 is being \"very similar\", 0 is \"not similar\" (i.e. no connection in wn)\n",
    "wn.similarity.path(get_synset(\"Betrug\")[0], get_synset(\"Verbrechen\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2657,
   "id": "9bd7e1a7-5795-44cb-86b4-5a2eaebc23cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 2657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WUP similarity measure \n",
    "wn.similarity.wup(get_synset(\"Betrug\")[0], get_synset(\"Verbrechen\")[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d69998-c55b-4daa-a7b1-b559e45aecf4",
   "metadata": {},
   "source": [
    "The retrieval of the common synset path results in the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2658,
   "id": "f8f0431c-f854-460c-be00-2b5facfd2f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of 'Betrug' and 'Verbrechen':\n",
      "\n",
      "Handlung\n",
      "Treulosigkeit\n",
      "Liegenschaft\n",
      "Vermögen\n",
      "Vermögen\n",
      "Topf\n",
      "Frevel\n",
      "Treulosigkeit\n"
     ]
    }
   ],
   "source": [
    "# get list of common paths\n",
    "paths = [list(reversed([get_synset(\"Betrug\")[0]] + p)) for p in get_synset(\"Verbrechen\")[0].hypernym_paths()]\n",
    "print(\"Path of 'Betrug' and 'Verbrechen':\\n\")\n",
    "\n",
    "# for each hypernym in the path\n",
    "for el in paths:\n",
    "    for item in el:\n",
    "        print(item.lemmas()[0]) # print the lemma of the hypernym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fd4f2-be61-45a0-961f-4acc8e855b8c",
   "metadata": {},
   "source": [
    "To illustrate the hypernym paths and the according path similarity scores for each edge between the nodes please see the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2659,
   "id": "31a41313-52ac-427f-b7ff-fd47183ee7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path 1 Betrug\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      " path: 0.9230769230769231\n",
      "  Synset('odenet-15937-n') Frevel\n",
      "  path: 0.8333333333333334\n",
      "   Synset('odenet-6999-n') Topf\n",
      "   path: 0.7272727272727273\n",
      "    Synset('odenet-9850-n') Vermögen\n",
      "    path: 0.6\n",
      "     Synset('odenet-4667-n') Vermögen\n",
      "     path: 0.4444444444444444\n",
      "      Synset('odenet-10390-n') Liegenschaft\n",
      "      path: 0.25\n",
      "\n",
      "Path 2 Betrug\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      " path: 0.9230769230769231\n",
      "  Synset('odenet-5502-n') Handlung\n",
      "  path: 0.5\n",
      "_____________________________________________\n",
      "\n",
      "Path 1 Verbrechen\n",
      " Synset('odenet-5502-n') Handlung\n",
      " path: 0.5\n",
      "\n",
      "Path 2 Verbrechen\n",
      " Synset('odenet-15937-n') Frevel\n",
      " path: 0.8333333333333334\n",
      "  Synset('odenet-6999-n') Topf\n",
      "  path: 0.7272727272727273\n",
      "   Synset('odenet-9850-n') Vermögen\n",
      "   path: 0.6\n",
      "    Synset('odenet-4667-n') Vermögen\n",
      "    path: 0.4444444444444444\n",
      "     Synset('odenet-10390-n') Liegenschaft\n",
      "     path: 0.25\n"
     ]
    }
   ],
   "source": [
    "# initiate words that we want to compare \n",
    "word1 = \"Betrug\"\n",
    "word2 = \"Verbrechen\"\n",
    "\n",
    "# retrieve synset\n",
    "synset1 = get_synset(word1)[0]\n",
    "p = 1 # initiate path count\n",
    "\n",
    "# for each path\n",
    "for path in wn.taxonomy.hypernym_paths(synset1):\n",
    "    print(\"\\nPath\",p, word1) \n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synsets and lemmas\n",
    "        print(\" \" * i, \"path:\", wn.similarity.wup(word, ss)) # retrieve path similarity\n",
    "    p = p+1\n",
    "    \n",
    "print(\"_\"*45)    \n",
    "synset2 = get_synset(word2)[0]\n",
    "p = 1 # initiate path count\n",
    "\n",
    "# for each path\n",
    "for path in wn.taxonomy.hypernym_paths(synset2):\n",
    "    print(\"\\nPath\",p, word2)\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synsets and lemmas\n",
    "        print(\" \" * i, \"path:\", wn.similarity.wup(word, ss)) # retrieve path similarity\n",
    "    p = p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2660,
   "id": "ca02fe0d-a6db-4a99-81f2-62d2ce586283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common hypernyms of 'Betrug' and 'Verbrechen': \n",
      "\n",
      "Liegenschaft\n",
      "krimineller Akt\n",
      "Frevel\n",
      "Vermögen\n",
      "Handlung\n",
      "Topf\n",
      "Vermögen\n"
     ]
    }
   ],
   "source": [
    "print(\"Common hypernyms of 'Betrug' and 'Verbrechen': \\n\")\n",
    "\n",
    "for x in sorted(get_synset(\"betrug\")[0].common_hypernyms(get_synset(\"verbrechen\")[0])):\n",
    "    print(x.lemmas()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817ae37-3d7c-4e2e-b67b-35f1db0b6e76",
   "metadata": {},
   "source": [
    "The following code retrieves the **Lowest Common Hypernym** of both concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0c5ec261-db58-438f-a9ce-e80cd9e2546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krimineller Akt\n"
     ]
    }
   ],
   "source": [
    "# retrieve lowest common hypernym \n",
    "lowest_common = get_synset(\"Betrug\")[0].lowest_common_hypernyms(get_synset(\"Verbrechen\")[0])\n",
    "\n",
    "for s in lowest_common:\n",
    "    print(s.lemmas()[0]) # print lemma of hypernym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04f439-df52-44bc-856f-75eb109a3e2d",
   "metadata": {},
   "source": [
    "To work with similarity scores on our `compounds` data frame we first initiate a list of the nouns of our data frame to have a closer look at. Furthermore we initiate two empty data frames to compute and save the similarity scores. For this, we create a matrix containing all nouns as columns and as rows. The values then constitute the similarity scores for each combination of nouns. \n",
    "\n",
    "Then we run the `WUP` function and the `PATH` function on our nouns and save the computed similarity scores to the new data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2662,
   "id": "186949b5-3127-44af-a843-a137a124b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of nouns from compound words to work with for similarity measures\n",
    "nouns = compounds.second_part.tolist()\n",
    "\n",
    "# create data frame (matrix like) with all nouns as columns and rows to compute similarity \n",
    "nouns_wup = pd.DataFrame(index = nouns, columns = nouns)\n",
    "nouns_sim = pd.DataFrame(index = nouns, columns = nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2663,
   "id": "673d9c8c-5169-41a4-a862-1d205849cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill dataframe with similarity scores (\"wup\" function)\n",
    "\n",
    "# iterate over columns and rows\n",
    "for w in nouns:\n",
    "    for ww in nouns:\n",
    "        try: \n",
    "            wn_w = get_synset(w)[0] # retrieve synset information for column word and for row word\n",
    "            wn_ww = get_synset(ww)[0]\n",
    "            sim = wn.similarity.wup(wn_w, wn_ww, True) # compute similarity score\n",
    "            nouns_wup[ww].loc[w] = sim # change value in cell\n",
    "        except:\n",
    "            nouns_wup[ww].loc[w] = np.NaN # if there is no score, return \"None\"\n",
    "            \n",
    "            \n",
    "# round all values to 3 decimals \n",
    "nouns_wup = nouns_wup.round(3)\n",
    "\n",
    "# to fill dataframe with similarity scores (\"path\" function)\n",
    "\n",
    "# iterate over columns and rows\n",
    "for w in nouns:\n",
    "    for ww in nouns:\n",
    "        try: \n",
    "            wn_w = get_synset(w)[0] # retrieve synset information for column word and for row word\n",
    "            wn_ww = get_synset(ww)[0]\n",
    "            sim = wn.similarity.path(wn_w, wn_ww, True) # compute similarity score\n",
    "            nouns_sim[ww].loc[w] = sim # change value in cell\n",
    "        except:\n",
    "            nouns_sim[ww].loc[w] = np.NaN # if there is no score, return \"None\"\n",
    "\n",
    "# round all values to 3 decimals \n",
    "nouns_sim = nouns_sim.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c1a27-410a-48fb-82ce-add4702c9688",
   "metadata": {},
   "source": [
    "Our matrix-like data frames now look as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2664,
   "id": "883668e1-4b33-4fac-8647-ef83b789fa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abzockerei</th>\n",
       "      <th>aktivismus</th>\n",
       "      <th>aktivist</th>\n",
       "      <th>aktivistin</th>\n",
       "      <th>alarm</th>\n",
       "      <th>alarmist</th>\n",
       "      <th>anbeter</th>\n",
       "      <th>apokalypse</th>\n",
       "      <th>apokalyptiker</th>\n",
       "      <th>apostel</th>\n",
       "      <th>...</th>\n",
       "      <th>zar</th>\n",
       "      <th>zerrüttung</th>\n",
       "      <th>zerstörer</th>\n",
       "      <th>zerstörung</th>\n",
       "      <th>zeugs</th>\n",
       "      <th>zipfel</th>\n",
       "      <th>zirkus</th>\n",
       "      <th>zunft</th>\n",
       "      <th>zwang</th>\n",
       "      <th>überhitzung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abzockerei</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivismus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivist</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivistin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alarm</th>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abzockerei aktivismus  aktivist aktivistin     alarm alarmist  \\\n",
       "abzockerei        1.0        NaN  0.142857        NaN       0.2      NaN   \n",
       "aktivismus        NaN        NaN       NaN        NaN       NaN      NaN   \n",
       "aktivist     0.142857        NaN       1.0        NaN  0.142857      NaN   \n",
       "aktivistin        NaN        NaN       NaN        NaN       NaN      NaN   \n",
       "alarm             0.2        NaN  0.142857        NaN       1.0      NaN   \n",
       "\n",
       "             anbeter apokalypse apokalyptiker   apostel  ...       zar  \\\n",
       "abzockerei  0.166667        0.2           NaN      0.25  ...      0.25   \n",
       "aktivismus       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "aktivist    0.166667   0.142857           NaN  0.166667  ...  0.166667   \n",
       "aktivistin       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "alarm       0.166667        0.2           NaN      0.25  ...      0.25   \n",
       "\n",
       "           zerrüttung zerstörer zerstörung     zeugs    zipfel zirkus  \\\n",
       "abzockerei        0.2      0.25       0.25  0.142857      0.25  0.125   \n",
       "aktivismus        NaN       NaN        NaN       NaN       NaN    NaN   \n",
       "aktivist     0.142857  0.166667   0.166667  0.111111  0.166667    0.1   \n",
       "aktivistin        NaN       NaN        NaN       NaN       NaN    NaN   \n",
       "alarm             0.2      0.25       0.25  0.142857      0.25  0.125   \n",
       "\n",
       "               zunft     zwang überhitzung  \n",
       "abzockerei  0.166667      0.25         NaN  \n",
       "aktivismus       NaN       NaN         NaN  \n",
       "aktivist       0.125  0.166667         NaN  \n",
       "aktivistin       NaN       NaN         NaN  \n",
       "alarm       0.166667      0.25         NaN  \n",
       "\n",
       "[5 rows x 248 columns]"
      ]
     },
     "execution_count": 2664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2665,
   "id": "298b5b0b-092a-4058-9941-c0f7c9d84e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abzockerei</th>\n",
       "      <th>aktivismus</th>\n",
       "      <th>aktivist</th>\n",
       "      <th>aktivistin</th>\n",
       "      <th>alarm</th>\n",
       "      <th>alarmist</th>\n",
       "      <th>anbeter</th>\n",
       "      <th>apokalypse</th>\n",
       "      <th>apokalyptiker</th>\n",
       "      <th>apostel</th>\n",
       "      <th>...</th>\n",
       "      <th>zar</th>\n",
       "      <th>zerrüttung</th>\n",
       "      <th>zerstörer</th>\n",
       "      <th>zerstörung</th>\n",
       "      <th>zeugs</th>\n",
       "      <th>zipfel</th>\n",
       "      <th>zirkus</th>\n",
       "      <th>zunft</th>\n",
       "      <th>zwang</th>\n",
       "      <th>überhitzung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abzockerei</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivismus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivist</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivistin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alarm</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abzockerei aktivismus aktivist aktivistin     alarm alarmist  \\\n",
       "abzockerei        1.0        NaN     0.25        NaN  0.333333      NaN   \n",
       "aktivismus        NaN        NaN      NaN        NaN       NaN      NaN   \n",
       "aktivist         0.25        NaN      1.0        NaN      0.25      NaN   \n",
       "aktivistin        NaN        NaN      NaN        NaN       NaN      NaN   \n",
       "alarm        0.333333        NaN     0.25        NaN       1.0      NaN   \n",
       "\n",
       "             anbeter apokalypse apokalyptiker   apostel  ...       zar  \\\n",
       "abzockerei  0.285714   0.333333           NaN       0.4  ...       0.4   \n",
       "aktivismus       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "aktivist    0.285714       0.25           NaN  0.285714  ...  0.285714   \n",
       "aktivistin       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "alarm       0.285714   0.333333           NaN       0.4  ...       0.4   \n",
       "\n",
       "           zerrüttung zerstörer zerstörung zeugs    zipfel    zirkus  \\\n",
       "abzockerei   0.333333       0.4        0.4  0.25       0.4  0.222222   \n",
       "aktivismus        NaN       NaN        NaN   NaN       NaN       NaN   \n",
       "aktivist         0.25  0.285714   0.285714   0.2  0.285714  0.181818   \n",
       "aktivistin        NaN       NaN        NaN   NaN       NaN       NaN   \n",
       "alarm        0.333333       0.4        0.4  0.25       0.4  0.222222   \n",
       "\n",
       "               zunft     zwang überhitzung  \n",
       "abzockerei  0.285714       0.4         NaN  \n",
       "aktivismus       NaN       NaN         NaN  \n",
       "aktivist    0.222222  0.285714         NaN  \n",
       "aktivistin       NaN       NaN         NaN  \n",
       "alarm       0.285714       0.4         NaN  \n",
       "\n",
       "[5 rows x 248 columns]"
      ]
     },
     "execution_count": 2665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_wup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2666,
   "id": "7a8fc149-d796-46f7-81b5-133a3a433f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file \n",
    "nouns_sim.to_csv(\"../output/nouns_sim.csv\")\n",
    "nouns_wup.to_csv(\"../output/nouns_wup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f6aec-720f-41c0-af5c-3b191ba7c830",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Retrieve Words\n",
    "\n",
    "In a next step, we gather all cells with similarity scores higher than 0.5. The word combinations having a score higher than 0.5 are then saved to a dictionary from which we then create a new column in our original `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2667,
   "id": "2fd78f4c-3a18-4546-8ac1-3db2b5b3f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with keys from \n",
    "wup_sim_dict = dict.fromkeys(nouns)\n",
    "\n",
    "# for each key look up similar words and save to dict \n",
    "for key, values in wup_sim_dict.items():\n",
    "    \n",
    "    # update values such that the list of \"very similar\" words is saved for each key \n",
    "    words = nouns_wup.index[nouns_wup[key] > 0.5].tolist()\n",
    "    \n",
    "    if key in words:\n",
    "        words.remove(key) # remove the key word from the values list (since they always get score 1.0)\n",
    "        \n",
    "    wup_sim_dict[key] = words # save words to according dictionary key\n",
    "\n",
    "# do for both data frames (similarity measures)\n",
    "sim_dict = dict.fromkeys(nouns)\n",
    "\n",
    "for key, values in sim_dict.items():\n",
    "    \n",
    "    # update values such that the list of \"very similar\" words is saved for each key \n",
    "    words = nouns_sim.index[nouns_sim[key] > 0.5].tolist()\n",
    "    \n",
    "    if key in words:\n",
    "        words.remove(key) # remove the key word from the values list (since they always get score 1.0)\n",
    "    \n",
    "    sim_dict[key] = words # save words to according dictionary key\n",
    "    \n",
    "# add information to compounds data frame \n",
    "compounds['path'] = compounds.second_part.map(sim_dict)\n",
    "compounds['wup'] = compounds.second_part.map(wup_sim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b40160-d9a8-404a-90e8-709c7acfe6dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.4 Stemming of second part nouns \n",
    "Stemming is performed using the following stemmers provided via the `nltk` library: `cistem`, `porter`, `lancaster`, `snowball`\n",
    "\n",
    "We save all stemming results to our data frame to be able to combine the information of the output of all stemmers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2669,
   "id": "656c4efd-be9c-4ef1-b4d6-b7861d4858db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# initiate stemmers \n",
    "cistem = Cistem(case_insensitive=True)\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"german\")\n",
    "\n",
    "# the following functions all take a string as an input and return the according stem for each stemmer \n",
    "def stem_cistem(string):\n",
    "    return cistem.stem(string)\n",
    "\n",
    "def stem_porter(string):\n",
    "    return porter.stem(string)\n",
    "\n",
    "def stem_lancaster(string):\n",
    "    return lancaster.stem(string)\n",
    "\n",
    "def stem_snowball(string):\n",
    "    return snowball.stem(string)\n",
    "\n",
    "# apply stemmers to dataframe \n",
    "compounds['stem_cistem'] = compounds.second_part.apply(stem_cistem)\n",
    "compounds['stem_porter'] = compounds.second_part.apply(stem_porter)\n",
    "compounds['stem_lancaster'] = compounds.second_part.apply(stem_lancaster)\n",
    "compounds['stem_snowball'] = compounds.second_part.apply(stem_snowball)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09613ce-47f3-463a-9892-1f7aa402b159",
   "metadata": {},
   "source": [
    "After having applied each stemmer to our data frame, we check the output of each stemmer for duplicates. I.e. are there any compounds giving us the same stem? We conclude these compounds to be semantically related to each other in some way and therefore create a list of words with the same stem and save this list for each word to a new column `share_stemmer`. Since this is being done for each stemmer, we obtain four new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2670,
   "id": "72358c9d-bf08-4c30-ac1c-962326cb5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# for each stemming column check whether there are same entries of words that could be connected\n",
    "# get duplicates of stem column\n",
    "cistem_stem_words = compounds[compounds.duplicated(['stem_cistem'])].stem_cistem.tolist()\n",
    "# create list of duplicate words \n",
    "cistem_duplicates = compounds[pd.DataFrame(compounds.stem_cistem.tolist()).isin(cistem_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "porter_stem_words = compounds[compounds.duplicated(['stem_porter'])].stem_porter.tolist()\n",
    "# create list of duplicate words \n",
    "porter_duplicates = compounds[pd.DataFrame(compounds.stem_porter.tolist()).isin(porter_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "lancaster_stem_words = compounds[compounds.duplicated(['stem_lancaster'])].stem_lancaster.tolist()\n",
    "# create list of duplicate words \n",
    "lancaster_duplicates = compounds[pd.DataFrame(compounds.stem_lancaster.tolist()).isin(lancaster_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "snowball_stem_words = compounds[compounds.duplicated(['stem_snowball'])].stem_snowball.tolist()\n",
    "# create list of duplicate words \n",
    "snowball_duplicates = compounds[pd.DataFrame(compounds.stem_snowball.tolist()).isin(snowball_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# create new column and add common stem words there (do for every stemmer)\n",
    "\n",
    "# cistem stemmer\n",
    "compounds[\"share_cistem\"] = np.NaN\n",
    "\n",
    "# for each stem word that has duplicate in the data frame\n",
    "for stem in cistem_stem_words:\n",
    "    # for compound that has a shared stem with another compound\n",
    "    idx = compounds.index[compounds['stem_cistem'] == stem]\n",
    "    for i in idx:\n",
    "        # get list of words that share stem\n",
    "        share = compounds.second_part[compounds['stem_cistem'] == stem].tolist()\n",
    "        # save that list to the new column\n",
    "        compounds.share_cistem.loc[i] = share\n",
    "\n",
    "# porter stemmer\n",
    "compounds[\"share_porter\"] = np.NaN\n",
    "for stem in porter_stem_words:\n",
    "    idx = compounds.index[compounds['stem_porter'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_porter'] == stem].tolist()\n",
    "        compounds.share_porter.loc[i] = share\n",
    "\n",
    "# lancaster stemmer\n",
    "compounds[\"share_lancaster\"] = np.NaN\n",
    "for stem in lancaster_stem_words:\n",
    "    idx = compounds.index[compounds['stem_lancaster'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_lancaster'] == stem].tolist()\n",
    "        compounds.share_lancaster.loc[i] = share\n",
    "\n",
    "# snowball stemmer\n",
    "compounds[\"share_snowball\"] = np.NaN\n",
    "for stem in snowball_stem_words:\n",
    "    idx = compounds.index[compounds['stem_snowball'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_snowball'] == stem].tolist()\n",
    "        compounds.share_snowball.loc[i] = share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6616b-8a6d-492d-b12a-b2497ced3860",
   "metadata": {},
   "source": [
    "This is how the new columns look like ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2671,
   "id": "e17a0824-dbcb-46a6-b79f-874376653e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>path</th>\n",
       "      <th>wup</th>\n",
       "      <th>stem_cistem</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>share_cistem</th>\n",
       "      <th>share_porter</th>\n",
       "      <th>share_lancaster</th>\n",
       "      <th>share_snowball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockere</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivism</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>[jemand, irgendjemand, jeder beliebige]</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[demagoge, macher]</td>\n",
       "      <td>aktivi</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>akt</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>aktivisti</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>[Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>klimazipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>[zipfeln, zipfel, zipfels]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazipfeln, klimazipfel, klimazipfels]</td>\n",
       "      <td>[bestes Stück, Zipfel, Schwengel, Stößel, Schn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cylinder, round shape, form, shape, attribute...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>klimazirkus</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>[zirkusse, zirkus, zirkussen, zirkusses]</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazirkusse, klimazirkus, klimazirkussen, k...</td>\n",
       "      <td>[Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...</td>\n",
       "      <td>[vorweisen, vorzeigen]</td>\n",
       "      <td>[Versuch, Vorsatz, Unternehmung, Rastlosigkeit...</td>\n",
       "      <td>[bowl, sports stadium, stadium, arena, constru...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[kabarett, show]</td>\n",
       "      <td>zirku</td>\n",
       "      <td>zirku</td>\n",
       "      <td>zirk</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>klimazunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>[zunft, zünfte, zünften]</td>\n",
       "      <td>zunft</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimazunft, klimazünfte, klimazünften]</td>\n",
       "      <td>[Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...</td>\n",
       "      <td>[Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...</td>\n",
       "      <td>[Gestaltung, Erreichung, Realisierung, Verwirk...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zunf</td>\n",
       "      <td>zunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>klimazwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>[zwange, zwängen, zwangs, zwanges, zwang, zwänge]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazwange, klimazwängen, klimazwangs, klima...</td>\n",
       "      <td>[Erpressung, Nötigung, Bedingung, Auflage, Res...</td>\n",
       "      <td>[Befehlssatz, Befehlsvorrat, Befehlsrepertoire]</td>\n",
       "      <td>[Gruppierung, Clusterung, Bündelung]</td>\n",
       "      <td>[regulating, regulation, control, activity, hu...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>klimaüberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>[überhitzungen, überhitzung]</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaüberhitzungen, klimaüberhitzung]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>uberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>uberhitz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  second_part  \\\n",
       "0     klimaabzockerei   abzockerei   \n",
       "1     klimaaktivismus   aktivismus   \n",
       "2       klimaaktivist     aktivist   \n",
       "3     klimaaktivistin   aktivistin   \n",
       "4          klimaalarm        alarm   \n",
       "..                ...          ...   \n",
       "243       klimazipfel       zipfel   \n",
       "244       klimazirkus       zirkus   \n",
       "245        klimazunft        zunft   \n",
       "246        klimazwang        zwang   \n",
       "247  klimaüberhitzung  überhitzung   \n",
       "\n",
       "                                            noun_forms        lemma genus  \\\n",
       "0                           [abzockerei, abzockereien]   abzockerei     f   \n",
       "1                                         [aktivismus]   aktivismus     m   \n",
       "2                               [aktivisten, aktivist]     aktivist     m   \n",
       "3                          [aktivistinnen, aktivistin]   aktivistin     f   \n",
       "4            [alarms, alarm, alarmen, alarme, alarmes]        alarm     m   \n",
       "..                                                 ...          ...   ...   \n",
       "243                         [zipfeln, zipfel, zipfels]       zipfel     m   \n",
       "244           [zirkusse, zirkus, zirkussen, zirkusses]       zirkus     m   \n",
       "245                           [zunft, zünfte, zünften]        zunft     f   \n",
       "246  [zwange, zwängen, zwangs, zwanges, zwang, zwänge]        zwang     m   \n",
       "247                       [überhitzungen, überhitzung]  überhitzung     f   \n",
       "\n",
       "                                        compound_forms  \\\n",
       "0                 [klimaabzockerei, klimaabzockereien]   \n",
       "1                                    [klimaaktivismus]   \n",
       "2                     [klimaaktivisten, klimaaktivist]   \n",
       "3                [klimaaktivistinnen, klimaaktivistin]   \n",
       "4    [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "..                                                 ...   \n",
       "243          [klimazipfeln, klimazipfel, klimazipfels]   \n",
       "244  [klimazirkusse, klimazirkus, klimazirkussen, k...   \n",
       "245            [klimazunft, klimazünfte, klimazünften]   \n",
       "246  [klimazwange, klimazwängen, klimazwangs, klima...   \n",
       "247             [klimaüberhitzungen, klimaüberhitzung]   \n",
       "\n",
       "                                         related_words  \\\n",
       "0    [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                   []   \n",
       "2    [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                   []   \n",
       "4    [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "..                                                 ...   \n",
       "243  [bestes Stück, Zipfel, Schwengel, Stößel, Schn...   \n",
       "244  [Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...   \n",
       "245  [Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...   \n",
       "246  [Erpressung, Nötigung, Bedingung, Auflage, Res...   \n",
       "247                                                 []   \n",
       "\n",
       "                                             hypernyms  \\\n",
       "0                                  [Zinssatz, Zinsfuß]   \n",
       "1                                                   []   \n",
       "2                          [Volksvertreter, Politiker]   \n",
       "3                                                   []   \n",
       "4    [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244                             [vorweisen, vorzeigen]   \n",
       "245  [Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...   \n",
       "246    [Befehlssatz, Befehlsvorrat, Befehlsrepertoire]   \n",
       "247                                                 []   \n",
       "\n",
       "                                                 roots  \\\n",
       "0                                  [Zinssatz, Zinsfuß]   \n",
       "1                                                   []   \n",
       "2              [jemand, irgendjemand, jeder beliebige]   \n",
       "3                                                   []   \n",
       "4    [Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244  [Versuch, Vorsatz, Unternehmung, Rastlosigkeit...   \n",
       "245  [Gestaltung, Erreichung, Realisierung, Verwirk...   \n",
       "246               [Gruppierung, Clusterung, Bündelung]   \n",
       "247                                                 []   \n",
       "\n",
       "                                          en_hypernyms  ... path  \\\n",
       "0    [robbery, stealing, thieving, theft, larceny, ...  ...   []   \n",
       "1                                                  NaN  ...   []   \n",
       "2    [reformer, meliorist, crusader, social reforme...  ...   []   \n",
       "3                                                  NaN  ...   []   \n",
       "4    [fear, fearfulness, fright, emotion, feeling, ...  ...   []   \n",
       "..                                                 ...  ...  ...   \n",
       "243  [cylinder, round shape, form, shape, attribute...  ...   []   \n",
       "244  [bowl, sports stadium, stadium, arena, constru...  ...   []   \n",
       "245                                                 []  ...   []   \n",
       "246  [regulating, regulation, control, activity, hu...  ...   []   \n",
       "247                                                NaN  ...   []   \n",
       "\n",
       "                    wup  stem_cistem  stem_porter stem_lancaster  \\\n",
       "0                    []   abzockerei   abzockerei      abzockere   \n",
       "1                    []    aktivismu    aktivismu       aktivism   \n",
       "2    [demagoge, macher]       aktivi     aktivist            akt   \n",
       "3                    []    aktivisti   aktivistin     aktivistin   \n",
       "4                    []        alarm        alarm          alarm   \n",
       "..                  ...          ...          ...            ...   \n",
       "243                  []       zipfel       zipfel         zipfel   \n",
       "244    [kabarett, show]        zirku        zirku           zirk   \n",
       "245                  []         zunf        zunft          zunft   \n",
       "246                  []        zwang        zwang          zwang   \n",
       "247                  []  uberhitzung  überhitzung    überhitzung   \n",
       "\n",
       "    stem_snowball share_cistem share_porter    share_lancaster share_snowball  \n",
       "0      abzockerei          NaN          NaN                NaN            NaN  \n",
       "1      aktivismus          NaN          NaN                NaN            NaN  \n",
       "2        aktivist          NaN          NaN                NaN            NaN  \n",
       "3      aktivistin          NaN          NaN                NaN            NaN  \n",
       "4           alarm          NaN          NaN  [alarm, alarmist]            NaN  \n",
       "..            ...          ...          ...                ...            ...  \n",
       "243        zipfel          NaN          NaN                NaN            NaN  \n",
       "244        zirkus          NaN          NaN                NaN            NaN  \n",
       "245         zunft          NaN          NaN                NaN            NaN  \n",
       "246         zwang          NaN          NaN                NaN            NaN  \n",
       "247      uberhitz          NaN          NaN                NaN            NaN  \n",
       "\n",
       "[248 rows x 22 columns]"
      ]
     },
     "execution_count": 2671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ba114-d4ec-4607-a97b-f6295393b708",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.5 Compute Distance\n",
    "Additional to the stems, we now compute the distance of two strings by meanings of their letters. The `Jaro Distance` computes the similarity between two strings and is offered by `nltk`. An example output for the Lancaster Stemmer is shown below. Since the Lancaster Stemmer also puts the words **Professor** and **Presse** into relation, we retrospectively discard it for this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2673,
   "id": "8ad1467c-659f-4d5b-9acb-1a2e24f5255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apokalypse => apokalyptiker | Score: 0.872053872053872\n",
      "apokalyptiker => apokalypse | Score: 0.872053872053872\n",
      "betrug => betrüger | Score: 0.888888888888889\n",
      "betrüger => betrug | Score: 0.888888888888889\n",
      "freund => freundin | Score: 0.9166666666666666\n",
      "freundin => freund | Score: 0.9166666666666666\n",
      "gerechtigkeit => ungerechtigkeit | Score: 0.9555555555555555\n",
      "konfusion => konsens | Score: 0.8888888888888888\n",
      "konsens => konfusion | Score: 0.8888888888888888\n",
      "leugner => leugnung | Score: 0.875\n",
      "leugnung => leugner | Score: 0.875\n",
      "lüge => lügner | Score: 0.8888888888888888\n",
      "lügner => lüge | Score: 0.8888888888888888\n",
      "notfall => notlage | Score: 0.8888888888888888\n",
      "notlage => notfall | Score: 0.8888888888888888\n",
      "presse => professor | Score: 0.9047619047619048\n",
      "professor => presse | Score: 0.9047619047619048\n",
      "propaganda => propagandafilm | Score: 0.8809523809523809\n",
      "propagandafilm => propaganda | Score: 0.8809523809523809\n",
      "propagandafilm => propaganda | Score: 0.8809523809523809\n",
      "propaganda => propagandafilm | Score: 0.8809523809523809\n",
      "schwindel => schwindler | Score: 0.9296296296296296\n",
      "schwindler => schwindel | Score: 0.9296296296296296\n",
      "sekte => sektierer | Score: 0.9333333333333332\n",
      "sektierer => sekte | Score: 0.9333333333333332\n",
      "skeptiker => skeptikerin | Score: 0.8787878787878787\n",
      "skeptikerin => skeptiker | Score: 0.8787878787878787\n",
      "streit => streiter | Score: 0.9166666666666666\n",
      "streiter => streit | Score: 0.9166666666666666\n",
      "ungerechtigkeit => gerechtigkeit | Score: 0.9555555555555555\n",
      "zerstörer => zerstörung | Score: 0.9\n",
      "zerstörung => zerstörer | Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Compute Jaro Distance of stems \n",
    "\n",
    "dist_cistem = [] # initiate empty list \n",
    "\n",
    "# iterate over all rows in columns\n",
    "for w in compounds.stem_cistem:\n",
    "    for ww in compounds.stem_cistem:\n",
    "        \n",
    "        # compute distance score for all possible combinations\n",
    "        dist = distance.jaro_similarity(w, ww) \n",
    "        \n",
    "        # if distance score is between 0.87 and 1.0 (indicating exact same string)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            # retrieve complete words \n",
    "            word1 = compounds[compounds.stem_cistem == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_cistem == ww].second_part.values[0]\n",
    "            dist_cistem.append([word1, word2]) # and save to list\n",
    "            \n",
    "# Do this for all stem columns \n",
    "dist_porter = []\n",
    "for w in compounds.stem_porter:\n",
    "    for ww in compounds.stem_porter:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_porter == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_porter == ww].second_part.values[0]\n",
    "            dist_porter.append([word1, word2])\n",
    "                   \n",
    "dist_snowball = []\n",
    "for w in compounds.stem_snowball:\n",
    "    for ww in compounds.stem_snowball:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_snowball == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_snowball == ww].second_part.values[0]\n",
    "            dist_snowball.append([word1, word2])\n",
    "            \n",
    "            \n",
    "# to exclude: some combinations are not as accurate   \n",
    "# Presse - Professor should not be connected \n",
    "dist_lancaster = []\n",
    "for w in compounds.stem_lancaster:\n",
    "    for ww in compounds.stem_lancaster:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_lancaster == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_lancaster == ww].second_part.values[0]\n",
    "            print(word1,\"=>\", word2, \"| Score:\", dist)\n",
    "            dist_lancaster.append([word1, word2])\n",
    "            \n",
    "            \n",
    "# combine findings\n",
    "dist_stemmer = dist_cistem + dist_porter + dist_snowball\n",
    "\n",
    "# get unique values \n",
    "dist_stemmer = [list(x) for x in {tuple(x) for x in dist_stemmer}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bea7d-fcde-4970-ac9b-99d03610eac2",
   "metadata": {},
   "source": [
    "After having retrieved the string similarity scores, we save the scores to the `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2674,
   "id": "802bee02-b733-438d-af94-a689b01aa37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# apply list of distances to data frame \n",
    "compounds[\"dist_stemmer\"] = np.NaN\n",
    "\n",
    "# for each word combination\n",
    "for w in dist_stemmer:\n",
    "    # retrieve index of first element \n",
    "    idx = compounds.index[compounds['second_part'] == w[0]][0]\n",
    "    # save both elements to new column \"dist_stemmer\"\n",
    "    compounds.dist_stemmer.loc[idx] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d920e7-d982-497c-845f-7351d1d75097",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.6 Combine Related Words\n",
    "In a next step, we combine the information we just retrieved by looking for *stem duplicates* with the information we got from the *path similarity* and the computation of *distance*. This leads us to a list of **similar words** which are either similar based on the similarity score or on the stem of the word. This information is saved to a new column `similar_words`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2675,
   "id": "cdbe2fea-30b1-4c45-989d-393720e2b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the following columns into new column \"similar_words\"\n",
    "\n",
    "# map the columns\n",
    "mapping = {\"wup\": \"similar_words\",\n",
    "           \"share_cistem\": \"similar_words\",\n",
    "           \"share_porter\": \"similar_words\",\n",
    "           \"share_lancaster\": \"similar_words\",\n",
    "           \"dist_stemmer\": \"similar_words\"}\n",
    "\n",
    "# and save to new column\n",
    "compounds[\"similar_words\"] = compounds.groupby(mapping, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2676,
   "id": "6f2eb33d-67ff-4f6c-9b03-8920b6e0a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>stem_cistem</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>share_cistem</th>\n",
       "      <th>share_porter</th>\n",
       "      <th>share_lancaster</th>\n",
       "      <th>share_snowball</th>\n",
       "      <th>dist_stemmer</th>\n",
       "      <th>similar_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockere</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivism</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivismus, aktivist]</td>\n",
       "      <td>[aktivismus, aktivist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>[jemand, irgendjemand, jeder beliebige]</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivi</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>akt</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivist, aktivismus]</td>\n",
       "      <td>[demagoge, macher, aktivist, aktivismus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivisti</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivistin, aktivist]</td>\n",
       "      <td>[aktivistin, aktivist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>[Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>[alarm, alarmist, alarm, alarmist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          original second_part                                 noun_forms  \\\n",
       "0  klimaabzockerei  abzockerei                 [abzockerei, abzockereien]   \n",
       "1  klimaaktivismus  aktivismus                               [aktivismus]   \n",
       "2    klimaaktivist    aktivist                     [aktivisten, aktivist]   \n",
       "3  klimaaktivistin  aktivistin                [aktivistinnen, aktivistin]   \n",
       "4       klimaalarm       alarm  [alarms, alarm, alarmen, alarme, alarmes]   \n",
       "\n",
       "        lemma genus                                     compound_forms  \\\n",
       "0  abzockerei     f               [klimaabzockerei, klimaabzockereien]   \n",
       "1  aktivismus     m                                  [klimaaktivismus]   \n",
       "2    aktivist     m                   [klimaaktivisten, klimaaktivist]   \n",
       "3  aktivistin     f              [klimaaktivistinnen, klimaaktivistin]   \n",
       "4       alarm     m  [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "\n",
       "                                       related_words  \\\n",
       "0  [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                 []   \n",
       "2  [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                 []   \n",
       "4  [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "\n",
       "                                           hypernyms  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2                        [Volksvertreter, Politiker]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "\n",
       "                                               roots  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2            [jemand, irgendjemand, jeder beliebige]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...   \n",
       "\n",
       "                                        en_hypernyms  ... stem_cistem  \\\n",
       "0  [robbery, stealing, thieving, theft, larceny, ...  ...  abzockerei   \n",
       "1                                                NaN  ...   aktivismu   \n",
       "2  [reformer, meliorist, crusader, social reforme...  ...      aktivi   \n",
       "3                                                NaN  ...   aktivisti   \n",
       "4  [fear, fearfulness, fright, emotion, feeling, ...  ...       alarm   \n",
       "\n",
       "  stem_porter stem_lancaster stem_snowball share_cistem share_porter  \\\n",
       "0  abzockerei      abzockere    abzockerei          NaN          NaN   \n",
       "1   aktivismu       aktivism    aktivismus          NaN          NaN   \n",
       "2    aktivist            akt      aktivist          NaN          NaN   \n",
       "3  aktivistin     aktivistin    aktivistin          NaN          NaN   \n",
       "4       alarm          alarm         alarm          NaN          NaN   \n",
       "\n",
       "     share_lancaster share_snowball            dist_stemmer  \\\n",
       "0                NaN            NaN                     NaN   \n",
       "1                NaN            NaN  [aktivismus, aktivist]   \n",
       "2                NaN            NaN  [aktivist, aktivismus]   \n",
       "3                NaN            NaN  [aktivistin, aktivist]   \n",
       "4  [alarm, alarmist]            NaN       [alarm, alarmist]   \n",
       "\n",
       "                              similar_words  \n",
       "0                                        []  \n",
       "1                    [aktivismus, aktivist]  \n",
       "2  [demagoge, macher, aktivist, aktivismus]  \n",
       "3                    [aktivistin, aktivist]  \n",
       "4        [alarm, alarmist, alarm, alarmist]  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca69a25-cc82-4563-8325-0ec51d3982a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Named Entity Recognition\n",
    "In this section we will work with the context data frames that we created by retrieving the concordances in R, i.e. `pro_context` and `con_context` to extract entities that are used in the context of the compound words.\n",
    "\n",
    "They contain, inter alia, the following columns:\n",
    "- `pre`: contains up to 5 sentences that appear to the left of the key word phrase\n",
    "- `keyword`: the sentence containing the key word\n",
    "- `post`: contains up to 5 sentences that appear to the right of the key word phrase\n",
    "- `pattern`: the key word \n",
    "\n",
    "The extraction of named entities seeks to identify entities such as **persons** and **organisations** from the input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8983a3e-a3da-4137-8faf-3e5e27f39d53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.1 Preprocessing\n",
    "\n",
    "In the following, we preprocess the csv files we retrieved from R. The function `get_full_text` takes a data frame as its input and combines the strings contained in the three columns `pre`, `keyword` and `post` to generate the full text in which a key word (in our case the compound word) is found. \n",
    "(Additionally we reset and drop the index.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "id": "ec8e0039-f903-403a-b7fe-397a74618a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get full text (combination of pre, keyword and post column) for each keyword, i.e. compound word \n",
    "\n",
    "def get_full_text(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves the full text preceding and following the keyword phrase and saves full text to new column.\n",
    "    Arg: \n",
    "        df: the data frame containing the keyword-in-context information (columns \"pre\", \"keyword\" and \"post\").\n",
    "    Returns: \n",
    "        The original data frame with a new column \"full\" containing the combined text columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.replace(np.nan,'',regex=True)\n",
    "    \n",
    "    # create full text for each column\n",
    "    df[\"full\"] = df[\"pre\"] + df[\"keyword\"] + df[\"post\"]\n",
    "    \n",
    "    # convert full text column to string\n",
    "    df.full = df.full.astype(str)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "id": "7c09b23f-eaf4-4e36-abd1-1004499c1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to both data frames \n",
    "pro_context = get_full_text(pro_context)\n",
    "con_context = get_full_text(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6169bdd-e8d1-4d7b-82b9-a0cfac65dac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.2 Retrieve Entities\n",
    "The following functions use spaCys Named Entity Recognition pipelines to retrieve the entities for our data frame. Additionally we create columns for `Persons` and `Organisations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "id": "a88b2282-fa79-4473-a5ad-097e9f436c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to retrieve information regarding named entities\n",
    "\n",
    "def get_ner(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text) # create spaCy nlp element\n",
    "    entities = [] # create empty list\n",
    "    try: \n",
    "        for ent in doc.ents: # iterate over entities in nlp element\n",
    "            entities.append([ent.text, ent.label_]) # and save word and according label as list\n",
    "    except:\n",
    "        entities.append(\"None\")\n",
    "        \n",
    "    return entities # return list of entities\n",
    "\n",
    "def get_persons(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities with the label \"PER\" from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The \"PER\" entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    persons = [] # create empty list\n",
    "    for label in text: # for each text-label pair \n",
    "        if label[1] == \"PER\": # if label is \"PERSON\"\n",
    "            persons.append(label[0]) # save text to list\n",
    "            \n",
    "    return set(persons) # return a set of the list to remove duplicates \n",
    "\n",
    "def get_organisations(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities with the label \"ORG\" from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The \"ORG\" entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    organisations = [] # create empty list\n",
    "    for label in text: # for each text-label pair \n",
    "        if label[1] == \"ORG\": # if label is \"ORGANISATION\"\n",
    "            organisations.append(label[0]) # save text to list\n",
    "            \n",
    "    return set(organisations) # return a set of the list to remove duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "id": "f011d850-da3d-40ff-85a5-8fe8ead8de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions to data frames - note: the following lines take a while to run\n",
    "pro_context[\"entities\"] = pro_context.full.apply(get_ner)\n",
    "con_context[\"entities\"] = con_context.full.apply(get_ner)\n",
    "\n",
    "pro_context[\"persons\"] = pro_context.entities.apply(get_persons)\n",
    "con_context[\"persons\"] = con_context.entities.apply(get_persons)\n",
    "\n",
    "pro_context[\"organisations\"] = pro_context.entities.apply(get_organisations)\n",
    "con_context[\"organisations\"] = con_context.entities.apply(get_organisations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2a2fd-ad31-45ae-a365-21158f38f95f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.3 Cleaning \n",
    "\n",
    "To evaluate and clean the entities that were retrieved via `spacy` we create a list of unique **persons** and **organisations** from both data frames and manually evaluate those lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "id": "b835a3c7-5c02-4b2d-a28e-a3f33f205366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERSONS\n",
    "# retrieve persons from both data frames\n",
    "pro_persons = pro_context.persons.apply(list).tolist()\n",
    "con_persons = con_context.persons.apply(list).tolist()\n",
    "persons = pro_persons + con_persons # concatenate lists \n",
    "persons = [pers for sublist in persons for pers in sublist] # flatten nested list\n",
    "persons = [s.strip('.') for s in persons] # clean list (remove \".\" symbol from beginning of string)\n",
    "persons = set(persons) # get list of unique persons\n",
    "\n",
    "### ORGANISATIONS\n",
    "# retrieve organizations from both data frames\n",
    "pro_org = pro_context.organisations.apply(list).tolist()\n",
    "con_org = con_context.organisations.apply(list).tolist()\n",
    "organisations = pro_org + con_org # concatenate lists \n",
    "organisations = [org for sublist in organisations for org in sublist] # flatten nested list\n",
    "organisations = [s.strip('.') for s in organisations] # clean list (remove \".\" symbol from beginning of string)\n",
    "organisations = set(organisations) # get list of unique persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "id": "8b1e2d92-4d43-492a-bdff-7c276fd45c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save lists to text files \n",
    "\n",
    "# open file in write mode\n",
    "with open('../evaluation/persons.txt', 'w') as file:\n",
    "    for item in persons:\n",
    "        # write each item on a new line\n",
    "        file.write(\"%s\\n\" % item)\n",
    "\n",
    "# open file in write mode\n",
    "with open('../evaluation/organisations.txt', 'w') as file:\n",
    "    for item in organisations:\n",
    "        # write each item on a new line\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da243964-291e-41b5-b12d-3aff12483d64",
   "metadata": {},
   "source": [
    "Afterwards, we load the cleaned lists back into Python and apply those to our data frames to make sure we only keep the cleaned entities. For this, we compare the list of organisations and persons with the list of cleaned entities and their spellings. This gives us a final list of entities (in a unified spelling) for each occurrence of the compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2140,
   "id": "d65c0b11-78fc-45e6-9ca3-951187ca5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files back into python and preprocess tables \n",
    "persons_cleaned = pd.read_csv(\"../evaluation/persons_cleaned.csv\", sep=\";\")\n",
    "# create new column for each spelling\n",
    "persons_cleaned.spellings = persons_cleaned.spellings.replace(np.nan, \" \").str.split(\", \") \n",
    "persons_cleaned = persons_cleaned.explode('spellings')\n",
    "\n",
    "organisations_cleaned = pd.read_csv(\"../evaluation/organisations_cleaned.csv\", sep=\";\")\n",
    "# create new column for each spelling\n",
    "organisations_cleaned.spellings = organisations_cleaned.spellings.replace(np.nan, \" \").str.split(\", \")\n",
    "organisations_cleaned = organisations_cleaned.explode('spellings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2145,
   "id": "9ed920e7-9cf2-4650-b9a4-87e431b9fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_organisations(df):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function compares the organisation entities with a cleaned list of organisations and retrieves the full names.\n",
    "    Arg: \n",
    "        df: a data frame containing ORG entities \n",
    "    Returns: \n",
    "        A list of cleaned ORG entities for the data frame. \n",
    "    \"\"\"\n",
    "    \n",
    "    # initiate empty list\n",
    "    ORG = []\n",
    "    \n",
    "    # for each list of \"ORG\" entities in the data frame\n",
    "    for row in df.organisations:\n",
    "        \n",
    "        # initiate empty list\n",
    "        org = []\n",
    "        \n",
    "        # for each entitiy in the row\n",
    "        for entity in row:\n",
    "\n",
    "            # replace double whitespaces by single one and strip whitespaces from left and right end\n",
    "            entity_cleaned = entity.replace(\"  \", \" \").strip()\n",
    "            \n",
    "            # if entity matches a full name in our cleaned list\n",
    "            if entity_cleaned in organisations_cleaned.organisation.tolist():\n",
    "                full_name = entity_cleaned # entity equal the full name of the person\n",
    "                \n",
    "            # if entity matches one of the spellings in cleaned list\n",
    "            elif entity_cleaned in organisations_cleaned.spellings.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = organisations_cleaned[organisations_cleaned.spellings == entity_cleaned].organisation.values[0]\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                full_name = \" \" # return empty string\n",
    "\n",
    "      \n",
    "            # append the full name to the list for each row \n",
    "            org.append(full_name)\n",
    "            \n",
    "            # remove the empty strings \n",
    "            org = [x for x in org if x != \" \"]\n",
    "\n",
    "        # append the list of full names of each row to the final list\n",
    "        ORG.append(set(org))\n",
    "\n",
    "    # return full list of names \n",
    "    return ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2161,
   "id": "22a8a102-ad64-4ce6-9ef9-a744fff9921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"ORG\"] = clean_organisations(pro_context)\n",
    "con_context[\"ORG\"] = clean_organisations(con_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2557,
   "id": "204344b1-ce86-4b08-9866-f3da950a7f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve PERSON entities\n",
    "\n",
    "def clean_persons(df):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function compares the person entities with a cleaned list of persons and retrieves the full names.\n",
    "    Arg: \n",
    "        df: a data frame containing PER entities \n",
    "    Returns: \n",
    "        A list of cleaned PER entities for the data frame. \n",
    "    \"\"\"\n",
    "\n",
    "    # initiate empty list\n",
    "    PERS = []\n",
    "\n",
    "    # for each list of \"PERS\" entities in the data frame\n",
    "    for row in df.persons:\n",
    "        \n",
    "        # initiate empty list\n",
    "        pers = []\n",
    "        \n",
    "        # for each entitiy in the row\n",
    "        for entity in row:\n",
    "\n",
    "            # normalize string: replace full stop symbols \n",
    "            entity_cleaned = entity.replace(\".\",\" \")\n",
    "            \n",
    "            # replace double whitespaces by single one and strip whitespaces from left and right end\n",
    "            entity_cleaned = entity_cleaned.replace(\"  \", \" \").strip()\n",
    "            \n",
    "            # remove digits\n",
    "            entity_cleaned = ''.join((x for x in entity_cleaned if not x.isdigit()))\n",
    "\n",
    "            # if entity matches a full name in our cleaned list\n",
    "            if entity_cleaned in persons_cleaned.full.tolist():\n",
    "                full_name = entity_cleaned # entity equal the full name of the person\n",
    "\n",
    "            # if entity matches a last name in our cleaned list\n",
    "            elif entity_cleaned in persons_cleaned.last_name.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.last_name == entity_cleaned].full.values[0]\n",
    "\n",
    "            # if entity matches one of the spellings in cleaned list\n",
    "            elif entity_cleaned in persons_cleaned.spellings.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.spellings == entity_cleaned].full.values[0]\n",
    "                \n",
    "            # if entity is in potential genitive form (has an \"s\" at the end), e.g. \"Greta Thunbergs\"\n",
    "            # strip the s from the end of the string and check again for matches to full names \n",
    "            elif entity_cleaned.rstrip(\"s\") in persons_cleaned.full.tolist():\n",
    "                full_name = entity_cleaned.strip(\"s\")\n",
    "\n",
    "            # if entity is in potential genitive form (has an \"s\" at the end), e.g. \"Greta Thunbergs\"\n",
    "            # strip the s from the end of the string and check again for matches to last names\n",
    "            elif entity_cleaned.rstrip(\"s\") in persons_cleaned.last_name.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.last_name == (entity_cleaned.strip(\"s\"))].full.values[0]\n",
    "\n",
    "            # if no match is found\n",
    "            else:\n",
    "                full_name = \" \" # return empty string\n",
    "\n",
    "            # append the full name to the list for each row \n",
    "            pers.append(full_name)\n",
    "            \n",
    "            # remove the empty strings \n",
    "            pers = [x for x in pers if x != \" \"]\n",
    "\n",
    "        # append the list of full names of each row to the final list\n",
    "        PERS.append(set(pers))\n",
    "\n",
    "    # return full list of names \n",
    "    return PERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2558,
   "id": "0d1d5722-e60a-490e-b347-407fd271242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"PERS\"] = clean_persons(pro_context)\n",
    "con_context[\"PERS\"] = clean_persons(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db903c-6385-49fa-8e4d-fcfe63f53961",
   "metadata": {},
   "source": [
    "Save the final information we need for the definitions to the `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2566,
   "id": "0e9e4c5d-2cb8-4efe-8548-78c673e845a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty values with nan\n",
    "pro_context.PERS = pro_context.PERS.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "pro_context.ORG = pro_context.ORG.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "# convert values into list\n",
    "pro_context.PERS = pro_context.PERS.map(list, na_action='ignore')\n",
    "pro_context.ORG = pro_context.ORG.map(list, na_action='ignore')\n",
    "\n",
    "# retrieve dictionary with PERSONS and ORGANISATIONS with each compound as a key and entities as values\n",
    "pro_PERS = pro_context[pro_context.PERS.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"PERS\"].apply(list).to_dict()\n",
    "pro_ORG = pro_context[pro_context.ORG.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"ORG\"].apply(list).to_dict()\n",
    "\n",
    "# retrieve dictionary with PERSONS and ORGANISATIONS with each compound as a key and entities as values\n",
    "con_PERS = con_context[con_context.PERS.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"PERS\"].apply(list).to_dict()\n",
    "con_ORG = con_context[con_context.ORG.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"ORG\"].apply(list).to_dict()\n",
    "\n",
    "# add information to compound data frame \n",
    "compounds['PERS_pro']= compounds['original'].map(pro_PERS)\n",
    "compounds['ORG_pro']= compounds['original'].map(pro_ORG)\n",
    "\n",
    "compounds['PERS_con']= compounds['original'].map(con_PERS)\n",
    "compounds['ORG_con']= compounds['original'].map(con_ORG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcbb687-e594-4161-a41a-333fcd0610a9",
   "metadata": {},
   "source": [
    "With the following function we flatten the nested lists of entities for the final knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3031,
   "id": "eb853f85-31cf-478c-8901-79ef367c6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function flattens a nested list. \n",
    "    Arg: \n",
    "        nested_list: a nested list of the format [[item1],[item2]].\n",
    "    Returns: \n",
    "        A flattened version of the nested list, e.g. [item1,item2], else NaN.\n",
    "     \"\"\"       \n",
    "    \n",
    "    try:\n",
    "        return [item for sublist in nested_list for item in sublist]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3032,
   "id": "35fd5b13-0575-4820-a406-479f4fb2b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten entity lists \n",
    "compounds[\"PERS_pro\"] = compounds.PERS_pro.apply(flatten_list)\n",
    "compounds[\"PERS_con\"] = compounds.PERS_con.apply(flatten_list)\n",
    "compounds[\"ORG_pro\"] = compounds.ORG_pro.apply(flatten_list)\n",
    "compounds[\"ORG_con\"] = compounds.ORG_con.apply(flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080abc8-e7b8-48cc-8916-e0b53816ce56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.4 Visualization \n",
    "In the following we will have a look at the specific procedure that is done by `spacys` NER pipeline. To visualize an example, let's have a look at the following sentence that we retrieved from the pro corpus: \n",
    "\n",
    "**\"Dank Greta und FFF ist endlich Bewegung in den Stillstand bei der Klimarettung gekommen.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "id": "ca7b768b-1245-46ff-9057-d3e935d76610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex1 = \"Gerrit Hansen von der Klimaaktivistengruppe Germanwatch.\"\n",
    "ex2 = \"Dank Greta und FFF ist endlich Bewegung in den Stillstand bei der Klimarettung gekommen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "id": "892d85b1-1a80-4524-8c1c-0e9f006ac77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greta 5 10 PER\n",
      "FFF 15 18 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(ex2) # create spacy nlp object\n",
    "\n",
    "# for each entity\n",
    "for e in doc.ents:\n",
    "    # print text, start and end character, label\n",
    "    print(e.text, e.start_char, e.end_char, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "id": "a60259d6-449a-4389-906b-5415cbc5b600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                   BIO Tag         Entity               \n",
      "Dank                      O                                    \n",
      "Greta                     B               PER                  \n",
      "und                       O                                    \n",
      "FFF                       B               ORG                  \n",
      "ist                       O                                    \n",
      "endlich                   O                                    \n",
      "Bewegung                  O                                    \n",
      "in                        O                                    \n",
      "den                       O                                    \n",
      "Stillstand                O                                    \n",
      "bei                       O                                    \n",
      "der                       O                                    \n",
      "Klimarettung              O                                    \n",
      "gekommen                  O                                    \n",
      ".                         O                                    \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':{23}} {'BIO Tag':{15}} {'Entity':{20}} \")\n",
    "# for each token\n",
    "for token in doc:\n",
    "    # print BIO tag and entity information\n",
    "    print(f\"{token.text:{25}} {token.ent_iob_:{13}}   {token.ent_type_:{20}} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "id": "61365225-08f3-413b-9047-0b5982bf720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dank \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " und \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FFF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " ist endlich Bewegung in den Stillstand bei der Klimarettung gekommen .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the entities\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0840f9-72d9-464c-a515-04710ce206af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Dependency Parsing\n",
    "In this section we apply dependency parsing ot the context of the compounds to potentially obtain modifiers that further specify the compound words (in other words, they are dependent on the compound words).\n",
    "\n",
    "For a full list of German/English dependency labels please see:\n",
    "\n",
    "https://github.com/explosion/spaCy/blob/master/spacy/glossary.py  \n",
    "https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/annotation/tiger_scheme-syntax.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1547842-c96e-4bc2-8ce2-679e21615730",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.1 Retrieve Dependencies\n",
    "\n",
    "Lets retrieve dependency information with the `spacy` library for our context data frames. \n",
    "\n",
    "The function `get_dependencies` retrieves all modifiers recursively to also check for cases such as found in: \n",
    "\n",
    "**\"Über den weltweit bekanntesten (und wohl aggressivsten) Klimaaktivisten Bill McKibben\"**\n",
    "\n",
    "We want *weltweit*, *bekanntesten*, *wohl* and *aggressivsten* to be found as well. Therefore the following function takes heads which are modifiers of the compound and looks recursively for their dependents too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3544,
   "id": "06dcafa9-dcca-470f-9f19-be7ca4271c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dependencies(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function recursively retrieves the dependencies of words being dependent of a compound word. \n",
    "    Arg: \n",
    "        df: a data frame containing the compound words for which we want to count the POS tags of the dependent words.\n",
    "    Returns: \n",
    "        A nested list consisting of the compound words as keys and the dependency information, \n",
    "        i.e. the dependent word, the POS tag of the dependent word, the dependency tag.\n",
    "    \"\"\"   \n",
    "    \n",
    "    #deps = dict.fromkeys(set(df.pattern)) # initiate a dictionary with the compounds as keys\n",
    "    deps = []\n",
    "    mods = [\"mo\", \"mnr\", \"nk\"] # the list of dependency tags we are interested in\n",
    "    pos = [\"ADJA\", \"ADJD\", \"PAV\", \"PROAV\", \"PDAT\", \"PIAT\", \"PIDAT\",\n",
    "          \"PPOSAT\", \"PRELAT\", \"PTKA\", \"PWAT\", \"PWAV\", \"ADJ\", \"ADP\", \"ADV\"] # the list of POS tags we are interested in\n",
    "    \n",
    "    # iterate over rows of data frame \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        doc = nlp(row[\"keyword\"]) # retrieve sentence with keyword for according row\n",
    "        \n",
    "        tok_deps = [] # initiate empty list of dependencies for that row\n",
    "        \n",
    "        # for each token in the sentence \n",
    "        for token in doc:\n",
    "        \n",
    "            # check for words being dependent on our compound word, i.e. the compound is the head \n",
    "            if str(token.head.text).lower().startswith(row[\"pattern\"].lower()):\n",
    "                \n",
    "                # if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                if token.dep_ in mods and token.tag_ in pos:\n",
    "                        \n",
    "                    # append information (lemma, token, pos tag, dependency tag, head word) to dependency list\n",
    "                    tok_deps.append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text])\n",
    "                    \n",
    "                    # use the new found word as new head \n",
    "                    new_head = token.text\n",
    "                    \n",
    "                    # check if we have words that are dependent on our new head\n",
    "                    for token in doc:\n",
    "                        \n",
    "                        # if yes\n",
    "                        if token.head.text == new_head:\n",
    "                            \n",
    "                            # and if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                            if token.dep_ in mods and token.tag_ in pos:\n",
    "                                \n",
    "                                # append it to our list\n",
    "                                tok_deps.append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text])\n",
    "\n",
    "\n",
    "            # if we have a conjunct of one of the modifiers and another modifier\n",
    "            if token.head.text == \"und\" and token.tag_ in pos and token.dep_ == \"cj\":\n",
    "                \n",
    "                # append to list\n",
    "                tok_deps.append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text])\n",
    "                \n",
    "                # use token as new head \n",
    "                next_head = token.text\n",
    "                \n",
    "                # check for dependent words \n",
    "                for token in doc:\n",
    "                    \n",
    "                    # if yes\n",
    "                    if token.head.text == next_head: \n",
    "                        \n",
    "                        # and if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                        if token.dep_ in mods and token.tag_ in pos:\n",
    "                                \n",
    "                            # append information to list\n",
    "                            tok_deps.append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text])\n",
    "\n",
    "            # if none of the options apply, move on\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        # append to final list \n",
    "        deps.append(tok_deps)\n",
    "        \n",
    "    # return final list with dependency information for each row in the data frame\n",
    "    return deps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fe5cb-7f25-4a3f-b626-d242dbb0dad9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2 Retrieve Modifiers from Dependencies\n",
    "\n",
    "Next, since the `get_dependencies` function outputs a dictionary full of dependency information for each compound, we use the function `get_mods` to retrieve the modifiers from the `get_dependencies` output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3546,
   "id": "4cd40f7a-ebac-45cf-b0a0-5fbba8b05b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mods(column):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the modifiers of the column created via the \"get_dependencies\" above. \n",
    "    Arg: \n",
    "        column: a data frame column containing list of dependencies from which we want to retrieve the very first element.\n",
    "    Returns: \n",
    "        A list of modifier words for each compound word if possible, else 0.\n",
    "    \"\"\" \n",
    "    \n",
    "    mods = [] # initiate empty list\n",
    "    \n",
    "    # for each list of dependencies\n",
    "    for deps in column:\n",
    "        \n",
    "        # if list ist empty\n",
    "        if column == \"[]\":\n",
    "            return 0 # return 0\n",
    "           \n",
    "        else:\n",
    "            # if we have the string \"innen\" do not append to modifiers\n",
    "            if deps[0] == \"innen\":\n",
    "                pass\n",
    "            \n",
    "            # else \n",
    "            else:\n",
    "            # get first element and save to new list\n",
    "                mods.append(deps[0])\n",
    "            \n",
    "    return mods # return list of modifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760b532-8c66-46cd-bff5-4fe89fefd04f",
   "metadata": {},
   "source": [
    "Let's apply both functions and save the dependency information to a new column `dependencies` and the list of modifiers to a new column `modifiers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3553,
   "id": "da7da8f8-8c84-40f5-99d1-e3028cb8444a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply function to data frames \n",
    "con_deps = get_dependencies(con_context)\n",
    "pro_deps = get_dependencies(pro_context)\n",
    "\n",
    "# retrieve dependencies and save to new column in our data frame \n",
    "pro_context[\"dependencies\"] = pro_deps\n",
    "con_context[\"dependencies\"] = con_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3578,
   "id": "b988d6fc-17bc-4728-82d0-0f91964c138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply modifier function to find all modifiers and save to new column \"modifiers\"\n",
    "pro_context[\"modifiers\"] = pro_context.dependencies.apply(get_mods)\n",
    "con_context[\"modifiers\"] = con_context.dependencies.apply(get_mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e42ac-08aa-48b7-a749-214c29ce5f11",
   "metadata": {},
   "source": [
    "Add information the the `compounds` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3607,
   "id": "8945c3dd-49ec-4bfa-820c-5a700667b687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert modifiers column into dictionary\n",
    "pro_mods = pro_context.groupby(\"pattern\")[\"modifiers\"].apply(list).to_dict()\n",
    "con_mods = con_context.groupby(\"pattern\")[\"modifiers\"].apply(list).to_dict()\n",
    "\n",
    "# map to compounds data frame\n",
    "compounds[\"pro_mods\"] = compounds.original.map(pro_mods)\n",
    "compounds[\"pro_mods\"] = compounds.pro_mods.apply(flatten_list)\n",
    "compounds[\"con_mods\"] = compounds.original.map(con_mods)\n",
    "compounds[\"con_mods\"] = compounds.con_mods.apply(flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e6ab9-e6ae-48a6-a821-1667fcb4d11f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.3 Cleaning\n",
    "After the manual evaluation of the modifiers, we noticed that the list of modifiers still requires a lot of manual cleaning since some words contained in the modifiers column are neither adverbs nor adjectives (e.g. \"Flashcrash\") or are not useful for the definition phrasing part. \n",
    "Hence, we will manually clean the list of modifiers and load it back into Python to update the data frame with the cleaned version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626d3143-365c-4f4c-a322-91612009f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cleaned list of modifiers\n",
    "mods_cleaned = pd.read_csv(\"../evaluation/modifiers_cleaned.csv\", index_col = 0, sep=\";\")\n",
    "\n",
    "# update knowledge base with cleaned list \n",
    "compounds = compounds.set_index('original')\n",
    "compounds.update(mods_cleaned)\n",
    "compounds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e206c-9133-4c05-9279-209868bf9723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.4 Visualization\n",
    "\n",
    "To visualize what just happened in the functions we applied before, let's have a look at the following code. We will use the previously mentioned example phrase from the C2022 corpus: \n",
    "\n",
    "**Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben**\n",
    "\n",
    "For this phrase, we will retrieve the *tokens*, *dependency labels*, the according *heads* and a brief *explanation* of the dependency tag. The ouput is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "id": "fb9c7055-ded7-4b37-86ab-0c7a6ec7a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           Dependence      Head Text             Dependency Explained \n",
      "Über            ROOT =>         Über                  root \n",
      "den             nk =>           Klimaaktivisten       noun kernel element \n",
      "weltweit        mo =>           bekanntesten          modifier \n",
      "bekanntesten    nk =>           Klimaaktivisten       noun kernel element \n",
      "und             cd =>           bekanntesten          coordinating conjunction \n",
      "wohl            mo =>           aggressivsten         modifier \n",
      "aggressivsten   cj =>           und                   conjunct \n",
      "Klimaaktivisten nk =>           Über                  noun kernel element \n",
      "Bill            pnc =>          McKibben              proper noun component \n",
      "McKibben        nk =>           Klimaaktivisten       noun kernel element \n"
     ]
    }
   ],
   "source": [
    "text = \"Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben\"\n",
    "\n",
    "print(f\"{'Token':{15}} {'Dependence':{15}} {'Head Text':{20}}  {'Dependency Explained'} \")\n",
    "# for each token in the text\n",
    "for token in nlp(text):\n",
    "    # print the dependency information\n",
    "    print(f\"{token.text:{15}} {token.dep_+' =>':{13}}   {token.head.text:{20}}  {spacy.explain(token.dep_)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f5404-7445-444a-b826-6be2f2394eed",
   "metadata": {},
   "source": [
    "Heads are tagges as `ROOT`, a full list of the remaining dependency tags can be found here: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "\n",
    "Our function `get_dependencies_recursive` now seeks to retrieve all words that are being dependent on our compound word (and that are an adjective or adverb).\n",
    "\n",
    "Accordingly, for this example, our code retrieves the words (in this order):\n",
    "\n",
    "**bekanntesten** => **weltweit**  \n",
    "**aggressivsten** => **wohl** \n",
    "\n",
    "The syntactic structure of the phrase can also be visualized as a graph of dependencies as given here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9517ac56-0220-4dd8-8e39-1abfab8d459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"de\" id=\"c0ba6246e11c4d02945832d2377f8d0d-0\" class=\"displacy\" width=\"1800\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Über</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">den</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">weltweit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">bekanntesten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">und</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">wohl</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">aggressivsten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Klimaaktivisten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Bill</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">McKibben</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-0\" stroke-width=\"2px\" d=\"M245,439.5 C245,89.5 1270.0,89.5 1270.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-1\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cd</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M730.0,441.5 L738.0,429.5 722.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-5\" stroke-width=\"2px\" d=\"M770,439.5 C770,264.5 1085.0,264.5 1085.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,441.5 L1093.0,429.5 1077.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-6\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 1275.0,2.0 1275.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,441.5 L1283.0,429.5 1267.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pnc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c0ba6246e11c4d02945832d2377f8d0d-0-8\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c0ba6246e11c4d02945832d2377f8d0d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,441.5 L1618.0,429.5 1602.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex1 = \"Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben\"\n",
    "\n",
    "doc = nlp(ex1)\n",
    "displacy.render(doc, style=\"dep\")\n",
    "\n",
    "# to save the plot please un-comment the following lines\n",
    "\n",
    "#dep_plot = displacy.render(doc, style='dep', jupyter=False)\n",
    "#output_path = Path(\"../../plots/dependency_plot.svg\")\n",
    "#output_path.open(\"w\", encoding=\"utf-8\").write(dep_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b768c-d85a-4c40-a9be-7a968cd48ee0",
   "metadata": {},
   "source": [
    "Write necessary information about the modifiers to the `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2624,
   "id": "4d42f200-d81a-4f34-a558-e62a76ec775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve dictionary with MODIFIERS with each compound as a key and entities as values\n",
    "pro_mods = pro_context.set_index('pattern').to_dict()['modifiers']\n",
    "con_mods = con_context.set_index('pattern').to_dict()['modifiers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5ee4a-2e4e-487e-942f-e23cfae87b33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Sentiment Analysis\n",
    "This section applies two sentiment models, namely `GermanBert` and `TextBlob` to the context of the compounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2449d-ae59-4f4c-a872-7309a56ed3c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5.1 German Bert \n",
    "Firstly, we apply the `GermanBert` model to the data and save the output to a new column `bert` for each of the context data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "93ee3b45-502c-436d-ac7b-a4beef85e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the polarity of the German Bert model. \n",
    "    Arg: \n",
    "        text: a string for which we want to retrieve a polarity label.\n",
    "    Returns: \n",
    "        A polarity label, i.e. \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"   \n",
    "    \n",
    "    # retrieve polarity from German Bert model\n",
    "    pol = model.predict_sentiment([str(text)])\n",
    "        \n",
    "    return pol[0] # return polarity label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "id": "11124fd5-85b6-41b9-9439-6c4333237b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frames\n",
    "pro_context[\"bert\"] = pro_context.full.apply(get_bert)\n",
    "con_context[\"bert\"] = con_context.full.apply(get_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e95e0-9d59-40de-ad05-b65f45661744",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5.2 TextBlob\n",
    "\n",
    "Next, we apply the `TextBlob` model to the data and save the output to the column `blob`. The function `convert_sentiment` seeks to convert the continuous sentiment scores that are given by the `TextBlob` model into discrete polarity labels (neutral, negative or positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dba7e3af-ecc2-4607-a8cf-732075dc0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the polarity of the TextBlob model. \n",
    "    Arg: \n",
    "        text: a string for which we want to retrieve a polarity label.\n",
    "    Returns: \n",
    "        A sentiment score, ranging from -1.0 to 1.0\n",
    "    \"\"\"   \n",
    "        \n",
    "    # retrieve sentiment from TextBlob\n",
    "    blob = TextBlob(str(text))\n",
    "    \n",
    "    return blob.sentiment[0] # return sentiment score \n",
    "        \n",
    "\n",
    "def convert_sentiment(value, threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function converts the output of get_blob into discrete polarity labels, depending on the threshold that is set. \n",
    "    Arg: \n",
    "        value: a sentiment score, ranging from -1.0 to 1.0\n",
    "        threshold: a threshold score, ranging from -1.0 to 1.0\n",
    "    Returns: \n",
    "        A polarity label, i.e. \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"   \n",
    "    \n",
    "    # if sentiment score is higher or equal to threshold\n",
    "    if value >= threshold:\n",
    "        label = \"positive\" # polarity = positive\n",
    "        \n",
    "    # if sentiment score is lower or equal to the negative version of the threshold\n",
    "    if value <= -(threshold):\n",
    "        label = \"negative\" # polarity = negative\n",
    "        \n",
    "    # if sentiment score is between the negative version and the positive version of the threshold\n",
    "    else:\n",
    "        label = \"neutral\" # polarity = neutral\n",
    "        \n",
    "    return label # return label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "id": "0043d2ad-66c5-42ab-b743-bd6f762d95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"blob\"] = pro_context.full.apply(get_blob)\n",
    "con_context[\"blob\"] = con_context.full.apply(get_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625c71b-84ab-482b-8600-8ae4e15962bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5.3 Evaluate Sentiment Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92c579-338c-4aed-8dfa-a8d43a739074",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3.1 Check Sentiment\n",
    "In the following we will retrieve those texts that obtain different polarities from the two sentiment models. The idea is to assign the sentiment for these entries manually and to see if the manual annotation is in accordance with one of the models. This could help us decide for one of the models without the need of annotating a larger sample of texts. \n",
    "\n",
    "Firstly, we convert the polarity scores we retrieved via the `TextBlob` tool into discrete labels: `negative`, `neutral`, `positive` to be able to compare them to the polarity labels we got via `GermanBert`. \n",
    "\n",
    "To see which threshold for `TextBlob` gives us the most similar polarity labels (compared to the `GermanBert` output), different threshold values (0.0, 0.1, 0.2, 0.3, 0.4) are tested for the conversion into the discrete label format. This is done in the `test_parameters` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2167,
   "id": "f5710a80-1ca7-451d-bec8-ceae2db4dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameters(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function tests different threshold parameters and applies the convert_sentiment function to retrieve an output for each threshold value. \n",
    "    It counts for each threshold value how many polarity labels are different for the TextBlob and the GermanBert output. \n",
    "    Arg: \n",
    "        df: a data frame.\n",
    "    Returns: \n",
    "        An array consisting of threshold values and the according difference count.\n",
    "    \"\"\"   \n",
    "    \n",
    "    diff_scores = [] # initiate empty list\n",
    "    \n",
    "    # for different threshold values\n",
    "    for x in np.arange(0.0, 0.50, 0.10):\n",
    "        \n",
    "        # get discrete labels\n",
    "        df[\"blob_bin\"] = df.blob.apply(convert_sentiment, threshold = round(x,2))\n",
    "        \n",
    "        # compute difference of columns\n",
    "        diff = len(df[(df['bert'] == df['blob_bin']) == False])\n",
    "        \n",
    "        # save scores to list in format: threshold, difference count\n",
    "        diff_scores.append([round(x,2), diff])\n",
    "        \n",
    "    # return minimum of difference count and list of difference counts\n",
    "    return min(diff_scores, key=lambda x: x[1]), diff_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2168,
   "id": "52a09868-e078-4e94-a9a9-273e4d3a67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to data frames \n",
    "pro_diff = test_parameters(pro_context)\n",
    "con_diff = test_parameters(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d8972-9a84-403d-b7a9-f832d27d6714",
   "metadata": {},
   "source": [
    "Now, we can have a look at the difference counts that the range of parameters gives us (the first value is the threshold parameter, the second value is the difference count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2171,
   "id": "9980d53f-cd48-469a-973f-70fe35719819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contra data frame differences:\n",
      "\n",
      "   threshold    difference count\n",
      "-----------  ------------------\n",
      "        0                   753\n",
      "        0.1                 615\n",
      "        0.2                 550\n",
      "        0.3                 519\n",
      "        0.4                 515\n",
      "\n",
      "Pro data frame differences:\n",
      "\n",
      "   threshold    difference count\n",
      "-----------  ------------------\n",
      "        0                   279\n",
      "        0.1                 212\n",
      "        0.2                 188\n",
      "        0.3                 176\n",
      "        0.4                 173\n"
     ]
    }
   ],
   "source": [
    "print(\"Contra data frame differences:\\n\\n\", tabulate(con_diff[1], headers=[\"threshold\", \"difference count\"]))\n",
    "print(\"\\nPro data frame differences:\\n\\n\", tabulate(pro_diff[1], headers=[\"threshold\", \"difference count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd410e37-a806-431e-8a43-6cef984b1cc7",
   "metadata": {},
   "source": [
    "The tables clearly show that a threshold of 0.4 would give the least differences between both sentiment models. Accordingly, we apply the conversion of the `TextBlob` labels with a threshold of 0.4. This means that all values smaller than -0.4 will be considered *negative*, all value between -0.4 and + 0.4 will be considered *neutral* and all values higher than + 0.4 are indicated as *positive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2174,
   "id": "ec174f0b-c6fe-4977-9fc8-fb06c4992c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert continuous sentiment score to discrete score \n",
    "con_context[\"blob_labels\"] = con_context.blob.apply(convert_sentiment, threshold = 0.4)\n",
    "pro_context[\"blob_labels\"] = pro_context.blob.apply(convert_sentiment, threshold = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a840db-4641-4b8b-9670-ec60ad3866eb",
   "metadata": {},
   "source": [
    "Since the sample of texts with different polarity labels is still very large, we will try to further reduce it in the next step before we start with a manual annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0291e-f7fc-43da-a766-10e396ccb9fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3.2 Retrieve main Polarity for each Klima Compound\n",
    "\n",
    "In the end, we want to retrieve the prevailing polarity for each compound. For the manual evaluation of the polarity labels we now transform the data frame into a different format and get the full text of concordances for each compound word. This is done is `convert_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2178,
   "id": "0ba52437-5cc6-488f-bf71-4895e4a952b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe(df):\n",
    "    \"\"\"\n",
    "    This function groups a data frame by their key word (pattern) and concordances (i.e. context texts). \n",
    "    Arg: \n",
    "        df: a data frame.\n",
    "    Returns: \n",
    "        A grouped data frame with a column for the key word (pattern) and a column containing all concordances for the according key word (text_by_compound)\n",
    "    \"\"\"   \n",
    "    \n",
    "    # group data frame by pattern and the full text column\n",
    "    df[\"text_by_compound\"] =  df[[\"pattern\", \"full\"]].groupby(['pattern'])['full'].transform(lambda x: '//'.join(x))\n",
    "    # drop duplicates\n",
    "    df = df[['pattern','text_by_compound']].drop_duplicates()\n",
    "    # reset index\n",
    "    df = df.reset_index()\n",
    "    # drop index column\n",
    "    df = df.drop(\"index\", axis = 1)\n",
    "    # split text \n",
    "    df[\"text_by_compound\"] = df[\"text_by_compound\"].apply(lambda x: x.split(\"//\"))\n",
    "\n",
    "    return df # return grouped data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2179,
   "id": "550615f5-3e1f-45f9-b6dd-7737befad523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to data frame\n",
    "pro_sentiment = convert_dataframe(pro_context)\n",
    "con_sentiment = convert_dataframe(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e332186-ce75-439d-91af-d2dd76daf294",
   "metadata": {},
   "source": [
    "This is how the grouped data frame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2181,
   "id": "081cb0fc-a218-491c-bb63-34d6747d8f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>text_by_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>[Werner Kirstein Universität Leipzig Kirstein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>[Die SojalatteAdabeis der grünen Stadtbiotope ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>[Von diesem Beitrag werden Sie bei LeitMedien ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>[Klimaaktivisten sind aus anderem Holz geschni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>[Es ist die marxistische WassermelonenAgenda g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pattern                                   text_by_compound\n",
       "0  klimaabzockerei  [Werner Kirstein Universität Leipzig Kirstein ...\n",
       "1  klimaaktivismus  [Die SojalatteAdabeis der grünen Stadtbiotope ...\n",
       "2    klimaaktivist  [Von diesem Beitrag werden Sie bei LeitMedien ...\n",
       "3  klimaaktivistin  [Klimaaktivisten sind aus anderem Holz geschni...\n",
       "4       klimaalarm  [Es ist die marxistische WassermelonenAgenda g..."
      ]
     },
     "execution_count": 2181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbf597-7257-4a75-ba5e-3f1cfe8ccaad",
   "metadata": {},
   "source": [
    "To get the most common polarity label for each compound, we now group the data frames by pattern and polarity label and save the information to the transformed data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2182,
   "id": "2642439f-f422-48ae-b1e1-f03539d0a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common polarity label for each compound\n",
    "pro_sentiment_bert = pro_context.groupby(['pattern'])['bert'].max().tolist()\n",
    "con_sentiment_bert = con_context.groupby(['pattern'])['bert'].max().tolist()\n",
    "pro_sentiment_blob = pro_context.groupby(['pattern'])['blob_bin'].max().tolist()\n",
    "con_sentiment_blob = con_context.groupby(['pattern'])['blob_bin'].max().tolist()\n",
    "\n",
    "# and save to new column in info data frame \n",
    "pro_sentiment[\"bert\"] = pro_sentiment_bert\n",
    "con_sentiment[\"bert\"] = con_sentiment_bert\n",
    "\n",
    "pro_sentiment[\"blob\"] = pro_sentiment_blob\n",
    "con_sentiment[\"blob\"] = con_sentiment_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558a5ad-dde6-45dc-99a7-8cc9065a9fe0",
   "metadata": {},
   "source": [
    "The polarity labels that we retrieved before are now given in our tranformed data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2183,
   "id": "b432ec42-24ae-4333-99a8-8ecdef90bfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>text_by_compound</th>\n",
       "      <th>bert</th>\n",
       "      <th>blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>[Aber das macht nichts . Denn Du kannst trotzd...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>[Why should I be studying for a future that so...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>[Why should I be studying for a future that so...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaasyl</td>\n",
       "      <td>[Sie haben im Januar eine Podiumsdiskussion or...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaaufschrei</td>\n",
       "      <td>[Beispielsweise mittels zivilem Ungehorsam und...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pattern                                   text_by_compound  \\\n",
       "0  klimaaktivismus  [Aber das macht nichts . Denn Du kannst trotzd...   \n",
       "1    klimaaktivist  [Why should I be studying for a future that so...   \n",
       "2  klimaaktivistin  [Why should I be studying for a future that so...   \n",
       "3        klimaasyl  [Sie haben im Januar eine Podiumsdiskussion or...   \n",
       "4   klimaaufschrei  [Beispielsweise mittels zivilem Ungehorsam und...   \n",
       "\n",
       "       bert     blob  \n",
       "0   neutral  neutral  \n",
       "1  positive  neutral  \n",
       "2  positive  neutral  \n",
       "3   neutral  neutral  \n",
       "4   neutral  neutral  "
      ]
     },
     "execution_count": 2183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c2360-1068-4b5b-bf85-61ec21f6b120",
   "metadata": {},
   "source": [
    "Next, we want to retrieve for which compound words the polarity labels we obtained from `TextBlob` and `GermanBert` differ and save those rows to a csv file (`con_sentiment_diff_by_compound.csv` and `pro_sentiment_diff_by_compound.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2184,
   "id": "df9dccad-3b0a-4259-be5e-348ce579fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve columns where we have different sentiment scores \n",
    "con_sentiment_diff = con_sentiment[(con_sentiment['bert'] == con_sentiment['blob']) == False]\n",
    "pro_sentiment_diff = pro_sentiment[(pro_sentiment['bert'] == pro_sentiment['blob']) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d350-8aad-4c3d-a2e1-2d849a371523",
   "metadata": {},
   "source": [
    "This is the case for 13 compounds for the P2022 and 56 compound for the C2022 corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2188,
   "id": "81251fa6-af95-4de7-8cf0-a37af639e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds with different sentiment labels (P2022): 13\n",
      "Number of compounds with different sentiment labels (C2022): 56\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of compounds with different sentiment labels (P2022):\", len(pro_sentiment_diff))\n",
    "print(\"Number of compounds with different sentiment labels (C2022):\", len(con_sentiment_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2190,
   "id": "ecc1f047-9062-4196-b069-c596a1a88f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new information to csv files\n",
    "#con_sentiment_diff.to_csv(\"../evaluation/con_sentiment_diff_by_compound.csv\", index = False)\n",
    "#pro_sentiment_diff.to_csv(\"../evaluation/pro_sentiment_diff_by_compound.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3236e-005d-4d7b-8789-df8239467e62",
   "metadata": {},
   "source": [
    "For those files we now perform a manual annotation of the sentiment. This can be found in the files `pro_sentiment_diff_manual.csv` and `con_sentiment_diff_manual.csv` in the column `manual`. Since also the manual annotation did not help in deciding for one of the models, we discard this approach of obtaining polarities. (See the thesis paper for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885934d4-1c1d-4e75-ac5b-5836c2bcd872",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5.4 Derive Connotations\n",
    "To derive the connotation of each climate compound in the discourse of climate change we exploit the simple assumption that connotation of a compound may be directly derived by the sentiment that the second constituent can be associated with. Accordingly, in the following, we will obtain a polarity label for each of the second constituents by re-applying both models and look for differences again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a5768af7-7904-47c4-aabe-ee3fc7be625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sentiment models to the second constituent of the compounds\n",
    "compounds[\"bert\"] = compounds.second_part.apply(get_bert)\n",
    "compounds[\"blob\"] = compounds.second_part.apply(get_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a9fa5-2f63-467f-88f6-bdf1c17d8d27",
   "metadata": {},
   "source": [
    "After the application of both sentiment models to the second constituent of the compounds, we figured that a lot of compounds did not receive the expected sentiment label/score.\n",
    "Accordingly, we repeat the procedure on the column containing the related words, since here we have more words that can contribute to find the prevailing sentiment of the second constituent. For some cases there are no related words available. Accordingly, for those cases we retrieve the sentiment label/score of the second constituent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b5e6dce1-0bfe-4a30-bbc4-5668ce5f4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_bert(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the sentiment labels from the GermanBert model for the related words in the compounds data frame.\n",
    "    Arg: \n",
    "        row: a row with a list of related words.\n",
    "    Returns: \n",
    "        The most common polarity label of all words in the list (GermanBert), else None.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # initiate empty lists \n",
    "    bert = []\n",
    "    if row:\n",
    "        # for each string in the list of related words\n",
    "        for string in row:\n",
    "            bert.append(get_bert(string)) # retrieve sentiment label and append to list\n",
    "        \n",
    "        bert = Counter(bert).most_common(1)[0][0] # retrieve most common sentiment label for list of related words \n",
    "        \n",
    "        return bert\n",
    "    \n",
    "    # if there is no list of related words, return None       \n",
    "    else:        \n",
    "        return \n",
    "    \n",
    "    \n",
    "def get_common_blob(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the sentiment scores from the TextBlob model for the related words in the compounds data frame.\n",
    "    Arg: \n",
    "        row: a row with a list of related words.\n",
    "    Returns: \n",
    "        The average sentiment score of all words in the list (TextBlob), else None.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # initiate empty lists \n",
    "    blob = []\n",
    "    if row:\n",
    "        # for each string in the list of related words\n",
    "        for string in row:\n",
    "            blob.append(get_blob(string)) # retrieve sentiment score and append to list\n",
    "            \n",
    "        blob = sum(blob)/len(blob) # get average score for the list of words\n",
    "        \n",
    "        return blob \n",
    "    \n",
    "    # if there is no list of related words, return None\n",
    "    else:        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "841287cd-61ea-47e6-84c9-e24ae39fa497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n"
     ]
    }
   ],
   "source": [
    "# apply bert function to the column of related words \n",
    "compounds[\"bert_related\"] = compounds.related_words.apply(get_common_bert)\n",
    "compounds.bert_related.fillna(compounds.bert, inplace=True) # if no related words are available, use sentiment label of second constituent\n",
    "\n",
    "\n",
    "# apply blob function to the column of related words \n",
    "compounds[\"blob_related\"] = compounds.related_words.apply(get_common_blob)\n",
    "compounds.blob_related.fillna(compounds.blob, inplace=True) # if no related words are available, use sentiment scores of second constituent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242da4ab-8f0e-4fa8-a17f-48ac522b812d",
   "metadata": {},
   "source": [
    "Next, we manually evaluate the sentiment of the compounds. Since in most cases `TextBlob` and `GermanBert` gave different labels and due to the fact that none of the models correctly identifies all sentiments, we decide to manually assign a polarity label to the compounds. This is done in the file `../evaluation/compounds_sentiment.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7f36967f-8770-48a8-8881-0dce37919f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_sentiment = pd.read_csv(\"../evaluation/compounds_sentiment.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "774da39a-43db-4cc7-a6f4-7c6a5fe7faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>bert</th>\n",
       "      <th>bert_related</th>\n",
       "      <th>blob</th>\n",
       "      <th>blob_related</th>\n",
       "      <th>manual_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>klimazeugs</td>\n",
       "      <td>zeugs</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>klimazipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>klimazirkus</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>klimazunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>klimazwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            original second_part      bert bert_related  blob  blob_related  \\\n",
       "0    klimaabzockerei  abzockerei  negative     negative   0.0      0.000000   \n",
       "1    klimaaktivismus  aktivismus  positive     positive   0.0      0.000000   \n",
       "2      klimaaktivist    aktivist  positive     positive   0.0      0.666667   \n",
       "3    klimaaktivistin  aktivistin  positive     positive   0.0      0.000000   \n",
       "4         klimaalarm       alarm  negative     negative   0.0      0.000000   \n",
       "..               ...         ...       ...          ...   ...           ...   \n",
       "243       klimazeugs       zeugs  negative     negative   0.0      0.000000   \n",
       "244      klimazipfel      zipfel  positive     negative   0.0      0.029412   \n",
       "245      klimazirkus      zirkus  positive     negative   0.0      0.000000   \n",
       "246       klimazunft       zunft  negative     negative   0.0      0.000000   \n",
       "247       klimazwang       zwang  negative     negative  -1.0     25.000000   \n",
       "\n",
       "    manual_sentiment  \n",
       "0           negative  \n",
       "1            neutral  \n",
       "2            neutral  \n",
       "3            neutral  \n",
       "4           negative  \n",
       "..               ...  \n",
       "243          neutral  \n",
       "244          neutral  \n",
       "245          neutral  \n",
       "246          neutral  \n",
       "247         negative  \n",
       "\n",
       "[248 rows x 7 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead12410-f541-422f-9303-78df8210f2f0",
   "metadata": {},
   "source": [
    "Let's have a look at the sentiment scores and polarity labels that we obtained by the two models: The `GermanBert` model has an accuracy of 0.56 for both columns (`bert` and `bert_related`) and `TextBlob` has an accuracy of 0.46 for the `blob` column and 0.57 for `blob_related`. For `TextBlob` we use a threshold value of 0.001 here since the manual evaluation suggests that all values higher or lower than 0.0 should be *positive* or *negative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "246f87df-b6a7-493a-beac-f962ad72aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy bert:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5443548387096774"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy bert:\")\n",
    "len(compounds_sentiment[compounds_sentiment.bert == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e4e3863d-e0f3-4925-9cf9-ae725bb753e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy bert_related:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5645161290322581"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy bert_related:\")\n",
    "len(compounds_sentiment[compounds_sentiment.bert_related == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c214d7d5-c0eb-4095-81b7-a7558802e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy blob:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46774193548387094"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_sentiment[\"blob_label\"] = compounds_sentiment.blob.apply(lambda x: convert_sentiment(x, 0.001))\n",
    "compounds_sentiment[\"blob_related_label\"] = compounds_sentiment.blob_related.apply(lambda x: convert_sentiment(x, 0.001))\n",
    "\n",
    "print(\"Accuracy blob:\")\n",
    "len(compounds_sentiment[compounds_sentiment.blob_label == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "eb916d47-28ba-4b0e-89c7-f0b0cdc0b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy blob_related:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5846774193548387"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy blob_related:\")\n",
    "len(compounds_sentiment[compounds_sentiment.blob_related_label == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a379076-d753-482b-acb3-9403ce34bda3",
   "metadata": {},
   "source": [
    "Finally, we save the manual sentiment labels to the final `compounds` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "54d460fa-f529-40d5-892a-218d13cbd5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/q0xtplm10vxb6sm32kbzrbh00000gn/T/ipykernel_82173/3252193463.py:2: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  compounds = pd.concat([compounds, compounds_sentiment.manual_sentiment], 1)\n"
     ]
    }
   ],
   "source": [
    "# append column with manual sentiment to compounds data frame\n",
    "compounds = pd.concat([compounds, compounds_sentiment.manual_sentiment], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "53a88a27-6efb-4552-b54e-7efb455c3e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>con_sarcasm</th>\n",
       "      <th>pro_attr</th>\n",
       "      <th>con_attr</th>\n",
       "      <th>tf_pro</th>\n",
       "      <th>tf_con</th>\n",
       "      <th>tfidf_pro</th>\n",
       "      <th>tfidf_con</th>\n",
       "      <th>pro_colls</th>\n",
       "      <th>con_colls</th>\n",
       "      <th>manual_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>['Zinssatz', 'Zinsfuß']</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>['jemand', 'irgendjemand', 'jeder beliebige']</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>{Self}</td>\n",
       "      <td>{External}</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "      <td>0.158891</td>\n",
       "      <td>0.608632</td>\n",
       "      <td>[lieb, indigen, Klimaziel, liebe, schon]</td>\n",
       "      <td>[bekannt, Jahr, Bill]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{Self}</td>\n",
       "      <td>{External}</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.141808</td>\n",
       "      <td>0.103716</td>\n",
       "      <td>[Tan, Philippinen, Aktivist, Howey, Uganda, Va...</td>\n",
       "      <td>[schwedisch, Greta]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>['Gunst', 'Wohlwollen', 'Geneigtheit', 'Zugewa...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.398275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Erzeugung, Flashcrash, erklären, Schulz, posa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>klimazipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>[zipfeln, zipfel, zipfels]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazipfeln, klimazipfel, klimazipfels]</td>\n",
       "      <td>[bestes Stück, Zipfel, Schwengel, Stößel, Schn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cylinder, round shape, form, shape, attribute...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>klimazirkus</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>[zirkusse, zirkus, zirkussen, zirkusses]</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazirkusse, klimazirkus, klimazirkussen, k...</td>\n",
       "      <td>[Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...</td>\n",
       "      <td>[vorweisen, vorzeigen]</td>\n",
       "      <td>['Versuch', 'Vorsatz', 'Unternehmung', 'Rastlo...</td>\n",
       "      <td>[bowl, sports stadium, stadium, arena, constru...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>klimazunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>[zunft, zünfte, zünften]</td>\n",
       "      <td>zunft</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimazunft, klimazünfte, klimazünften]</td>\n",
       "      <td>[Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...</td>\n",
       "      <td>[Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...</td>\n",
       "      <td>['Gestaltung', 'Erreichung', 'Realisierung', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>klimazwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>[zwange, zwängen, zwangs, zwanges, zwang, zwänge]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazwange, klimazwängen, klimazwangs, klima...</td>\n",
       "      <td>[Erpressung, Nötigung, Bedingung, Auflage, Res...</td>\n",
       "      <td>[Befehlssatz, Befehlsvorrat, Befehlsrepertoire]</td>\n",
       "      <td>['Gruppierung', 'Clusterung', 'Bündelung']</td>\n",
       "      <td>[regulating, regulation, control, activity, hu...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>klimaüberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>[überhitzungen, überhitzung]</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaüberhitzungen, klimaüberhitzung]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  second_part  \\\n",
       "0     klimaabzockerei   abzockerei   \n",
       "1     klimaaktivismus   aktivismus   \n",
       "2       klimaaktivist     aktivist   \n",
       "3     klimaaktivistin   aktivistin   \n",
       "4          klimaalarm        alarm   \n",
       "..                ...          ...   \n",
       "243       klimazipfel       zipfel   \n",
       "244       klimazirkus       zirkus   \n",
       "245        klimazunft        zunft   \n",
       "246        klimazwang        zwang   \n",
       "247  klimaüberhitzung  überhitzung   \n",
       "\n",
       "                                            noun_forms        lemma genus  \\\n",
       "0                           [abzockerei, abzockereien]   abzockerei     f   \n",
       "1                                         [aktivismus]   aktivismus     m   \n",
       "2                               [aktivisten, aktivist]     aktivist     m   \n",
       "3                          [aktivistinnen, aktivistin]   aktivistin     f   \n",
       "4            [alarms, alarm, alarmen, alarme, alarmes]        alarm     m   \n",
       "..                                                 ...          ...   ...   \n",
       "243                         [zipfeln, zipfel, zipfels]       zipfel     m   \n",
       "244           [zirkusse, zirkus, zirkussen, zirkusses]       zirkus     m   \n",
       "245                           [zunft, zünfte, zünften]        zunft     f   \n",
       "246  [zwange, zwängen, zwangs, zwanges, zwang, zwänge]        zwang     m   \n",
       "247                       [überhitzungen, überhitzung]  überhitzung     f   \n",
       "\n",
       "                                        compound_forms  \\\n",
       "0                 [klimaabzockerei, klimaabzockereien]   \n",
       "1                                    [klimaaktivismus]   \n",
       "2                     [klimaaktivisten, klimaaktivist]   \n",
       "3                [klimaaktivistinnen, klimaaktivistin]   \n",
       "4    [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "..                                                 ...   \n",
       "243          [klimazipfeln, klimazipfel, klimazipfels]   \n",
       "244  [klimazirkusse, klimazirkus, klimazirkussen, k...   \n",
       "245            [klimazunft, klimazünfte, klimazünften]   \n",
       "246  [klimazwange, klimazwängen, klimazwangs, klima...   \n",
       "247             [klimaüberhitzungen, klimaüberhitzung]   \n",
       "\n",
       "                                         related_words  \\\n",
       "0    [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                   []   \n",
       "2    [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                   []   \n",
       "4    [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "..                                                 ...   \n",
       "243  [bestes Stück, Zipfel, Schwengel, Stößel, Schn...   \n",
       "244  [Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...   \n",
       "245  [Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...   \n",
       "246  [Erpressung, Nötigung, Bedingung, Auflage, Res...   \n",
       "247                                                 []   \n",
       "\n",
       "                                             hypernyms  \\\n",
       "0                                  [Zinssatz, Zinsfuß]   \n",
       "1                                                   []   \n",
       "2                          [Volksvertreter, Politiker]   \n",
       "3                                                   []   \n",
       "4    [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244                             [vorweisen, vorzeigen]   \n",
       "245  [Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...   \n",
       "246    [Befehlssatz, Befehlsvorrat, Befehlsrepertoire]   \n",
       "247                                                 []   \n",
       "\n",
       "                                                 roots  \\\n",
       "0                              ['Zinssatz', 'Zinsfuß']   \n",
       "1                                                   []   \n",
       "2        ['jemand', 'irgendjemand', 'jeder beliebige']   \n",
       "3                                                   []   \n",
       "4    ['Gunst', 'Wohlwollen', 'Geneigtheit', 'Zugewa...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244  ['Versuch', 'Vorsatz', 'Unternehmung', 'Rastlo...   \n",
       "245  ['Gestaltung', 'Erreichung', 'Realisierung', '...   \n",
       "246         ['Gruppierung', 'Clusterung', 'Bündelung']   \n",
       "247                                                 []   \n",
       "\n",
       "                                          en_hypernyms  ... con_sarcasm  \\\n",
       "0    [robbery, stealing, thieving, theft, larceny, ...  ...    0.000000   \n",
       "1                                                  NaN  ...    0.000000   \n",
       "2    [reformer, meliorist, crusader, social reforme...  ...    0.010753   \n",
       "3                                                  NaN  ...    0.000000   \n",
       "4    [fear, fearfulness, fright, emotion, feeling, ...  ...    0.000000   \n",
       "..                                                 ...  ...         ...   \n",
       "243  [cylinder, round shape, form, shape, attribute...  ...    0.000000   \n",
       "244  [bowl, sports stadium, stadium, arena, constru...  ...    0.000000   \n",
       "245                                                 []  ...    0.000000   \n",
       "246  [regulating, regulation, control, activity, hu...  ...    0.000000   \n",
       "247                                                NaN  ...    0.000000   \n",
       "\n",
       "    pro_attr    con_attr tf_pro tf_con tfidf_pro tfidf_con  \\\n",
       "0        NaN         NaN      0      1       NaN  0.000000   \n",
       "1        NaN         NaN      3      1  0.015903  0.000000   \n",
       "2     {Self}  {External}     61     66  0.158891  0.608632   \n",
       "3     {Self}  {External}     24      9  0.141808  0.103716   \n",
       "4        NaN         NaN      0     38       NaN  0.398275   \n",
       "..       ...         ...    ...    ...       ...       ...   \n",
       "243      NaN         NaN      0      1       NaN  0.000000   \n",
       "244      NaN         NaN      0      6       NaN  0.068872   \n",
       "245      NaN         NaN      0      1       NaN  0.000000   \n",
       "246      NaN         NaN      0      1       NaN  0.000000   \n",
       "247      NaN         NaN      0      3       NaN  0.038371   \n",
       "\n",
       "                                             pro_colls  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2             [lieb, indigen, Klimaziel, liebe, schon]   \n",
       "3    [Tan, Philippinen, Aktivist, Howey, Uganda, Va...   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "243                                                NaN   \n",
       "244                                                NaN   \n",
       "245                                                NaN   \n",
       "246                                                NaN   \n",
       "247                                                NaN   \n",
       "\n",
       "                                             con_colls manual_sentiment  \n",
       "0                                                  NaN         negative  \n",
       "1                                                  NaN          neutral  \n",
       "2                                [bekannt, Jahr, Bill]          neutral  \n",
       "3                                  [schwedisch, Greta]          neutral  \n",
       "4    [Erzeugung, Flashcrash, erklären, Schulz, posa...         negative  \n",
       "..                                                 ...              ...  \n",
       "243                                                NaN          neutral  \n",
       "244                                                NaN          neutral  \n",
       "245                                                NaN          neutral  \n",
       "246                                                NaN          neutral  \n",
       "247                                                NaN         negative  \n",
       "\n",
       "[248 rows x 42 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ae234-0768-4f33-9a9d-2481ba79856a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 6. Self- vs. External Attribution\n",
    "To evaluate whether compounds words are used by the specific subdiscourse in terms of a self- or external attribution, the `pro_context` and `con_context` data frames will be manually examined. For this, we retrieve the concepts that we determined in `compounds` and add this information to the context data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56daad3-febe-4d50-8857-70f1ca278bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve concept information from compounds data frame to the both context data frames \n",
    "concept_dict = dict(zip(compounds.original, compounds.concept))\n",
    "\n",
    "# create copy of context data frames \n",
    "pro_sample = pro_context\n",
    "con_sample = con_context\n",
    "\n",
    "# map the concept categories to the new data frames\n",
    "pro_sample[\"concept\"] = pro_sample['pattern'].map(concept_dict) \n",
    "con_sample[\"concept\"] = con_sample['pattern'].map(concept_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcdf94-65a0-43a3-966b-e566f79c409b",
   "metadata": {},
   "source": [
    "Then we filter the data frames for `persons` and `groups` since we attempt to identify the attribution of each subdiscourse for those concept categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3198,
   "id": "d00c2e3a-d196-4780-978f-e67043763380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for persons and groups\n",
    "pro_sample = pro_sample[(pro_sample.concept == \"person\") | (pro_sample.concept == \"group\")]\n",
    "con_sample = con_sample[(con_sample.concept == \"person\") | (con_sample.concept == \"group\")]\n",
    "\n",
    "# retrieve a random subset for 80% of each compound word \n",
    "pro_sample = pro_sample[[\"pattern\",\"keyword\", \"concept\"]].groupby(\"pattern\").sample(frac=0.8, replace=False, random_state=1)\n",
    "con_sample = con_sample[[\"pattern\",\"keyword\", \"concept\"]].groupby(\"pattern\").sample(frac=0.8, replace=False, random_state=1)\n",
    "\n",
    "# add origin tags \n",
    "pro_sample[\"origin\"] = \"P2022\"\n",
    "con_sample[\"origin\"] = \"C2022\"\n",
    "\n",
    "# concatenate both data frames \n",
    "full_sample =  pd.concat([pro_sample,con_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3203,
   "id": "9a83acfc-7d34-4fb9-a96f-bf870ad15a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full sample to evaluation folder \n",
    "#full_sample.to_csv(\"../evaluation/attr_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae959924-1916-4ec7-9665-aa9e01b8ea54",
   "metadata": {},
   "source": [
    "After the manual annotation of whether a term is used to describe the own group (indicated by the tag `self`) or to refer to the opposing group (indicated by the tag `external`), we load the annotated data frame back into Python to save the information to our context data frames. The tag `None` is used if from the context phrases no attribution information could be obtained at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efd5a364-7751-4011-8ccc-7174a38a24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sample = pd.read_csv(\"../evaluation/attr_manual.csv\", sep =\";\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9676373-bf01-4911-ae2e-3a7a0643d055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>origin</th>\n",
       "      <th>pattern</th>\n",
       "      <th>keyword</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>person</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimabrandstifter</td>\n",
       "      <td>„ Wenn Sie einem Klimabrandstifter vier weiter...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>person</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimafachkraft</td>\n",
       "      <td>Doch eine Klimafachkraft mit der Ausbildung Ka...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>person</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimagott</td>\n",
       "      <td>Mit EMobil werden wir nichts ( in Worten : NIC...</td>\n",
       "      <td>Exernal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>person</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimagott</td>\n",
       "      <td>verbiegen , um dem Klimagott zu huldigen .</td>\n",
       "      <td>Exernal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>person</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimagöttin</td>\n",
       "      <td>Politiker die sie regieren und der neuen „ Kli...</td>\n",
       "      <td>Exernal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>group</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimamafia</td>\n",
       "      <td>Vor Jahren war der Mann noch nicht infiziert ,...</td>\n",
       "      <td>External</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>group</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimamafia</td>\n",
       "      <td>Jetzt wird es von der Klimamafia verwendet , u...</td>\n",
       "      <td>External</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>group</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimaplanwirtschaft</td>\n",
       "      <td>So hat uns bald dahingerafft die KlimaPlanwirt...</td>\n",
       "      <td>External</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>group</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimastaat</td>\n",
       "      <td>Aber der soziale Tod ist ihnen in dem von viel...</td>\n",
       "      <td>External</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>group</td>\n",
       "      <td>C2022</td>\n",
       "      <td>klimataliban</td>\n",
       "      <td>Befeuert noch in ihrem Wahn KleinGretas KlimaT...</td>\n",
       "      <td>External</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     concept origin              pattern  \\\n",
       "514   person  C2022    klimabrandstifter   \n",
       "571   person  C2022       klimafachkraft   \n",
       "672   person  C2022            klimagott   \n",
       "674   person  C2022            klimagott   \n",
       "676   person  C2022          klimagöttin   \n",
       "...      ...    ...                  ...   \n",
       "1215   group  C2022           klimamafia   \n",
       "1217   group  C2022           klimamafia   \n",
       "1282   group  C2022  klimaplanwirtschaft   \n",
       "1797   group  C2022           klimastaat   \n",
       "1800   group  C2022         klimataliban   \n",
       "\n",
       "                                                keyword annotation  \n",
       "514   „ Wenn Sie einem Klimabrandstifter vier weiter...       None  \n",
       "571   Doch eine Klimafachkraft mit der Ausbildung Ka...       None  \n",
       "672   Mit EMobil werden wir nichts ( in Worten : NIC...    Exernal  \n",
       "674          verbiegen , um dem Klimagott zu huldigen .    Exernal  \n",
       "676   Politiker die sie regieren und der neuen „ Kli...    Exernal  \n",
       "...                                                 ...        ...  \n",
       "1215  Vor Jahren war der Mann noch nicht infiziert ,...   External  \n",
       "1217  Jetzt wird es von der Klimamafia verwendet , u...   External  \n",
       "1282  So hat uns bald dahingerafft die KlimaPlanwirt...   External  \n",
       "1797  Aber der soziale Tod ist ihnen in dem von viel...   External  \n",
       "1800  Befeuert noch in ihrem Wahn KleinGretas KlimaT...   External  \n",
       "\n",
       "[788 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd94596a-06d1-4679-914f-6a57351a433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve attribution for both subdiscourses\n",
    "pro_attribution = annotated_sample[annotated_sample.origin == \"P2022\"].groupby(\"pattern\")[\"annotation\"].apply(set).to_dict()\n",
    "con_attribution = annotated_sample[annotated_sample.origin == \"C2022\"].groupby(\"pattern\")[\"annotation\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9e3b7cdd-4577-4b9a-bec0-cf130d4b3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and map to compounds data frame \n",
    "compounds[\"pro_attr\"] = compounds.original.map(pro_attribution)\n",
    "compounds[\"con_attr\"] = compounds.original.map(con_attribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffed73-a313-4eda-ac26-909073cf7ff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6.1 Simplistic Sarcasm Detection\n",
    "To be able to give further information about the use of each compound in the according discourse, we use the following very simplistic approach to detect sarcastic mentionings of the compounds in the key word phrases:\n",
    "The manual evaluation of section 6 suggests that if a compound appears in quotation marks it is very likely to be used sarcastically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12bfb98a-7040-4402-b9aa-c915d8f1b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quotations(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks for each key word sentence whether the compound words is mentioned in quotation marks. \n",
    "    Arg: \n",
    "        df: a data frame containing the compound and a key word phrase.\n",
    "    Returns: \n",
    "        A new column with the binary label 1 if the compound is used in quotation marks, else 0.\n",
    "    \"\"\" \n",
    "    \n",
    "    # for each row in the data frame\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        string = row['keyword'].lower() # retrieve key word phrase\n",
    "        compound = row['pattern'] # retrieve compound word\n",
    "    \n",
    "        # retrieve exact positions of compound word in the string\n",
    "        for match in re.finditer(compound, string):\n",
    "            start = match.start() # get start position\n",
    "            end = match.end() # and end position\n",
    "\n",
    "            try: \n",
    "                # if 1 position before the starting point, we have one of the following quotation marks\n",
    "                if string[start-1] == '\"' or string[start-1] == \"\"\"'\"\"\" or string[start-1] == \"\"\"„\"\"\":\n",
    "                    x = True # set x to True\n",
    "\n",
    "                # or if 2 position before the starting point, we have one of the following quotation marks\n",
    "                elif string[start-2] == \"\"\" \" \"\"\" or str1[start-2] == \"\"\"'\"\"\" or string[start-2] == \"\"\"„\"\"\":\n",
    "                    x = True # set x to True\n",
    "                    \n",
    "                # else, x is False\n",
    "                else:\n",
    "                    x = False\n",
    "\n",
    "            except:\n",
    "                x = False\n",
    "                y = False\n",
    "\n",
    "            try:\n",
    "                # if 1 position after the end point, we have one of the following quotation marks\n",
    "                if string[end+1] == '\"' or string[end+1] == \"\"\"'\"\"\" or string[end+1] == \"\"\"„\"\"\":\n",
    "                    y = True # set y to True\n",
    "                    \n",
    "                # if 2 positions after the end point, we have one of the following quotation marks\n",
    "                elif string[end+2] == '\"' or string[end+2] == \"\"\"'\"\"\" or string[end+2] == \"\"\"“\"\"\":\n",
    "                    y = True # set y to True\n",
    "                    \n",
    "                # else, y is False\n",
    "                else:\n",
    "                    y = False \n",
    "                    \n",
    "            except:\n",
    "                x = False\n",
    "                y = False\n",
    "\n",
    "            # if we have 2 quotations marks surrounding the compound\n",
    "            if x and y == True:\n",
    "                df.at[idx,'quotes'] = 1 # set value in new column to 1 \n",
    " \n",
    "            # if not:\n",
    "            else:\n",
    "                df.at[idx,'quotes'] = 0 # set value in new column to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "956eeee9-eca5-4d26-948d-4f13d0bb38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to both data frames \n",
    "find_quotations(pro_context)\n",
    "find_quotations(con_context)\n",
    "\n",
    "# convert type of number from float to integer\n",
    "pro_context.quotes = pro_context.quotes.astype('int32')\n",
    "con_context.quotes = con_context.quotes.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42569af-b5fa-4d3a-9b2d-8c6da1cadde4",
   "metadata": {},
   "source": [
    "Then, we want to compute the proportion of how often the compound word is used in quotation marks and map the information to the compounds data frame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84da91ef-ef41-49fb-a782-a56068e5905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count proportion of each compound of how many appearances are sarcastic and how many are not\n",
    "pro_sarcasm = pro_context.groupby('pattern')['quotes'].mean().to_dict()\n",
    "con_sarcasm = con_context.groupby('pattern')['quotes'].mean().to_dict()\n",
    "\n",
    "# map information for each compound to compounds data frame \n",
    "compounds[\"pro_sarcasm\"] = compounds.original.map(pro_sarcasm)\n",
    "compounds[\"con_sarcasm\"] = compounds.original.map(con_sarcasm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b22e68-ee80-4127-a528-aec9366b2dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 7. Term Frequencies\n",
    "In the following, we will add the term frequencies and the TF-IDF scores that we computed in R to our compounds data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cc09c3a8-645a-4153-9c47-1738e890dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tf data frame \n",
    "tf = pd.read_csv(\"../../R/output/tf_complete.csv\", header=1, names=[\"original\", \"tf_pro\", \"tf_con\"])\n",
    "\n",
    "# load tfidf data frame\n",
    "tf_idf = pd.read_csv(\"../../R/output/tfidf_complete.csv\", header=0, index_col =0, names=[\"original\", \"tfidf_con\", \"tfidf_pro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb8e2824-ba37-4fec-8755-25665a5b2568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>tf_pro</th>\n",
       "      <th>tf_con</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimagerechtigkeit</td>\n",
       "      <td>280</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimapäckchen</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimarettung</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  tf_pro  tf_con\n",
       "0     klimaaktivistin      24       9\n",
       "1  klimagerechtigkeit     280      16\n",
       "2       klimaaktivist      61      66\n",
       "3       klimapäckchen       9       0\n",
       "4        klimarettung      13      48"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3339975a-218e-4f88-b20c-247a17f38944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>tfidf_con</th>\n",
       "      <th>tfidf_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>0.608632</td>\n",
       "      <td>0.158891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>0.103716</td>\n",
       "      <td>0.141808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>0.398275</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          original  tfidf_con  tfidf_pro\n",
       "1  klimaabzockerei   0.000000        NaN\n",
       "2  klimaaktivismus   0.000000   0.015903\n",
       "3    klimaaktivist   0.608632   0.158891\n",
       "4  klimaaktivistin   0.103716   0.141808\n",
       "5       klimaalarm   0.398275        NaN"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c5313-51b8-468b-839d-af9493d2dc73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set index\n",
    "compounds = compounds.set_index('original')\n",
    "tf = tf.set_index('original')\n",
    "tf_idf = tf_idf.set_index('original')\n",
    "\n",
    "# append new columns\n",
    "compounds = pd.concat([compounds, tf.tf_pro, tf.tf_con, tf_idf.tfidf_pro, tf_idf.tfidf_con], 1)\n",
    "compounds.tf_pro = compounds.tf_pro.astype('Int64') # convert into integers\n",
    "compounds.tf_con = compounds.tf_con.astype('Int64') # convert into integers\n",
    "\n",
    "# reset index\n",
    "compounds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261fae8-3b9c-4fb3-bf3f-d7f65cc66ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 8. Collocations\n",
    "\n",
    "Add collocation that we retrieved in R to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f338eabe-7ab5-4e5d-a369-f1162e5cbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load collocations\n",
    "colls_pro = pd.read_csv(\"../../R/output/top_colls_pro_cleaned.csv\", sep=\";\", index_col = 0)\n",
    "colls_con = pd.read_csv(\"../../R/output/top_colls_con_cleaned.csv\", sep=\";\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0e158764-e7f9-45d0-95a5-19be72bfc1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n</th>\n",
       "      <th>keyword</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bekannt</td>\n",
       "      <td>4</td>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jahr</td>\n",
       "      <td>2</td>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill</td>\n",
       "      <td>4</td>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>schwedisch</td>\n",
       "      <td>3</td>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Greta</td>\n",
       "      <td>5</td>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>breite</td>\n",
       "      <td>2</td>\n",
       "      <td>klimawahn</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>liegen</td>\n",
       "      <td>2</td>\n",
       "      <td>klimawahnsinn</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>beenden</td>\n",
       "      <td>2</td>\n",
       "      <td>klimawahnsinn</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Deutschland</td>\n",
       "      <td>2</td>\n",
       "      <td>klimawahnsinn</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>John</td>\n",
       "      <td>2</td>\n",
       "      <td>klimazar</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  n          keyword   tag\n",
       "1        bekannt  4    klimaaktivist   pre\n",
       "2           Jahr  2    klimaaktivist   pre\n",
       "3           Bill  4    klimaaktivist  post\n",
       "4     schwedisch  3  klimaaktivistin   pre\n",
       "5          Greta  5  klimaaktivistin  post\n",
       "..           ... ..              ...   ...\n",
       "134       breite  2        klimawahn  post\n",
       "135       liegen  2    klimawahnsinn   pre\n",
       "136      beenden  2    klimawahnsinn  post\n",
       "137  Deutschland  2    klimawahnsinn  post\n",
       "138         John  2         klimazar  post\n",
       "\n",
       "[134 rows x 4 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colls_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6ac0b99d-f499-4d39-9507-16d82c5ec00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to compounds data frame\n",
    "colls_con_dict = colls_con.groupby(\"keyword\")[\"word\"].apply(list).to_dict()\n",
    "colls_pro_dict = colls_pro.groupby(\"keyword\")[\"word\"].apply(list).to_dict()\n",
    "\n",
    "compounds[\"pro_colls\"] = compounds.original.map(colls_pro_dict)\n",
    "compounds[\"con_colls\"] = compounds.original.map(colls_con_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4088183-932b-4da6-b551-1d6b447ae269",
   "metadata": {},
   "source": [
    "# TO DELETE\n",
    "\n",
    "## UPDATE DATA FRAMES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fdaa333-0cce-4175-9b34-66bbe3286e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pro_context.to_csv(\"../output/pro_info.csv\", index = False)\n",
    "#con_context.to_csv(\"../output/con_info.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1e579d-87b7-4fcf-96af-a5e720ce543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge_base.to_csv(\"../output/knowledge_base.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d2573a0-373f-4425-b4d9-0c5b41ef2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compounds = compounds.drop(['tf_pro', 'tf_con', 'tfidf_pro','tfidf_con'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff31fac-c34f-41e3-b4d9-edaabb1e2b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

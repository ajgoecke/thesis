{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38df0cc2-7334-4f73-b857-038a7569477e",
   "metadata": {},
   "source": [
    "# Requirements to run this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "id": "f8cd4f02-03c6-4dd5-849e-6db8dd5697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install wn\n",
    "#!pip3 install spacy\n",
    "#!python -m spacy download de_core_news_md\n",
    "#!pip3 install networkx\n",
    "#!pip3 install tabulate\n",
    "#!pip3 install germansentiment\n",
    "#!pip3 install -U numpy\n",
    "#!pip3 install -U textblob-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "id": "dd5ba1c4-db3e-4cbd-b6b8-8b23c2551f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# requirements \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "from tabulate import tabulate\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp.add_pipe('spacytextblob')\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import wn\n",
    "import wn.taxonomy\n",
    "#wn.download('odenet')\n",
    "#wn.download('oewn:2021')\n",
    "\n",
    "en = wn.Wordnet('oewn:2021')\n",
    "de = wn.Wordnet('odenet:1.4')\n",
    "\n",
    "from wn.similarity import path\n",
    "\n",
    "#from ast import literal_eval\n",
    "#df['POS_tags'] = df['POS_tags'].apply(literal_eval)\n",
    "\n",
    "# to convert \"['armageddon', 'untergang']\" strings that should be list of strings back to lists \n",
    "def convert_literal(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts strings of the format \"['armageddon', 'untergang']\" to list objects.\n",
    "    Arg: \n",
    "        string: a string which is meant to be a list object.\n",
    "    Returns: \n",
    "        A proper list object if input is a string, else np.nan\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "# https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff61f0-3ae7-410b-8d60-ff9c185fea6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Load Files\n",
    "\n",
    "The file `compounds.csv` is the output of the `preprocessing.ipynb` file. The file is loaded here into a pandas data frame and columns are preprocessed such that they have the correct format for the upcoming pieces of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "d4723891-a286-4d08-b8a4-51c9dc7bf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load compounds file (output of preprocessing.ipynb)\n",
    "compounds = pd.read_csv(\"../files/compounds_info.csv\")\n",
    "\n",
    "# load context files (i.e. output of corpus-based methods notebook)\n",
    "pro_context = pd.read_csv(\"../../R/output/pro_context_new.csv\")\n",
    "con_context = pd.read_csv(\"../../R/output/con_context_new.csv\")\n",
    "\n",
    "pro_colls = pd.read_csv(\"../../R/output/top_collocations_pro.csv\", index_col = \"Unnamed: 0\")\n",
    "con_colls = pd.read_csv(\"../../R/output/top_collocations_con.csv\", index_col = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2635,
   "id": "77fade38-a3ca-4f6a-82b7-e51427dcb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pd.read_csv(\"../files/compounds_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "id": "b193b968-8a0c-4ddf-8d3d-7ea0e64f923d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sim_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sim_words'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bx/q0xtplm10vxb6sm32kbzrbh00000gn/T/ipykernel_4340/848689716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# these columns are then converted into a \"proper\" list format by using the function we created previously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcompounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_literal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load csv files with similarity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sim_words'"
     ]
    }
   ],
   "source": [
    "# load required files \n",
    "\n",
    "# load compounds file (output of preprocessing.ipynb)\n",
    "compounds = pd.read_csv(\"../output/compounds_info.csv\")\n",
    "\n",
    "# when loading the csv file columns consisting of lists of strings are not loaded properly, \n",
    "# that's why we create a list of those columns here\n",
    "list_cols = [\"noun_forms\", \"compound_forms\", \"sim_words\", \"wup_sim_words\", \"related_words\", \n",
    "             \"hypernyms\", \"share_cistem\", \"share_porter\", \"share_lancaster\", \"share_snowball\", \"similar_words\"]\n",
    "\n",
    "# these columns are then converted into a \"proper\" list format by using the function we created previously\n",
    "for col in list_cols:\n",
    "    compounds[col] = compounds[col].apply(convert_literal)\n",
    "\n",
    "# load csv files with similarity scores\n",
    "#### ATTENTION! I CHANGED FILES/FOLDERS HERE!!!\n",
    "\n",
    "#nouns_wup = pd.read_csv(\"textmining/nouns_wup.csv\", index_col = 0)\n",
    "#nouns_sim = pd.read_csv(\"textmining/nouns_sim.csv\", index_col = 0)\n",
    "\n",
    "# load data frames with NER / Sentiment / DEP information\n",
    "#con_sent_df = pd.read_csv(\"textmining/con_sent_ner.csv\")\n",
    "#pro_sent_df = pd.read_csv(\"textmining/pro_sent_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714686a9-fab8-4146-811c-53f621d2dc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### USED TO OBTAIN WORD FORMS AS TXT/CSV FILE\n",
    "\n",
    "#compound_forms = compounds.compound_forms.tolist()\n",
    "#compound_forms = [item for sublist in compound_forms for item in sublist]\n",
    "\n",
    "# open file in write mode\n",
    "#with open('compound_forms.txt', 'w') as fp:\n",
    "#    for item in compound_forms:\n",
    "#        # write each item on a new line\n",
    "#        fp.write(\"%s\\n\" % item)\n",
    "#    print('Done')\n",
    "\n",
    "#compound_forms_df = compounds.loc[:,[\"original\", \"compound_forms\"]]\n",
    "#compound_forms_df.to_csv(\"compound_forms_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "id": "083574dd-15e0-4885-a987-94fb360ac490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarmes, alarme, alarm, alarmen]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarmes, klimaalarme, klima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>klimazipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>[zipfel, zipfels, zipfeln]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazipfel, klimazipfels, klimazipfeln]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>klimazirkus</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>[zirkus, zirkussen, zirkusses, zirkusse]</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazirkus, klimazirkussen, klimazirkusses, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>klimazunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>[zünfte, zunft, zünften]</td>\n",
       "      <td>zunft</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimazünfte, klimazunft, klimazünften]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>klimazwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>[zwänge, zwanges, zwang, zwängen, zwange, zwangs]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazwänge, klimazwanges, klimazwang, klimaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>klimaüberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>[überhitzung, überhitzungen]</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaüberhitzung, klimaüberhitzungen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  second_part  \\\n",
       "0     klimaabzockerei   abzockerei   \n",
       "1     klimaaktivismus   aktivismus   \n",
       "2       klimaaktivist     aktivist   \n",
       "3     klimaaktivistin   aktivistin   \n",
       "4          klimaalarm        alarm   \n",
       "..                ...          ...   \n",
       "243       klimazipfel       zipfel   \n",
       "244       klimazirkus       zirkus   \n",
       "245        klimazunft        zunft   \n",
       "246        klimazwang        zwang   \n",
       "247  klimaüberhitzung  überhitzung   \n",
       "\n",
       "                                            noun_forms        lemma genus  \\\n",
       "0                           [abzockerei, abzockereien]   abzockerei     f   \n",
       "1                                         [aktivismus]   aktivismus     m   \n",
       "2                               [aktivisten, aktivist]     aktivist     m   \n",
       "3                          [aktivistinnen, aktivistin]   aktivistin     f   \n",
       "4            [alarms, alarmes, alarme, alarm, alarmen]        alarm     m   \n",
       "..                                                 ...          ...   ...   \n",
       "243                         [zipfel, zipfels, zipfeln]       zipfel     m   \n",
       "244           [zirkus, zirkussen, zirkusses, zirkusse]       zirkus     m   \n",
       "245                           [zünfte, zunft, zünften]        zunft     f   \n",
       "246  [zwänge, zwanges, zwang, zwängen, zwange, zwangs]        zwang     m   \n",
       "247                       [überhitzung, überhitzungen]  überhitzung     f   \n",
       "\n",
       "                                        compound_forms  \n",
       "0                 [klimaabzockerei, klimaabzockereien]  \n",
       "1                                    [klimaaktivismus]  \n",
       "2                     [klimaaktivisten, klimaaktivist]  \n",
       "3                [klimaaktivistinnen, klimaaktivistin]  \n",
       "4    [klimaalarms, klimaalarmes, klimaalarme, klima...  \n",
       "..                                                 ...  \n",
       "243          [klimazipfel, klimazipfels, klimazipfeln]  \n",
       "244  [klimazirkus, klimazirkussen, klimazirkusses, ...  \n",
       "245            [klimazünfte, klimazunft, klimazünften]  \n",
       "246  [klimazwänge, klimazwanges, klimazwang, klimaz...  \n",
       "247             [klimaüberhitzung, klimaüberhitzungen]  \n",
       "\n",
       "[248 rows x 6 columns]"
      ]
     },
     "execution_count": 1764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "id": "1ac2baf0-cb77-41f6-b3ea-58081afc68ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_words = list(set(pro_context.pattern.tolist()))\n",
    "con_words = list(set(con_context.pattern.tolist()))\n",
    "\n",
    "\n",
    "len(pro_words)+len(con_words)\n",
    "final_compounds = compounds.original.tolist()\n",
    "\n",
    "missing = []\n",
    "\n",
    "for i in final_compounds:\n",
    "    if i not in pro_words and i not in con_words:\n",
    "        missing.append(i)\n",
    "        \n",
    "\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac04c34-faaf-4d9a-8b1b-40e2fb9599cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46345df8-147a-427f-a5e6-60a4f6297856",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Convert Literals\n",
    "For most of the `csv` files the literals are not evaluated correctly, i.e. columns containing a list of strings is evaluated as a string when performing `pandas` `read_csv`. This issue is addressed by applying the `literal_eval` function of the `ast` library to the columns for which we have the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2965,
   "id": "5b102c99-d93e-454f-9e28-22a65644015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds['noun_forms'] = compounds.noun_forms.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['compound_forms'] = compounds.compound_forms.apply(lambda x: literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ec2d-5d8c-4763-b5cf-4ce45f776562",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Reduce Concordances\n",
    "The context that we retrieved via the concordances (`kwic`) in R unfortunately could only be performed on regex level. Accordingly also more complex forms of the compounds are contained in out data frame. E.g. for the compound *Klimagerechtigkeit* also concordances containing the key word *Klimagerechtigkeitspolitik* were included and for the compound *Klimaalarm* also key word phrases containin the adjective form *klimaalarmistisch* were retrieved. We want to get rid of the rows not containing **exact matches** of our compound word forms. \n",
    "\n",
    "Accordingly, we search for the rows that contain one of the compound forms and retrieve those columns for the data frames which we will then use for the upcoming analyses. \n",
    "\n",
    "To do this, we first get a list of the compound word forms from our `compounds` data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "id": "29a9d078-086c-4ae9-bf77-e8c7e491bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_forms = compounds.compound_forms.tolist()\n",
    "compound_forms = [item for sublist in compound_forms for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66499c-9d4f-40b3-b259-10abc76ae983",
   "metadata": {},
   "source": [
    "Then we apply the following function from `pandas` to check whether a string is contained in the row and to retrieve the according rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "id": "fcfb7adc-3aff-429c-abe2-f72d39bc2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all columns which contain a compound form\n",
    "pro_context = pro_context[pro_context.stack().str.contains(\" |\".join(compound_forms), case=False).groupby(level=0).any()]\n",
    "\n",
    "con_context = con_context[con_context.stack().str.contains(\" |\".join(compound_forms), case=False).groupby(level=0).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "b67ae8d8-34a1-459e-863e-d69370a6bc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for text in pro_context[pro_context.keyword.str.contains(\"klimagerechtigkeit\"+\" \", case=False)].keyword:\n",
    " #   print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770aae7-80d2-4e52-ae5d-31680a57b727",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Working with WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3a031-c4b4-481e-8669-01cf4b6af22e",
   "metadata": {},
   "source": [
    "## 2.1 Exploring Hierarchical Structures (Hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fa0d8-ba73-4e83-854a-22c80fe028a6",
   "metadata": {},
   "source": [
    "We create multiple functions which we need to retrieve the synset and additional more information of the noun from the `wn` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2638,
   "id": "a215763c-22a1-4903-aec5-0d640259c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the WordNet synsets of the input string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        The synsets of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        word = de.synsets(string, pos=\"n\") # retrieve synsets for the string\n",
    "        return word\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def get_lemmas(string):\n",
    "    \"\"\"\n",
    "    Returns the WordNet lemmas of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of lemmas (i.e. related words) of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmas = [] # initiate empty list\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            lemmas.append(s.lemmas()) # retrieve lemmas and append to list\n",
    " \n",
    "        return list(set([x for l in lemmas for x in l])) # return flattened list of lemmas \n",
    "    \n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    \n",
    "def get_hypernyms(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the WordNet hypernyms of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of hypernyms of the noun if available, else empty list.\n",
    "    \"\"\"\n",
    "       \n",
    "    hypers = [] # initiate empty list\n",
    "    #hypernyms = []\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            hypernyms = s.hypernyms() # retrieve hypernyms\n",
    "            \n",
    "            # for each hypernym\n",
    "            for el in hypernyms:\n",
    "                hypers.append(el.lemmas()) # retrieve lemmas\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return list(set([y for x in hypers for y in x])) # return flattened list of hypernyms \n",
    "      \n",
    "\n",
    "# for each compound save lemmas (as related words), hypernyms, definition to compound dataframe \n",
    "compounds['related_words'] = compounds.second_part.apply(get_lemmas)\n",
    "compounds['hypernyms'] = compounds.second_part.apply(get_hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98619b-8fa4-4a2f-9b37-8d170e58e85c",
   "metadata": {},
   "source": [
    "To visualize the hierarchical tree structure of the WordNet knowledge base, we will have a quick look at the following example. Here we see the hypernyms of the word \"Betrug\" (en: \"fraud\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2639,
   "id": "898ea735-32d8-4da6-a5f4-ef51627ab0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path 1\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      "  Synset('odenet-15937-n') Frevel\n",
      "   Synset('odenet-6999-n') Topf\n",
      "    Synset('odenet-9850-n') Vermögen\n",
      "     Synset('odenet-4667-n') Vermögen\n",
      "      Synset('odenet-10390-n') Liegenschaft\n",
      "\n",
      "Path 2\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      "  Synset('odenet-5502-n') Handlung\n",
      "\n",
      "Path 3\n",
      " Synset('odenet-8872-n') Rauheit\n",
      "  Synset('odenet-25840-n') Unglück\n"
     ]
    }
   ],
   "source": [
    "# retrieve synsets\n",
    "synsets = de.synsets('Betrug', pos='n')\n",
    "\n",
    "# for each synset\n",
    "p = 1\n",
    "for s in synsets: \n",
    "    \n",
    "    # for each hypernym path\n",
    "    for path in wn.taxonomy.hypernym_paths(s):\n",
    "        print(\"\\nPath\", p)\n",
    "        for i, ss in enumerate(path):\n",
    "            # print synset ID and lemma\n",
    "            print(' ' * i, ss, ss.lemmas()[0]) \n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9dbe7-7af4-46e0-827a-af02ab9198ac",
   "metadata": {},
   "source": [
    "Additionally, to retrieve the hypernym which is closest to the root of the knowledge base, we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2640,
   "id": "de921c6c-4b50-44e2-9bfa-0a162ca72fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roots(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the German WordNet roots of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of root concepts of the noun if available, else empty list.\n",
    "    \"\"\"\n",
    "    \n",
    "    roots = [] # initiate empty list\n",
    "    synsets = get_synset(string) # retrieve synsets\n",
    "\n",
    "    # for each synset\n",
    "    for s in synsets: \n",
    "        \n",
    "        # for each hypernym path\n",
    "        for path in wn.taxonomy.hypernym_paths(s):\n",
    "            \n",
    "            # retrieve root (i.e. last element of path)\n",
    "            roots.append(path[-1].lemmas())\n",
    "            \n",
    "    # return flattened list of root hypernyms\n",
    "    return [y for x in roots for y in x]\n",
    "\n",
    "# apply function\n",
    "compounds['roots'] = compounds.second_part.apply(get_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574f5a7-295c-4e66-a462-404bdb9d0a8e",
   "metadata": {},
   "source": [
    "To be able to specify whether the concept of the compound describes an action or a person, we will retrieve the english hypernym paths for each compound word. For this, we translate the synset and retrieve the hypernym paths as we did before for the German WordNet lexicon. Instead of only retrieving the root concept (which is always `entity` for the English lexicon), we retrieve the complete path and check for key words that give us the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2641,
   "id": "a69031b5-157b-47b9-a918-7d50b4fe79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_hypernyms(string):\n",
    "    \"\"\"\n",
    "    Returns the English WordNet hypernym paths of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of hypernyms of the noun if available, else NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        roots = [] # initiate empty list\n",
    "        synsets = get_synset(string)[0].translate(lexicon='oewn:2021') # get English version of synset \n",
    "    \n",
    "        # for each synset\n",
    "        for s in synsets: \n",
    "            \n",
    "            # for each hypernym path\n",
    "            for path in wn.taxonomy.hypernym_paths(s):\n",
    "                \n",
    "                # retrieve lemmas of hypernyms and append to list \n",
    "                roots.append(x.lemmas() for x in path)\n",
    "                \n",
    "        # flatten list\n",
    "        roots = [z for x in roots for y in x for z in y]\n",
    "        \n",
    "        return roots # return list\n",
    "    \n",
    "    # except no translation is available\n",
    "    except:\n",
    "        \n",
    "        return np.nan # then return NaN\n",
    "\n",
    "# apply function\n",
    "compounds['en_hypernyms'] = compounds.second_part.apply(get_en_hypernyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99946aac-a4c4-4c45-b88d-4cdfa413a169",
   "metadata": {},
   "source": [
    "For instance, let's retrieve the English hypernym paths for the following two words: \"Betrüger\" (person) and \"Betrug\" (action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2642,
   "id": "11021c26-b1af-423d-8d03-2390fcacc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betrüger - Hypernym Paths:\n",
      " Synset('oewn-09974494-n') chiseler\n",
      "  Synset('oewn-10017621-n') slicker\n",
      "   Synset('oewn-09657157-n') offender\n",
      "    Synset('oewn-09851208-n') bad person\n",
      "     Synset('oewn-00007846-n') soul\n",
      "      Synset('oewn-00004475-n') being\n",
      "       Synset('oewn-00004258-n') animate thing\n",
      "        Synset('oewn-00003553-n') unit\n",
      "         Synset('oewn-00002684-n') physical object\n",
      "          Synset('oewn-00001930-n') physical entity\n",
      "           Synset('oewn-00001740-n') entity\n",
      " Synset('oewn-09974494-n') chiseler\n",
      "  Synset('oewn-10017621-n') slicker\n",
      "   Synset('oewn-09657157-n') offender\n",
      "    Synset('oewn-09851208-n') bad person\n",
      "     Synset('oewn-00007846-n') soul\n",
      "      Synset('oewn-00004475-n') being\n",
      "       Synset('oewn-00007347-n') cause\n",
      "        Synset('oewn-00001930-n') physical entity\n",
      "         Synset('oewn-00001740-n') entity\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "\n",
      "\n",
      "Betrug - Hypernym Paths:\n",
      " Synset('oewn-00770581-n') fraud\n",
      "  Synset('oewn-00767761-n') criminal offence\n",
      "   Synset('oewn-00767587-n') offence\n",
      "    Synset('oewn-00746303-n') transgression\n",
      "     Synset('oewn-00734044-n') wrongdoing\n",
      "      Synset('oewn-00408356-n') activity\n",
      "       Synset('oewn-00030657-n') human action\n",
      "        Synset('oewn-00029677-n') event\n",
      "         Synset('oewn-00002137-n') abstraction\n",
      "          Synset('oewn-00001740-n') entity\n"
     ]
    }
   ],
   "source": [
    "word1 = \"Betrüger\"\n",
    "word2 = \"Betrug\"\n",
    "\n",
    "print(word1, \"- Hypernym Paths:\")\n",
    "# for each hypernym path of the synset\n",
    "for path in wn.taxonomy.hypernym_paths(get_synset(word1)[0].translate(lexicon='oewn:2021')[0]):\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synset and lemma \n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"_\"*50)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(word2, \"- Hypernym Paths:\")\n",
    "# for each hypernym path of the synset\n",
    "for path in wn.taxonomy.hypernym_paths(get_synset(word2)[0].translate(lexicon='oewn:2021')[0]):\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synset and lemma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c3d60-d103-49a5-b333-54266f4a116c",
   "metadata": {},
   "source": [
    "In the paths we can see the following key words for the two concepts:\n",
    "- **Betrüger**: person, soul\n",
    "- **Betrug**: activity, human action\n",
    "\n",
    "Also for other concepts we identify the following hypernyms:\n",
    "- **Vernunft**: psychological feature\n",
    "- **Staat**: people, grouping\n",
    "\n",
    "We finally decide for the *four* main categories which we want to use to specify the concepts by looking for the following key words:\n",
    "- Abstraction: rational motive, motive, state, psychological feature, attribute,\n",
    "                phenomenon, process, cause, physical object, abstract entity, artifact\n",
    "- Person: person, soul, image, spiritual being, ideal\n",
    "- Action: activity, human action, wrongdoing\n",
    "- Group: grouping, people\n",
    "- Location: location, area\n",
    "\n",
    "Accordingly, we will create lists of those key words and specify the concept in the upcoming lines and save the information to a new column `concept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2884,
   "id": "89c51187-0544-4515-9887-6c11339afc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate concepts\n",
    "abstraction = [\"rational motive\", \"motive\", \"state\", \"psychological feature\", \"attribute\",\n",
    "                \"phenomenon\", \"process\", \"cause\", \"physical object\", \"abstract entity\", \"artifact\"]\n",
    "person = [\"person\", \"soul\",  \"image\", \"spiritual being\", \"ideal\"] \n",
    "action = [\"activity\", \"human action\", \"wrongdoing\"]\n",
    "group = [\"grouping\", \"people\"]\n",
    "location = [\"location\", \"area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2885,
   "id": "8a522f7b-65d6-41f7-b3bb-ff94512df080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#person = [\"person\", \"being\", \"soul\", \"physical entity\"] # list of \"person\" key words \n",
    "#action = [\"activity\", \"action\", \"event\", \"abstraction\"] # list of \"action\" key words\n",
    "\n",
    "def specify_concept(hypernyms):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the concept label for each compound word.\n",
    "    Arg: \n",
    "        list: a list of hypernyms.\n",
    "    Returns: \n",
    "        A concept label (either \"person\" or \"action\") for the compound word, else NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of action key words\n",
    "        if len(set(location).intersection(set(hypernyms))) > 0:\n",
    "            return \"location\" # label as location\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of group key words    \n",
    "        elif len(set(group).intersection(set(hypernyms))) > 0:\n",
    "            return \"group\" # label as group\n",
    "        \n",
    "        # if there is at least one common element in list of hypernyms and list of action key words\n",
    "        elif len(set(action).intersection(set(hypernyms))) > 0:\n",
    "            return \"action\" # label as action\n",
    "       \n",
    "        # if there is at least one common element in list of hypernyms and list of person key words    \n",
    "        elif len(set(person).intersection(set(hypernyms))) > 0:\n",
    "            return \"person\" # label as person    \n",
    "    \n",
    "        # if there is at least one common element in list of hypernyms and list of abstraction key words    \n",
    "        elif len(set(abstraction).intersection(set(hypernyms))) > 0:\n",
    "            return \"abstraction\" # label as abstraction\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            return(np.nan)\n",
    "        \n",
    "    # if there is no comparison available (e.g. bc. no hypernym list is available for the row)\n",
    "    except:\n",
    "        return np.nan # return NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2886,
   "id": "4642fd16-7a6b-4837-9a49-c14a0785db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to en_hypernyms column\n",
    "compounds['concept'] = compounds.en_hypernyms.apply(specify_concept)\n",
    "\n",
    "# manually change the value of \"Milliardär\" since \n",
    "compounds.loc[(compounds.second_part == 'milliardär'),'concept']='person'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018f5f7-59b9-4233-8f95-bd679a9bfed5",
   "metadata": {},
   "source": [
    "Next, we will save those columns for which we could not identify a concept to a csv file to manually add the concept of those compound words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2891,
   "id": "a5ad24f0-4435-4c7b-8961-351a520334d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gather all columns that did not receive a concept\n",
    "concept_manual = compounds[compounds['concept'].isna()][[\"original\", \"concept\"]]\n",
    "\n",
    "#concept_manual.to_csv(\"../evaluation/concept_manual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633065e-f8bf-4631-8c3e-5b671d37d01b",
   "metadata": {},
   "source": [
    "With the automatic method we could specify concepts for 80.65% of the compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2893,
   "id": "21096ef0-5f12-4119-8fd2-843a144297b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.24193548387098"
      ]
     },
     "execution_count": 2893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - (len(concept_manual)/248)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f75e9-7c2d-4e49-9180-ad3370f43f4d",
   "metadata": {},
   "source": [
    "After the manual annotation of the remaining 48 concepts, we load the table back into Python and merge the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2895,
   "id": "7de82579-c595-4a55-a3e5-56947ef3f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated table\n",
    "concept_annotated = pd.read_csv(\"../evaluation/concept_manual.csv\", sep =\";\")\n",
    "\n",
    "# reset index and update values\n",
    "compounds = compounds.set_index('original')\n",
    "concept_annotated = concept_annotated.set_index('original')\n",
    "compounds.update(concept_annotated)\n",
    "compounds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f17b0-7e2f-4c27-b047-b64d5e9dc374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c5abad9-e20d-458f-9f8a-b671741c4f7a",
   "metadata": {},
   "source": [
    "## 2.2 Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2655,
   "id": "e79c2466-68cf-4efd-ae8b-257ba7284b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definition(string):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the WordNet definitions of a string.\n",
    "    Arg: \n",
    "        string: a noun.\n",
    "    Returns: \n",
    "        A list of definitions of the noun if available, else None.\n",
    "    \"\"\"\n",
    "    definition = [] # initiate empty list\n",
    "    \n",
    "    try:\n",
    "        # for each synset\n",
    "        for s in get_synset(string):\n",
    "            definition.append(s.definition()) # retrieve definition and append to list\n",
    " \n",
    "        return definition # return list of definitions\n",
    "    \n",
    "    except:\n",
    "        return \n",
    "    \n",
    "# create and save definition to new column in data frame\n",
    "compounds['definition'] = compounds.second_part.apply(get_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b89c70-1c1c-4a9e-8696-3ef1e91bcd2f",
   "metadata": {},
   "source": [
    "## 2.3 Similarity Measures\n",
    "Todo\n",
    "- [X] check similarities (how similar synsets are?) -> the ones that are very similar could be offered to have a closer look at\n",
    "- [X] create a matrix with all words as rows and all words as columns (word x word matrix) \n",
    "- [X] then calculate similarity for all words \n",
    "- [X] output pair of words for similarity scores higher than 0.5\n",
    "- [X] do matrix for wup similarity and for regular path function\n",
    "- [ ] (decide manually which one works better for our case)\n",
    "- [ ] (do same for lemma version of words) -> not necessary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf743c-7c9e-4c33-812d-d1aee1b2b168",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Similarity Measures\n",
    "Via the `wn` library there are multiple options to compute the similarity of two input words. For our case we used the following two ways of retrieving a similarity score. \n",
    "Here, a score of 1 means that words are \"very similar\" and 0 indicating that words are not similar at all.\n",
    "\n",
    "In the following example we can see how the two options differ in scoring for the same combination of words. While the `PATH` similarity score for **Betrug** und **Verbrechen** is 0.5, the `WUP` score is 0.9 - indicating a higher relatedness of the two concepts than the first method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2656,
   "id": "8f4daf45-f123-4268-a2f1-af99e346d900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PATH similarity measure\n",
    "# 1 is being \"very similar\", 0 is \"not similar\" (i.e. no connection in wn)\n",
    "wn.similarity.path(get_synset(\"Betrug\")[0], get_synset(\"Verbrechen\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2657,
   "id": "9bd7e1a7-5795-44cb-86b4-5a2eaebc23cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 2657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WUP similarity measure \n",
    "wn.similarity.wup(get_synset(\"Betrug\")[0], get_synset(\"Verbrechen\")[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d69998-c55b-4daa-a7b1-b559e45aecf4",
   "metadata": {},
   "source": [
    "The retrieval of the common synset path results in the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2658,
   "id": "f8f0431c-f854-460c-be00-2b5facfd2f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of 'Betrug' and 'Verbrechen':\n",
      "\n",
      "Handlung\n",
      "Treulosigkeit\n",
      "Liegenschaft\n",
      "Vermögen\n",
      "Vermögen\n",
      "Topf\n",
      "Frevel\n",
      "Treulosigkeit\n"
     ]
    }
   ],
   "source": [
    "paths = [list(reversed([get_synset(\"Betrug\")[0]] + p)) for p in get_synset(\"Verbrechen\")[0].hypernym_paths()]\n",
    "print(\"Path of 'Betrug' and 'Verbrechen':\\n\")\n",
    "for el in paths:\n",
    "    for item in el:\n",
    "        print(item.lemmas()[0])\n",
    "        #if item != el[-1]:\n",
    "            #print(\"|\")\n",
    "         #   print(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fd4f2-be61-45a0-961f-4acc8e855b8c",
   "metadata": {},
   "source": [
    "To illustrate the hypernym paths and the according path similarity scores for each edge between the nodes please see the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2659,
   "id": "31a41313-52ac-427f-b7ff-fd47183ee7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path 1 Betrug\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      " path: 0.9230769230769231\n",
      "  Synset('odenet-15937-n') Frevel\n",
      "  path: 0.8333333333333334\n",
      "   Synset('odenet-6999-n') Topf\n",
      "   path: 0.7272727272727273\n",
      "    Synset('odenet-9850-n') Vermögen\n",
      "    path: 0.6\n",
      "     Synset('odenet-4667-n') Vermögen\n",
      "     path: 0.4444444444444444\n",
      "      Synset('odenet-10390-n') Liegenschaft\n",
      "      path: 0.25\n",
      "\n",
      "Path 2 Betrug\n",
      " Synset('odenet-10880-n') krimineller Akt\n",
      " path: 0.9230769230769231\n",
      "  Synset('odenet-5502-n') Handlung\n",
      "  path: 0.5\n",
      "_____________________________________________\n",
      "\n",
      "Path 1 Verbrechen\n",
      " Synset('odenet-5502-n') Handlung\n",
      " path: 0.5\n",
      "\n",
      "Path 2 Verbrechen\n",
      " Synset('odenet-15937-n') Frevel\n",
      " path: 0.8333333333333334\n",
      "  Synset('odenet-6999-n') Topf\n",
      "  path: 0.7272727272727273\n",
      "   Synset('odenet-9850-n') Vermögen\n",
      "   path: 0.6\n",
      "    Synset('odenet-4667-n') Vermögen\n",
      "    path: 0.4444444444444444\n",
      "     Synset('odenet-10390-n') Liegenschaft\n",
      "     path: 0.25\n"
     ]
    }
   ],
   "source": [
    "word1 = \"Betrug\"\n",
    "word2 = \"Verbrechen\"\n",
    "\n",
    "# retrieve synset\n",
    "synset1 = get_synset(word1)[0]\n",
    "p = 1 # initiate path count\n",
    "\n",
    "# for each path\n",
    "for path in wn.taxonomy.hypernym_paths(synset1):\n",
    "    print(\"\\nPath\",p, word1) \n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synsets and lemmas\n",
    "        print(\" \" * i, \"path:\", wn.similarity.wup(word, ss)) # retrieve path similarity\n",
    "    p = p+1\n",
    "    \n",
    "print(\"_\"*45)    \n",
    "synset2 = get_synset(word2)[0]\n",
    "p = 1 # initiate path count\n",
    "\n",
    "# for each path\n",
    "for path in wn.taxonomy.hypernym_paths(synset2):\n",
    "    print(\"\\nPath\",p, word2)\n",
    "    for i, ss in enumerate(path):\n",
    "        print(' ' * i, ss, ss.lemmas()[0]) # print synsets and lemmas\n",
    "        print(\" \" * i, \"path:\", wn.similarity.wup(word, ss)) # retrieve path similarity\n",
    "    p = p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2660,
   "id": "ca02fe0d-a6db-4a99-81f2-62d2ce586283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common hypernyms of 'Betrug' and 'Verbrechen': \n",
      "\n",
      "Liegenschaft\n",
      "krimineller Akt\n",
      "Frevel\n",
      "Vermögen\n",
      "Handlung\n",
      "Topf\n",
      "Vermögen\n"
     ]
    }
   ],
   "source": [
    "print(\"Common hypernyms of 'Betrug' and 'Verbrechen': \\n\")\n",
    "\n",
    "for x in sorted(get_synset(\"betrug\")[0].common_hypernyms(get_synset(\"verbrechen\")[0])):\n",
    "    print(x.lemmas()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2661,
   "id": "0c5ec261-db58-438f-a9ce-e80cd9e2546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krimineller Akt\n"
     ]
    }
   ],
   "source": [
    "lowest_common = get_synset(\"Betrug\")[0].lowest_common_hypernyms(get_synset(\"Verbrechen\")[0])\n",
    "for s in lowest_common:\n",
    "    print(s.lemmas()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04f439-df52-44bc-856f-75eb109a3e2d",
   "metadata": {},
   "source": [
    "To work with similarity scores on our `compounds` data frame we first initiate a list of the nouns of our data frame to have a closer look at. Furthermore we initiate two empty data frames to compute and save the similarity scores.\n",
    "\n",
    "Then we run the `wup` function and the `path` function on our nouns and save the computed similarity scores to the new data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2662,
   "id": "186949b5-3127-44af-a843-a137a124b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of nouns from compound words to work with for similarity measures\n",
    "nouns = compounds.second_part.tolist()\n",
    "\n",
    "# create data frame (matrix like) with all nouns as columns and rows to compute similarity \n",
    "nouns_wup = pd.DataFrame(index = nouns, columns = nouns)\n",
    "nouns_sim = pd.DataFrame(index = nouns, columns = nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2663,
   "id": "673d9c8c-5169-41a4-a862-1d205849cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill dataframe with similarity scores (\"wup\" function)\n",
    "\n",
    "# iterate over columns and rows\n",
    "for w in nouns:\n",
    "    for ww in nouns:\n",
    "        try: \n",
    "            wn_w = get_synset(w)[0] # retrieve synset information for column word and for row word\n",
    "            wn_ww = get_synset(ww)[0]\n",
    "            sim = wn.similarity.wup(wn_w, wn_ww, True) # compute similarity score\n",
    "            nouns_wup[ww].loc[w] = sim # change value in cell\n",
    "        except:\n",
    "            nouns_wup[ww].loc[w] = np.NaN # if there is no score, return \"None\"\n",
    "            \n",
    "            \n",
    "# round all values to 3 decimals \n",
    "nouns_wup = nouns_wup.round(3)\n",
    "\n",
    "# to fill dataframe with similarity scores (\"path\" function)\n",
    "\n",
    "# iterate over columns and rows\n",
    "for w in nouns:\n",
    "    for ww in nouns:\n",
    "        try: \n",
    "            wn_w = get_synset(w)[0] # retrieve synset information for column word and for row word\n",
    "            wn_ww = get_synset(ww)[0]\n",
    "            sim = wn.similarity.path(wn_w, wn_ww, True) # compute similarity score\n",
    "            nouns_sim[ww].loc[w] = sim # change value in cell\n",
    "        except:\n",
    "            nouns_sim[ww].loc[w] = np.NaN # if there is no score, return \"None\"\n",
    "\n",
    "# round all values to 3 decimals \n",
    "nouns_sim = nouns_sim.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c1a27-410a-48fb-82ce-add4702c9688",
   "metadata": {},
   "source": [
    "Our matrix-like data frames now look as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2664,
   "id": "883668e1-4b33-4fac-8647-ef83b789fa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abzockerei</th>\n",
       "      <th>aktivismus</th>\n",
       "      <th>aktivist</th>\n",
       "      <th>aktivistin</th>\n",
       "      <th>alarm</th>\n",
       "      <th>alarmist</th>\n",
       "      <th>anbeter</th>\n",
       "      <th>apokalypse</th>\n",
       "      <th>apokalyptiker</th>\n",
       "      <th>apostel</th>\n",
       "      <th>...</th>\n",
       "      <th>zar</th>\n",
       "      <th>zerrüttung</th>\n",
       "      <th>zerstörer</th>\n",
       "      <th>zerstörung</th>\n",
       "      <th>zeugs</th>\n",
       "      <th>zipfel</th>\n",
       "      <th>zirkus</th>\n",
       "      <th>zunft</th>\n",
       "      <th>zwang</th>\n",
       "      <th>überhitzung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abzockerei</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivismus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivist</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivistin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alarm</th>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abzockerei aktivismus  aktivist aktivistin     alarm alarmist  \\\n",
       "abzockerei        1.0        NaN  0.142857        NaN       0.2      NaN   \n",
       "aktivismus        NaN        NaN       NaN        NaN       NaN      NaN   \n",
       "aktivist     0.142857        NaN       1.0        NaN  0.142857      NaN   \n",
       "aktivistin        NaN        NaN       NaN        NaN       NaN      NaN   \n",
       "alarm             0.2        NaN  0.142857        NaN       1.0      NaN   \n",
       "\n",
       "             anbeter apokalypse apokalyptiker   apostel  ...       zar  \\\n",
       "abzockerei  0.166667        0.2           NaN      0.25  ...      0.25   \n",
       "aktivismus       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "aktivist    0.166667   0.142857           NaN  0.166667  ...  0.166667   \n",
       "aktivistin       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "alarm       0.166667        0.2           NaN      0.25  ...      0.25   \n",
       "\n",
       "           zerrüttung zerstörer zerstörung     zeugs    zipfel zirkus  \\\n",
       "abzockerei        0.2      0.25       0.25  0.142857      0.25  0.125   \n",
       "aktivismus        NaN       NaN        NaN       NaN       NaN    NaN   \n",
       "aktivist     0.142857  0.166667   0.166667  0.111111  0.166667    0.1   \n",
       "aktivistin        NaN       NaN        NaN       NaN       NaN    NaN   \n",
       "alarm             0.2      0.25       0.25  0.142857      0.25  0.125   \n",
       "\n",
       "               zunft     zwang überhitzung  \n",
       "abzockerei  0.166667      0.25         NaN  \n",
       "aktivismus       NaN       NaN         NaN  \n",
       "aktivist       0.125  0.166667         NaN  \n",
       "aktivistin       NaN       NaN         NaN  \n",
       "alarm       0.166667      0.25         NaN  \n",
       "\n",
       "[5 rows x 248 columns]"
      ]
     },
     "execution_count": 2664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2665,
   "id": "298b5b0b-092a-4058-9941-c0f7c9d84e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abzockerei</th>\n",
       "      <th>aktivismus</th>\n",
       "      <th>aktivist</th>\n",
       "      <th>aktivistin</th>\n",
       "      <th>alarm</th>\n",
       "      <th>alarmist</th>\n",
       "      <th>anbeter</th>\n",
       "      <th>apokalypse</th>\n",
       "      <th>apokalyptiker</th>\n",
       "      <th>apostel</th>\n",
       "      <th>...</th>\n",
       "      <th>zar</th>\n",
       "      <th>zerrüttung</th>\n",
       "      <th>zerstörer</th>\n",
       "      <th>zerstörung</th>\n",
       "      <th>zeugs</th>\n",
       "      <th>zipfel</th>\n",
       "      <th>zirkus</th>\n",
       "      <th>zunft</th>\n",
       "      <th>zwang</th>\n",
       "      <th>überhitzung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abzockerei</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivismus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivist</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aktivistin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alarm</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abzockerei aktivismus aktivist aktivistin     alarm alarmist  \\\n",
       "abzockerei        1.0        NaN     0.25        NaN  0.333333      NaN   \n",
       "aktivismus        NaN        NaN      NaN        NaN       NaN      NaN   \n",
       "aktivist         0.25        NaN      1.0        NaN      0.25      NaN   \n",
       "aktivistin        NaN        NaN      NaN        NaN       NaN      NaN   \n",
       "alarm        0.333333        NaN     0.25        NaN       1.0      NaN   \n",
       "\n",
       "             anbeter apokalypse apokalyptiker   apostel  ...       zar  \\\n",
       "abzockerei  0.285714   0.333333           NaN       0.4  ...       0.4   \n",
       "aktivismus       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "aktivist    0.285714       0.25           NaN  0.285714  ...  0.285714   \n",
       "aktivistin       NaN        NaN           NaN       NaN  ...       NaN   \n",
       "alarm       0.285714   0.333333           NaN       0.4  ...       0.4   \n",
       "\n",
       "           zerrüttung zerstörer zerstörung zeugs    zipfel    zirkus  \\\n",
       "abzockerei   0.333333       0.4        0.4  0.25       0.4  0.222222   \n",
       "aktivismus        NaN       NaN        NaN   NaN       NaN       NaN   \n",
       "aktivist         0.25  0.285714   0.285714   0.2  0.285714  0.181818   \n",
       "aktivistin        NaN       NaN        NaN   NaN       NaN       NaN   \n",
       "alarm        0.333333       0.4        0.4  0.25       0.4  0.222222   \n",
       "\n",
       "               zunft     zwang überhitzung  \n",
       "abzockerei  0.285714       0.4         NaN  \n",
       "aktivismus       NaN       NaN         NaN  \n",
       "aktivist    0.222222  0.285714         NaN  \n",
       "aktivistin       NaN       NaN         NaN  \n",
       "alarm       0.285714       0.4         NaN  \n",
       "\n",
       "[5 rows x 248 columns]"
      ]
     },
     "execution_count": 2665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_wup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2666,
   "id": "7a8fc149-d796-46f7-81b5-133a3a433f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file \n",
    "nouns_sim.to_csv(\"../output/nouns_sim.csv\")\n",
    "nouns_wup.to_csv(\"../output/nouns_wup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f6aec-720f-41c0-af5c-3b191ba7c830",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Retrieve Words\n",
    "\n",
    "In a next step, we gather all cells with similarity scores higher than 0.5. The word combinations having a score higher than 0.5 are then saved to a dictionary from which we then create a new column in our original `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2667,
   "id": "2fd78f4c-3a18-4546-8ac1-3db2b5b3f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with keys from \n",
    "wup_sim_dict = dict.fromkeys(nouns)\n",
    "\n",
    "# for each key look up similar words and save to dict \n",
    "for key, values in wup_sim_dict.items():\n",
    "    \n",
    "    # update values such that the list of \"very similar\" words is saved for each key \n",
    "    words = nouns_wup.index[nouns_wup[key] > 0.5].tolist()\n",
    "    \n",
    "    if key in words:\n",
    "        words.remove(key) # remove the key word from the values list (since they always get score 1.0)\n",
    "        \n",
    "    wup_sim_dict[key] = words # save words to according dictionary key\n",
    "\n",
    "# do for both data frames (similarity measures)\n",
    "sim_dict = dict.fromkeys(nouns)\n",
    "\n",
    "for key, values in sim_dict.items():\n",
    "    \n",
    "    # update values such that the list of \"very similar\" words is saved for each key \n",
    "    words = nouns_sim.index[nouns_sim[key] > 0.5].tolist()\n",
    "    \n",
    "    if key in words:\n",
    "        words.remove(key) # remove the key word from the values list (since they always get score 1.0)\n",
    "    \n",
    "    sim_dict[key] = words # save words to according dictionary key\n",
    "    \n",
    "# add information to compounds data frame \n",
    "compounds['path'] = compounds.second_part.map(sim_dict)\n",
    "compounds['wup'] = compounds.second_part.map(wup_sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "id": "5b63c7a4-3374-4111-b724-58676a8fe325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compounds = compounds.iloc[: , :-2]\n",
    "#compounds = compounds.drop(['en_roots'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b40160-d9a8-404a-90e8-709c7acfe6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4 Stemming of second part nouns \n",
    "Stemming is performed using the following stemmers provided via the `nltk` library: `cistem`, `porter`, `lancaster`, `snowball`\n",
    "\n",
    "We save all stemming results to our data frame to be able later on to manually check the stemmers for accuracy and decide for a final one or combine the information of the multiple stemmers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2669,
   "id": "656c4efd-be9c-4ef1-b4d6-b7861d4858db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# initiate stemmers \n",
    "cistem = Cistem(case_insensitive=True)\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"german\")\n",
    "\n",
    "# the following functions all take a string as an input and return the according stem for each stemmer \n",
    "def stem_cistem(string):\n",
    "    return cistem.stem(string)\n",
    "\n",
    "def stem_porter(string):\n",
    "    return porter.stem(string)\n",
    "\n",
    "def stem_lancaster(string):\n",
    "    return lancaster.stem(string)\n",
    "\n",
    "def stem_snowball(string):\n",
    "    return snowball.stem(string)\n",
    "\n",
    "# apply stemmers to dataframe \n",
    "compounds['stem_cistem'] = compounds.second_part.apply(stem_cistem)\n",
    "compounds['stem_porter'] = compounds.second_part.apply(stem_porter)\n",
    "compounds['stem_lancaster'] = compounds.second_part.apply(stem_lancaster)\n",
    "compounds['stem_snowball'] = compounds.second_part.apply(stem_snowball)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09613ce-47f3-463a-9892-1f7aa402b159",
   "metadata": {},
   "source": [
    "After having applied each stemmer to our data frame, we check the output of each stemmer for duplicates. I.e. are there any compounds giving us the same stem? We conclude these compounds to be semantically related to each other in some way and therefore create a list of words with the same stem and save this list for each word to a new column `share_stemmer`. Since this is being done for each stemmer, we obtain four new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2670,
   "id": "72358c9d-bf08-4c30-ac1c-962326cb5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# for each stemming column check whether there are same entries of words that could be connected\n",
    "# get duplicates of stem column\n",
    "cistem_stem_words = compounds[compounds.duplicated(['stem_cistem'])].stem_cistem.tolist()\n",
    "# create list of duplicate words \n",
    "cistem_duplicates = compounds[pd.DataFrame(compounds.stem_cistem.tolist()).isin(cistem_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "porter_stem_words = compounds[compounds.duplicated(['stem_porter'])].stem_porter.tolist()\n",
    "# create list of duplicate words \n",
    "porter_duplicates = compounds[pd.DataFrame(compounds.stem_porter.tolist()).isin(porter_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "lancaster_stem_words = compounds[compounds.duplicated(['stem_lancaster'])].stem_lancaster.tolist()\n",
    "# create list of duplicate words \n",
    "lancaster_duplicates = compounds[pd.DataFrame(compounds.stem_lancaster.tolist()).isin(lancaster_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# get duplicates of stem column\n",
    "snowball_stem_words = compounds[compounds.duplicated(['stem_snowball'])].stem_snowball.tolist()\n",
    "# create list of duplicate words \n",
    "snowball_duplicates = compounds[pd.DataFrame(compounds.stem_snowball.tolist()).isin(snowball_stem_words).any(1).values].second_part.tolist()\n",
    "\n",
    "# create new column and add common stem words there (do for every stemmer)\n",
    "\n",
    "# cistem stemmer\n",
    "compounds[\"share_cistem\"] = np.NaN\n",
    "\n",
    "# for each stem word that has duplicate in the data frame\n",
    "for stem in cistem_stem_words:\n",
    "    # for compound that has a shared stem with another compound\n",
    "    idx = compounds.index[compounds['stem_cistem'] == stem]\n",
    "    for i in idx:\n",
    "        # get list of words that share stem\n",
    "        share = compounds.second_part[compounds['stem_cistem'] == stem].tolist()\n",
    "        # save that list to the new column\n",
    "        compounds.share_cistem.loc[i] = share\n",
    "\n",
    "# porter stemmer\n",
    "compounds[\"share_porter\"] = np.NaN\n",
    "for stem in porter_stem_words:\n",
    "    idx = compounds.index[compounds['stem_porter'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_porter'] == stem].tolist()\n",
    "        compounds.share_porter.loc[i] = share\n",
    "\n",
    "# lancaster stemmer\n",
    "compounds[\"share_lancaster\"] = np.NaN\n",
    "for stem in lancaster_stem_words:\n",
    "    idx = compounds.index[compounds['stem_lancaster'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_lancaster'] == stem].tolist()\n",
    "        compounds.share_lancaster.loc[i] = share\n",
    "\n",
    "# snowball stemmer\n",
    "compounds[\"share_snowball\"] = np.NaN\n",
    "for stem in snowball_stem_words:\n",
    "    idx = compounds.index[compounds['stem_snowball'] == stem]\n",
    "    for i in idx:\n",
    "        share = compounds.second_part[compounds['stem_snowball'] == stem].tolist()\n",
    "        compounds.share_snowball.loc[i] = share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2671,
   "id": "e17a0824-dbcb-46a6-b79f-874376653e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>path</th>\n",
       "      <th>wup</th>\n",
       "      <th>stem_cistem</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>share_cistem</th>\n",
       "      <th>share_porter</th>\n",
       "      <th>share_lancaster</th>\n",
       "      <th>share_snowball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockere</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivism</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>[jemand, irgendjemand, jeder beliebige]</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[demagoge, macher]</td>\n",
       "      <td>aktivi</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>akt</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>aktivisti</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>[Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>klimazipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>[zipfeln, zipfel, zipfels]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazipfeln, klimazipfel, klimazipfels]</td>\n",
       "      <td>[bestes Stück, Zipfel, Schwengel, Stößel, Schn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cylinder, round shape, form, shape, attribute...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>zipfel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>klimazirkus</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>[zirkusse, zirkus, zirkussen, zirkusses]</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazirkusse, klimazirkus, klimazirkussen, k...</td>\n",
       "      <td>[Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...</td>\n",
       "      <td>[vorweisen, vorzeigen]</td>\n",
       "      <td>[Versuch, Vorsatz, Unternehmung, Rastlosigkeit...</td>\n",
       "      <td>[bowl, sports stadium, stadium, arena, constru...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[kabarett, show]</td>\n",
       "      <td>zirku</td>\n",
       "      <td>zirku</td>\n",
       "      <td>zirk</td>\n",
       "      <td>zirkus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>klimazunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>[zunft, zünfte, zünften]</td>\n",
       "      <td>zunft</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimazunft, klimazünfte, klimazünften]</td>\n",
       "      <td>[Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...</td>\n",
       "      <td>[Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...</td>\n",
       "      <td>[Gestaltung, Erreichung, Realisierung, Verwirk...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zunf</td>\n",
       "      <td>zunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>zunft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>klimazwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>[zwange, zwängen, zwangs, zwanges, zwang, zwänge]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimazwange, klimazwängen, klimazwangs, klima...</td>\n",
       "      <td>[Erpressung, Nötigung, Bedingung, Auflage, Res...</td>\n",
       "      <td>[Befehlssatz, Befehlsvorrat, Befehlsrepertoire]</td>\n",
       "      <td>[Gruppierung, Clusterung, Bündelung]</td>\n",
       "      <td>[regulating, regulation, control, activity, hu...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>zwang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>klimaüberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>[überhitzungen, überhitzung]</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaüberhitzungen, klimaüberhitzung]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>uberhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>überhitzung</td>\n",
       "      <td>uberhitz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  second_part  \\\n",
       "0     klimaabzockerei   abzockerei   \n",
       "1     klimaaktivismus   aktivismus   \n",
       "2       klimaaktivist     aktivist   \n",
       "3     klimaaktivistin   aktivistin   \n",
       "4          klimaalarm        alarm   \n",
       "..                ...          ...   \n",
       "243       klimazipfel       zipfel   \n",
       "244       klimazirkus       zirkus   \n",
       "245        klimazunft        zunft   \n",
       "246        klimazwang        zwang   \n",
       "247  klimaüberhitzung  überhitzung   \n",
       "\n",
       "                                            noun_forms        lemma genus  \\\n",
       "0                           [abzockerei, abzockereien]   abzockerei     f   \n",
       "1                                         [aktivismus]   aktivismus     m   \n",
       "2                               [aktivisten, aktivist]     aktivist     m   \n",
       "3                          [aktivistinnen, aktivistin]   aktivistin     f   \n",
       "4            [alarms, alarm, alarmen, alarme, alarmes]        alarm     m   \n",
       "..                                                 ...          ...   ...   \n",
       "243                         [zipfeln, zipfel, zipfels]       zipfel     m   \n",
       "244           [zirkusse, zirkus, zirkussen, zirkusses]       zirkus     m   \n",
       "245                           [zunft, zünfte, zünften]        zunft     f   \n",
       "246  [zwange, zwängen, zwangs, zwanges, zwang, zwänge]        zwang     m   \n",
       "247                       [überhitzungen, überhitzung]  überhitzung     f   \n",
       "\n",
       "                                        compound_forms  \\\n",
       "0                 [klimaabzockerei, klimaabzockereien]   \n",
       "1                                    [klimaaktivismus]   \n",
       "2                     [klimaaktivisten, klimaaktivist]   \n",
       "3                [klimaaktivistinnen, klimaaktivistin]   \n",
       "4    [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "..                                                 ...   \n",
       "243          [klimazipfeln, klimazipfel, klimazipfels]   \n",
       "244  [klimazirkusse, klimazirkus, klimazirkussen, k...   \n",
       "245            [klimazunft, klimazünfte, klimazünften]   \n",
       "246  [klimazwange, klimazwängen, klimazwangs, klima...   \n",
       "247             [klimaüberhitzungen, klimaüberhitzung]   \n",
       "\n",
       "                                         related_words  \\\n",
       "0    [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                   []   \n",
       "2    [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                   []   \n",
       "4    [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "..                                                 ...   \n",
       "243  [bestes Stück, Zipfel, Schwengel, Stößel, Schn...   \n",
       "244  [Herumlärmen, Gelärme, Lärmerei, Rumlärmen, Zi...   \n",
       "245  [Gewerbe, Gilde, Innung, Gewerk, Zunft, Amt, B...   \n",
       "246  [Erpressung, Nötigung, Bedingung, Auflage, Res...   \n",
       "247                                                 []   \n",
       "\n",
       "                                             hypernyms  \\\n",
       "0                                  [Zinssatz, Zinsfuß]   \n",
       "1                                                   []   \n",
       "2                          [Volksvertreter, Politiker]   \n",
       "3                                                   []   \n",
       "4    [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244                             [vorweisen, vorzeigen]   \n",
       "245  [Arbeitsgemeinschaft, Arbeitskreis, Arbeitsgru...   \n",
       "246    [Befehlssatz, Befehlsvorrat, Befehlsrepertoire]   \n",
       "247                                                 []   \n",
       "\n",
       "                                                 roots  \\\n",
       "0                                  [Zinssatz, Zinsfuß]   \n",
       "1                                                   []   \n",
       "2              [jemand, irgendjemand, jeder beliebige]   \n",
       "3                                                   []   \n",
       "4    [Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...   \n",
       "..                                                 ...   \n",
       "243                                                 []   \n",
       "244  [Versuch, Vorsatz, Unternehmung, Rastlosigkeit...   \n",
       "245  [Gestaltung, Erreichung, Realisierung, Verwirk...   \n",
       "246               [Gruppierung, Clusterung, Bündelung]   \n",
       "247                                                 []   \n",
       "\n",
       "                                          en_hypernyms  ... path  \\\n",
       "0    [robbery, stealing, thieving, theft, larceny, ...  ...   []   \n",
       "1                                                  NaN  ...   []   \n",
       "2    [reformer, meliorist, crusader, social reforme...  ...   []   \n",
       "3                                                  NaN  ...   []   \n",
       "4    [fear, fearfulness, fright, emotion, feeling, ...  ...   []   \n",
       "..                                                 ...  ...  ...   \n",
       "243  [cylinder, round shape, form, shape, attribute...  ...   []   \n",
       "244  [bowl, sports stadium, stadium, arena, constru...  ...   []   \n",
       "245                                                 []  ...   []   \n",
       "246  [regulating, regulation, control, activity, hu...  ...   []   \n",
       "247                                                NaN  ...   []   \n",
       "\n",
       "                    wup  stem_cistem  stem_porter stem_lancaster  \\\n",
       "0                    []   abzockerei   abzockerei      abzockere   \n",
       "1                    []    aktivismu    aktivismu       aktivism   \n",
       "2    [demagoge, macher]       aktivi     aktivist            akt   \n",
       "3                    []    aktivisti   aktivistin     aktivistin   \n",
       "4                    []        alarm        alarm          alarm   \n",
       "..                  ...          ...          ...            ...   \n",
       "243                  []       zipfel       zipfel         zipfel   \n",
       "244    [kabarett, show]        zirku        zirku           zirk   \n",
       "245                  []         zunf        zunft          zunft   \n",
       "246                  []        zwang        zwang          zwang   \n",
       "247                  []  uberhitzung  überhitzung    überhitzung   \n",
       "\n",
       "    stem_snowball share_cistem share_porter    share_lancaster share_snowball  \n",
       "0      abzockerei          NaN          NaN                NaN            NaN  \n",
       "1      aktivismus          NaN          NaN                NaN            NaN  \n",
       "2        aktivist          NaN          NaN                NaN            NaN  \n",
       "3      aktivistin          NaN          NaN                NaN            NaN  \n",
       "4           alarm          NaN          NaN  [alarm, alarmist]            NaN  \n",
       "..            ...          ...          ...                ...            ...  \n",
       "243        zipfel          NaN          NaN                NaN            NaN  \n",
       "244        zirkus          NaN          NaN                NaN            NaN  \n",
       "245         zunft          NaN          NaN                NaN            NaN  \n",
       "246         zwang          NaN          NaN                NaN            NaN  \n",
       "247      uberhitz          NaN          NaN                NaN            NaN  \n",
       "\n",
       "[248 rows x 22 columns]"
      ]
     },
     "execution_count": 2671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ba114-d4ec-4607-a97b-f6295393b708",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.5 Compute Distance\n",
    "Additional to the stems, we now compute the distance of two strings by meanings of their letters. The `Jaro Distance` computes the similarity between two strings and is offered by `nltk`. An example output for the Lancaster Stemmer is shown below. Since the Lancaster Stemmer also puts the words *Professor* and *Presse* into relation, we retrospectively discard it for this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2672,
   "id": "245e398b-e91d-46c6-88f7-0609afa4f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2673,
   "id": "8ad1467c-659f-4d5b-9acb-1a2e24f5255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apokalypse => apokalyptiker | Score: 0.872053872053872\n",
      "apokalyptiker => apokalypse | Score: 0.872053872053872\n",
      "betrug => betrüger | Score: 0.888888888888889\n",
      "betrüger => betrug | Score: 0.888888888888889\n",
      "freund => freundin | Score: 0.9166666666666666\n",
      "freundin => freund | Score: 0.9166666666666666\n",
      "gerechtigkeit => ungerechtigkeit | Score: 0.9555555555555555\n",
      "konfusion => konsens | Score: 0.8888888888888888\n",
      "konsens => konfusion | Score: 0.8888888888888888\n",
      "leugner => leugnung | Score: 0.875\n",
      "leugnung => leugner | Score: 0.875\n",
      "lüge => lügner | Score: 0.8888888888888888\n",
      "lügner => lüge | Score: 0.8888888888888888\n",
      "notfall => notlage | Score: 0.8888888888888888\n",
      "notlage => notfall | Score: 0.8888888888888888\n",
      "presse => professor | Score: 0.9047619047619048\n",
      "professor => presse | Score: 0.9047619047619048\n",
      "propaganda => propagandafilm | Score: 0.8809523809523809\n",
      "propagandafilm => propaganda | Score: 0.8809523809523809\n",
      "propagandafilm => propaganda | Score: 0.8809523809523809\n",
      "propaganda => propagandafilm | Score: 0.8809523809523809\n",
      "schwindel => schwindler | Score: 0.9296296296296296\n",
      "schwindler => schwindel | Score: 0.9296296296296296\n",
      "sekte => sektierer | Score: 0.9333333333333332\n",
      "sektierer => sekte | Score: 0.9333333333333332\n",
      "skeptiker => skeptikerin | Score: 0.8787878787878787\n",
      "skeptikerin => skeptiker | Score: 0.8787878787878787\n",
      "streit => streiter | Score: 0.9166666666666666\n",
      "streiter => streit | Score: 0.9166666666666666\n",
      "ungerechtigkeit => gerechtigkeit | Score: 0.9555555555555555\n",
      "zerstörer => zerstörung | Score: 0.9\n",
      "zerstörung => zerstörer | Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Compute Jaro Distance of stems \n",
    "\n",
    "dist_cistem = [] # initiate empty list \n",
    "\n",
    "# iterate over all rows in columns\n",
    "for w in compounds.stem_cistem:\n",
    "    for ww in compounds.stem_cistem:\n",
    "        \n",
    "        # compute distance score for all possible combinations\n",
    "        dist = distance.jaro_similarity(w, ww) \n",
    "        \n",
    "        # if distance score is between 0.87 and 1.0 (indicating exact same string)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            # retrieve complete words \n",
    "            word1 = compounds[compounds.stem_cistem == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_cistem == ww].second_part.values[0]\n",
    "            dist_cistem.append([word1, word2]) # and save to list\n",
    "            \n",
    "# Do this for all stem columns \n",
    "dist_porter = []\n",
    "for w in compounds.stem_porter:\n",
    "    for ww in compounds.stem_porter:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_porter == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_porter == ww].second_part.values[0]\n",
    "            dist_porter.append([word1, word2])\n",
    "                   \n",
    "dist_snowball = []\n",
    "for w in compounds.stem_snowball:\n",
    "    for ww in compounds.stem_snowball:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_snowball == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_snowball == ww].second_part.values[0]\n",
    "            dist_snowball.append([word1, word2])\n",
    "            \n",
    "            \n",
    "# to exclude: some combinations are not as accurate   \n",
    "# Presse - Professor should not be connected \n",
    "dist_lancaster = []\n",
    "for w in compounds.stem_lancaster:\n",
    "    for ww in compounds.stem_lancaster:\n",
    "        dist = distance.jaro_similarity(w, ww)\n",
    "        if dist >= 0.87 and dist < 1.0:\n",
    "            word1 = compounds[compounds.stem_lancaster == w].second_part.values[0]\n",
    "            word2 = compounds[compounds.stem_lancaster == ww].second_part.values[0]\n",
    "            print(word1,\"=>\", word2, \"| Score:\", dist)\n",
    "            dist_lancaster.append([word1, word2])\n",
    "            \n",
    "            \n",
    "# combine findings\n",
    "dist_stemmer = dist_cistem + dist_porter + dist_snowball\n",
    "\n",
    "# get unique values \n",
    "dist_stemmer = [list(x) for x in {tuple(x) for x in dist_stemmer}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2674,
   "id": "802bee02-b733-438d-af94-a689b01aa37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# apply list of distances to data frame \n",
    "compounds[\"dist_stemmer\"] = np.NaN\n",
    "\n",
    "# for each word combination\n",
    "for w in dist_stemmer:\n",
    "    # retrieve index of first element \n",
    "    idx = compounds.index[compounds['second_part'] == w[0]][0]\n",
    "    # save both elements to new column \"dist_stemmer\"\n",
    "    compounds.dist_stemmer.loc[idx] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d920e7-d982-497c-845f-7351d1d75097",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.6 Combine Related Words\n",
    "In a next step, we combine the information we just retrieved by looking for \"stem duplicates\" with the information we got from the path similarity and the computation of distance. This leads us to a list of \"similar words\" which are either similar based on the similarity score or on the stem of the word. This information is saved to a new column `similar_words`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2675,
   "id": "cdbe2fea-30b1-4c45-989d-393720e2b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the following columns into new column \"similar_words\"\n",
    "\n",
    "mapping = {\"wup\": \"similar_words\",\n",
    "           \"share_cistem\": \"similar_words\",\n",
    "           \"share_porter\": \"similar_words\",\n",
    "           \"share_lancaster\": \"similar_words\",\n",
    "           \"dist_stemmer\": \"similar_words\"}\n",
    "\n",
    "compounds[\"similar_words\"] = compounds.groupby(mapping, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2676,
   "id": "6f2eb33d-67ff-4f6c-9b03-8920b6e0a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>stem_cistem</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>share_cistem</th>\n",
       "      <th>share_porter</th>\n",
       "      <th>share_lancaster</th>\n",
       "      <th>share_snowball</th>\n",
       "      <th>dist_stemmer</th>\n",
       "      <th>similar_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>abzockere</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivismu</td>\n",
       "      <td>aktivism</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivismus, aktivist]</td>\n",
       "      <td>[aktivismus, aktivist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>[jemand, irgendjemand, jeder beliebige]</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivi</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>akt</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivist, aktivismus]</td>\n",
       "      <td>[demagoge, macher, aktivist, aktivismus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>aktivisti</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aktivistin, aktivist]</td>\n",
       "      <td>[aktivistin, aktivist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>[Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alarm, alarmist]</td>\n",
       "      <td>[alarm, alarmist, alarm, alarmist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          original second_part                                 noun_forms  \\\n",
       "0  klimaabzockerei  abzockerei                 [abzockerei, abzockereien]   \n",
       "1  klimaaktivismus  aktivismus                               [aktivismus]   \n",
       "2    klimaaktivist    aktivist                     [aktivisten, aktivist]   \n",
       "3  klimaaktivistin  aktivistin                [aktivistinnen, aktivistin]   \n",
       "4       klimaalarm       alarm  [alarms, alarm, alarmen, alarme, alarmes]   \n",
       "\n",
       "        lemma genus                                     compound_forms  \\\n",
       "0  abzockerei     f               [klimaabzockerei, klimaabzockereien]   \n",
       "1  aktivismus     m                                  [klimaaktivismus]   \n",
       "2    aktivist     m                   [klimaaktivisten, klimaaktivist]   \n",
       "3  aktivistin     f              [klimaaktivistinnen, klimaaktivistin]   \n",
       "4       alarm     m  [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "\n",
       "                                       related_words  \\\n",
       "0  [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                 []   \n",
       "2  [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                 []   \n",
       "4  [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "\n",
       "                                           hypernyms  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2                        [Volksvertreter, Politiker]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "\n",
       "                                               roots  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2            [jemand, irgendjemand, jeder beliebige]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...   \n",
       "\n",
       "                                        en_hypernyms  ... stem_cistem  \\\n",
       "0  [robbery, stealing, thieving, theft, larceny, ...  ...  abzockerei   \n",
       "1                                                NaN  ...   aktivismu   \n",
       "2  [reformer, meliorist, crusader, social reforme...  ...      aktivi   \n",
       "3                                                NaN  ...   aktivisti   \n",
       "4  [fear, fearfulness, fright, emotion, feeling, ...  ...       alarm   \n",
       "\n",
       "  stem_porter stem_lancaster stem_snowball share_cistem share_porter  \\\n",
       "0  abzockerei      abzockere    abzockerei          NaN          NaN   \n",
       "1   aktivismu       aktivism    aktivismus          NaN          NaN   \n",
       "2    aktivist            akt      aktivist          NaN          NaN   \n",
       "3  aktivistin     aktivistin    aktivistin          NaN          NaN   \n",
       "4       alarm          alarm         alarm          NaN          NaN   \n",
       "\n",
       "     share_lancaster share_snowball            dist_stemmer  \\\n",
       "0                NaN            NaN                     NaN   \n",
       "1                NaN            NaN  [aktivismus, aktivist]   \n",
       "2                NaN            NaN  [aktivist, aktivismus]   \n",
       "3                NaN            NaN  [aktivistin, aktivist]   \n",
       "4  [alarm, alarmist]            NaN       [alarm, alarmist]   \n",
       "\n",
       "                              similar_words  \n",
       "0                                        []  \n",
       "1                    [aktivismus, aktivist]  \n",
       "2  [demagoge, macher, aktivist, aktivismus]  \n",
       "3                    [aktivistin, aktivist]  \n",
       "4        [alarm, alarmist, alarm, alarmist]  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85405836-704d-4f5c-9d2e-54d87c7fe15f",
   "metadata": {},
   "source": [
    "### Save and update compounds data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2677,
   "id": "7b1964c5-03fd-4d2f-bed5-6365b8d2824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update csv file \n",
    "compounds.to_csv(\"../output/compounds_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d26f0957-ada7-4dce-ae09-f54e2bb535d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test file \n",
    "#test = pd.read_csv(\"compounds.csv\")\n",
    "\n",
    "# replace all empty list strings with NaN\n",
    "#test.sim_words.replace('[]', np.NaN, inplace=True)\n",
    "\n",
    "# use this line to convert \"strings of lists\" to list of strings (must be done after importing the csv file\n",
    "#test['sim_words'] = test.sim_words.apply(convert_literal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca69a25-cc82-4563-8325-0ec51d3982a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Named Entity Recognition\n",
    "To extract named entities, we need to work with the context data frames that we created by retrieving the concordances in R, i.e. `pro_context` and `con_context`.\n",
    "\n",
    "They contain, inter alia, the following columns:\n",
    "- `pre`: contains up to 5 sentences that appear to the left of the key word phrase\n",
    "- `keyword`: the sentence containing the key word\n",
    "- `post`: contains up to 5 sentences that appear to the right of the key word phrase\n",
    "- `pattern`: the key word \n",
    "\n",
    "The extraction of named entities seeks to identify entities such as persons, organizations and locations from the input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8983a3e-a3da-4137-8faf-3e5e27f39d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Preprocessing\n",
    "\n",
    "In the following we preprocess the csv files we retrieved from R. The function `get_full_text` takes a data frame as its input and combines the strings contained in the three columns `pre`, `keyword` and `post` to generate the full text in which a key word (in our case the compound word) is found. \n",
    "(Additionally we reset and drop the index.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "id": "ec8e0039-f903-403a-b7fe-397a74618a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get full text (combination of pre, keyword and post column) for each keyword, i.e. compound word \n",
    "\n",
    "def get_full_text(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves the full text preceding and following the keyword phrase and saves full text to new column.\n",
    "    Arg: \n",
    "        df: the data frame containing the keyword-in-context information (columns \"pre\", \"keyword\" and \"post\").\n",
    "    Returns: \n",
    "        The original data frame with a new column \"full\" containing the combined text columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.replace(np.nan,'',regex=True)\n",
    "    \n",
    "    # create full text for each column\n",
    "    df[\"full\"] = df[\"pre\"] + df[\"keyword\"] + df[\"post\"]\n",
    "    \n",
    "    # convert full text column to string\n",
    "    df.full = df.full.astype(str)\n",
    "    \n",
    "    # group by pattern word (i.e. compound word)\n",
    "    #df['full'] = df[[\"pattern\", \"full\"]].groupby(['pattern'])['full'].transform(lambda x: ','.join(x))\n",
    "\n",
    "    # drop duplicates and save to new dataframe \n",
    "    #df = df[['pattern','full']].drop_duplicates()\n",
    "    #df = df.reset_index()\n",
    "    #df = df.drop(\"index\", axis =1)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "id": "7c09b23f-eaf4-4e36-abd1-1004499c1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to both data frames \n",
    "pro_context = get_full_text(pro_context)\n",
    "con_context = get_full_text(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6169bdd-e8d1-4d7b-82b9-a0cfac65dac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2 Retrieve Entities\n",
    "In the following functions we use spaCys Named Entity Recognition pipelines to retrieve the entities for our data frame. Additionally we create columns for `Persons` and `Organisations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "id": "a88b2282-fa79-4473-a5ad-097e9f436c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to retrieve information regarding named entities\n",
    "\n",
    "def get_ner(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text) # create spaCy nlp element\n",
    "    entities = [] # create empty list\n",
    "    try: \n",
    "        for ent in doc.ents: # iterate over entities in nlp element\n",
    "            entities.append([ent.text, ent.label_]) # and save word and according label as list\n",
    "    except:\n",
    "        entities.append(\"None\")\n",
    "        \n",
    "    return entities # return list of entities\n",
    "\n",
    "def get_persons(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities with the label \"PER\" from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The \"PER\" entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    persons = [] # create empty list\n",
    "    for label in text: # for each text-label pair \n",
    "        if label[1] == \"PER\": # if label is \"PERSON\"\n",
    "            persons.append(label[0]) # save text to list\n",
    "            \n",
    "    return set(persons) # return a set of the list to remove duplicates \n",
    "\n",
    "def get_organisations(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves entities with the label \"ORG\" from an input text.\n",
    "    Arg: \n",
    "        text: a string.\n",
    "    Returns: \n",
    "        The \"ORG\" entities for the given input string. \n",
    "    \"\"\"\n",
    "    \n",
    "    organisations = [] # create empty list\n",
    "    for label in text: # for each text-label pair \n",
    "        if label[1] == \"ORG\": # if label is \"ORGANISATION\"\n",
    "            organisations.append(label[0]) # save text to list\n",
    "            \n",
    "    return set(organisations) # return a set of the list to remove duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "id": "f011d850-da3d-40ff-85a5-8fe8ead8de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions to data frames - the following two lines take a while to run\n",
    "pro_context[\"entities\"] = pro_context.full.apply(get_ner)\n",
    "con_context[\"entities\"] = con_context.full.apply(get_ner)\n",
    "\n",
    "pro_context[\"persons\"] = pro_context.entities.apply(get_persons)\n",
    "con_context[\"persons\"] = con_context.entities.apply(get_persons)\n",
    "\n",
    "pro_context[\"organisations\"] = pro_context.entities.apply(get_organisations)\n",
    "con_context[\"organisations\"] = con_context.entities.apply(get_organisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b47a291-b85c-49de-9653-da655d97bf75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to csv file \n",
    "#pro_context.to_csv(\"../output/pro_ner.csv\", index = False)\n",
    "#con_context.to_csv(\"../output/con_ner.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2a2fd-ad31-45ae-a365-21158f38f95f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3 Cleaning \n",
    "\n",
    "To evaluate and clean the entities that were retrieved via `spacy` we create a list of unique persons and organizations from both data frames and manually check those lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "id": "b835a3c7-5c02-4b2d-a28e-a3f33f205366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERSONS\n",
    "# retrieve persons from both data frames\n",
    "pro_persons = pro_context.persons.apply(list).tolist()\n",
    "con_persons = con_context.persons.apply(list).tolist()\n",
    "persons = pro_persons + con_persons # concatenate lists \n",
    "persons = [pers for sublist in persons for pers in sublist] # flatten nested list\n",
    "persons = [s.strip('.') for s in persons] # clean list (remove \".\" symbol from beginning of string)\n",
    "persons = set(persons) # get list of unique persons\n",
    "\n",
    "### ORGANISATIONS\n",
    "# retrieve organizations from both data frames\n",
    "pro_org = pro_context.organisations.apply(list).tolist()\n",
    "con_org = con_context.organisations.apply(list).tolist()\n",
    "organisations = pro_org + con_org # concatenate lists \n",
    "organisations = [org for sublist in organisations for org in sublist] # flatten nested list\n",
    "organisations = [s.strip('.') for s in organisations] # clean list (remove \".\" symbol from beginning of string)\n",
    "organisations = set(organisations) # get list of unique persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "id": "8b1e2d92-4d43-492a-bdff-7c276fd45c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lists to text files \n",
    "\n",
    "# open file in write mode\n",
    "with open('../output/persons.txt', 'w') as file:\n",
    "    for item in persons:\n",
    "        # write each item on a new line\n",
    "        file.write(\"%s\\n\" % item)\n",
    "\n",
    "# open file in write mode\n",
    "with open('../output/organisations.txt', 'w') as file:\n",
    "    for item in organisations:\n",
    "        # write each item on a new line\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da243964-291e-41b5-b12d-3aff12483d64",
   "metadata": {},
   "source": [
    "Afterwards, we load the cleaned lists back into Python and apply those to our data frames to make sure we only keep the cleaned entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2140,
   "id": "d65c0b11-78fc-45e6-9ca3-951187ca5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back into python and compare to rows?\n",
    "persons_cleaned = pd.read_csv(\"../files/persons_cleaned.csv\", sep=\";\")\n",
    "persons_cleaned.spellings = persons_cleaned.spellings.replace(np.nan, \" \").str.split(\", \")\n",
    "persons_cleaned = persons_cleaned.explode('spellings')\n",
    "\n",
    "organisations_cleaned = pd.read_csv(\"../files/organisations_cleaned.csv\", sep=\";\")\n",
    "organisations_cleaned.spellings = organisations_cleaned.spellings.replace(np.nan, \" \").str.split(\", \")\n",
    "organisations_cleaned = organisations_cleaned.explode('spellings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2145,
   "id": "9ed920e7-9cf2-4650-b9a4-87e431b9fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_organisations(df):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function compares the organisation entities with a cleaned list of organisations and retrieves the full names.\n",
    "    Arg: \n",
    "        df: a data frame containing ORG entities \n",
    "    Returns: \n",
    "        A list of cleaned ORG entities for the data frame. \n",
    "    \"\"\"\n",
    "    \n",
    "    # initiate empty list\n",
    "    ORG = []\n",
    "    \n",
    "    # for each list of \"ORG\" entities in the data frame\n",
    "    for row in df.organisations:\n",
    "        \n",
    "        # initiate empty list\n",
    "        org = []\n",
    "        \n",
    "        # for each entitiy in the row\n",
    "        for entity in row:\n",
    "\n",
    "            # replace double whitespaces by single one and strip whitespaces from left and right end\n",
    "            entity_cleaned = entity.replace(\"  \", \" \").strip()\n",
    "            \n",
    "            # if entity matches a full name in our cleaned list\n",
    "            if entity_cleaned in organisations_cleaned.organisation.tolist():\n",
    "                full_name = entity_cleaned # entity equal the full name of the person\n",
    "                \n",
    "            # if entity matches one of the spellings in cleaned list\n",
    "            elif entity_cleaned in organisations_cleaned.spellings.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = organisations_cleaned[organisations_cleaned.spellings == entity_cleaned].organisation.values[0]\n",
    "                \n",
    "            \n",
    "            else:\n",
    "               # try:\n",
    "                    # try to find the entity in spelling variations\n",
    "                #    full_name = organisations_cleaned.loc[organisations_cleaned['spelling'].str.contains(entity_cleaned, case=False, na=False)].organisation.values[0]\n",
    "                    \n",
    "                # if not available\n",
    "               # except:\n",
    "               #     full_name = \" \" # return empty string\n",
    "        \n",
    "                full_name = \" \" # return empty string\n",
    "\n",
    "      \n",
    "            # append the full name to the list for each row \n",
    "            org.append(full_name)\n",
    "            \n",
    "            # remove the empty strings \n",
    "            org = [x for x in org if x != \" \"]\n",
    "\n",
    "        # append the list of full names of each row to the final list\n",
    "        ORG.append(set(org))\n",
    "\n",
    "    # return full list of names \n",
    "    return ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2161,
   "id": "22a8a102-ad64-4ce6-9ef9-a744fff9921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"ORG\"] = clean_organisations(pro_context)\n",
    "con_context[\"ORG\"] = clean_organisations(con_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2557,
   "id": "204344b1-ce86-4b08-9866-f3da950a7f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve PERSON entities\n",
    "\n",
    "def clean_persons(df):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function compares the person entities with a cleaned list of persons and retrieves the full names.\n",
    "    Arg: \n",
    "        df: a data frame containing PER entities \n",
    "    Returns: \n",
    "        A list of cleaned PER entities for the data frame. \n",
    "    \"\"\"\n",
    "\n",
    "    # initiate empty list\n",
    "    PERS = []\n",
    "\n",
    "    # for each list of \"PERS\" entities in the data frame\n",
    "    for row in df.persons:\n",
    "        \n",
    "        # initiate empty list\n",
    "        pers = []\n",
    "        \n",
    "        # for each entitiy in the row\n",
    "        for entity in row:\n",
    "\n",
    "            # normalize string: replace full stop symbols \n",
    "            entity_cleaned = entity.replace(\".\",\" \")\n",
    "            \n",
    "            # replace double whitespaces by single one and strip whitespaces from left and right end\n",
    "            entity_cleaned = entity_cleaned.replace(\"  \", \" \").strip()\n",
    "            \n",
    "            # remove digits\n",
    "            entity_cleaned = ''.join((x for x in entity_cleaned if not x.isdigit()))\n",
    "\n",
    "            # if entity matches a full name in our cleaned list\n",
    "            if entity_cleaned in persons_cleaned.full.tolist():\n",
    "                full_name = entity_cleaned # entity equal the full name of the person\n",
    "\n",
    "            # if entity matches a last name in our cleaned list\n",
    "            elif entity_cleaned in persons_cleaned.last_name.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.last_name == entity_cleaned].full.values[0]\n",
    "\n",
    "            # if entity matches one of the spellings in cleaned list\n",
    "            elif entity_cleaned in persons_cleaned.spellings.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.spellings == entity_cleaned].full.values[0]\n",
    "                \n",
    "            # if entity is in potential genitive form (has an \"s\" at the end), e.g. \"Greta Thunbergs\"\n",
    "            # strip the s from the end of the string and check again for matches to full names \n",
    "            elif entity_cleaned.rstrip(\"s\") in persons_cleaned.full.tolist():\n",
    "                full_name = entity_cleaned.strip(\"s\")\n",
    "\n",
    "            # if entity is in potential genitive form (has an \"s\" at the end), e.g. \"Greta Thunbergs\"\n",
    "            # strip the s from the end of the string and check again for matches to last names\n",
    "            elif entity_cleaned.rstrip(\"s\") in persons_cleaned.last_name.tolist():\n",
    "                # retrieve the full name from cleaned list\n",
    "                full_name = persons_cleaned[persons_cleaned.last_name == (entity_cleaned.strip(\"s\"))].full.values[0]\n",
    "\n",
    "            # if no match is found\n",
    "            else:\n",
    "                full_name = \" \" # return empty string\n",
    "\n",
    "            # append the full name to the list for each row \n",
    "            pers.append(full_name)\n",
    "            \n",
    "            # remove the empty strings \n",
    "            pers = [x for x in pers if x != \" \"]\n",
    "\n",
    "        # append the list of full names of each row to the final list\n",
    "        PERS.append(set(pers))\n",
    "\n",
    "    # return full list of names \n",
    "    return PERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2558,
   "id": "0d1d5722-e60a-490e-b347-407fd271242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"PERS\"] = clean_persons(pro_context)\n",
    "con_context[\"PERS\"] = clean_persons(con_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e4ff6-4a9c-4f5b-89a5-e02ada5a25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pro_context.insert(12, \"ORG\", \" \")\n",
    "#con_context.insert(12, \"ORG\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2165,
   "id": "93917551-5981-47b0-86a1-1ace25b8bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file \n",
    "#pro_context.to_csv(\"../output/pro_ner.csv\", index = False)\n",
    "#con_context.to_csv(\"../output/con_ner.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db903c-6385-49fa-8e4d-fcfe63f53961",
   "metadata": {},
   "source": [
    "Save the final information we need for the definitions to the `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2566,
   "id": "a4e69ae5-90d0-4bca-8a38-3069984567b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty values with nan\n",
    "pro_context.PERS = pro_context.PERS.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "pro_context.ORG = pro_context.ORG.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "# convert values into list\n",
    "pro_context.PERS = pro_context.PERS.map(list, na_action='ignore')\n",
    "pro_context.ORG = pro_context.ORG.map(list, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2567,
   "id": "652b9665-f7af-42f2-bdd0-b6652e75d3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace empty values with nan\n",
    "con_context.PERS = con_context.PERS.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "con_context.ORG = con_context.ORG.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "# convert values into list\n",
    "con_context.PERS = con_context.PERS.map(list, na_action='ignore')\n",
    "con_context.ORG = con_context.ORG.map(list, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2596,
   "id": "20805375-f7f4-4ab9-803a-02e14e66b13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve dictionary with PERSONS and ORGANISATIONS with each compound as a key and entities as values\n",
    "pro_PERS = pro_context[pro_context.PERS.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"PERS\"].apply(list).to_dict()\n",
    "pro_ORG = pro_context[pro_context.ORG.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"ORG\"].apply(list).to_dict()\n",
    "\n",
    "# retrieve dictionary with PERSONS and ORGANISATIONS with each compound as a key and entities as values\n",
    "con_PERS = con_context[con_context.PERS.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"PERS\"].apply(list).to_dict()\n",
    "con_ORG = con_context[con_context.ORG.isna() == False].groupby(by=[\"pattern\"], dropna=True)[\"ORG\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3029,
   "id": "ff33f632-d1d7-4de2-ae45-fb3b954429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add information to compound data frame \n",
    "compounds['PERS_pro']= compounds['original'].map(pro_PERS)\n",
    "compounds['ORG_pro']= compounds['original'].map(pro_ORG)\n",
    "\n",
    "compounds['PERS_con']= compounds['original'].map(con_PERS)\n",
    "compounds['ORG_con']= compounds['original'].map(con_ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3031,
   "id": "eb853f85-31cf-478c-8901-79ef367c6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function flattens a nested list. \n",
    "    Arg: \n",
    "        nested_list: a nested list of the format [[item1],[item2]].\n",
    "    Returns: \n",
    "        A flattened version of the nested list, e.g. [item1,item2], else NaN.\n",
    "     \"\"\"       \n",
    "    \n",
    "    try:\n",
    "        return [item for sublist in nested_list for item in sublist]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3032,
   "id": "35fd5b13-0575-4820-a406-479f4fb2b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten entity lists \n",
    "compounds[\"PERS_pro\"] = compounds.PERS_pro.apply(flatten_list)\n",
    "compounds[\"PERS_con\"] = compounds.PERS_con.apply(flatten_list)\n",
    "compounds[\"ORG_pro\"] = compounds.ORG_pro.apply(flatten_list)\n",
    "compounds[\"ORG_con\"] = compounds.ORG_con.apply(flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3034,
   "id": "e5743541-af7d-4448-b6fb-533eadc83d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compounds.to_csv(\"../output/compounds_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080abc8-e7b8-48cc-8916-e0b53816ce56",
   "metadata": {},
   "source": [
    "## 3.4 Visualization of the Entity Recognition\n",
    "In the following we will have a look at the specific procedure that is done by `spacys` NER pipeline. To visualize an example, let's have a look at the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1823,
   "id": "ca7b768b-1245-46ff-9057-d3e935d76610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex1 = \"Gerrit Hansen von der Klimaaktivistengruppe Germanwatch.\"\n",
    "ex2 = \"Dank Greta und FFF ist endlich Bewegung in den Stillstand bei der Klimarettung gekommen .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "id": "892d85b1-1a80-4524-8c1c-0e9f006ac77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greta 5 10 PER\n",
      "FFF 15 18 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(ex2)\n",
    "\n",
    "# for each entity\n",
    "for e in doc.ents:\n",
    "    # print text, start and end character, label\n",
    "    print(e.text, e.start_char, e.end_char, e.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "id": "a60259d6-449a-4389-906b-5415cbc5b600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                   BIO Tag         Entity               \n",
      "Dank                      O                                    \n",
      "Greta                     B               PER                  \n",
      "und                       O                                    \n",
      "FFF                       B               ORG                  \n",
      "ist                       O                                    \n",
      "endlich                   O                                    \n",
      "Bewegung                  O                                    \n",
      "in                        O                                    \n",
      "den                       O                                    \n",
      "Stillstand                O                                    \n",
      "bei                       O                                    \n",
      "der                       O                                    \n",
      "Klimarettung              O                                    \n",
      "gekommen                  O                                    \n",
      ".                         O                                    \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':{23}} {'BIO Tag':{15}} {'Entity':{20}} \")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{25}} {token.ent_iob_:{13}}   {token.ent_type_:{20}} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "id": "61365225-08f3-413b-9047-0b5982bf720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dank \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " und \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FFF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " ist endlich Bewegung in den Stillstand bei der Klimarettung gekommen .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0840f9-72d9-464c-a515-04710ce206af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Dependency Parsing\n",
    "- [X] get dependencies of pro_sent_df and con_sent_df\n",
    "- [X] check which words are dependent on our compounds \n",
    "\n",
    "\n",
    "German/English dependency labels\n",
    "https://github.com/explosion/spaCy/blob/master/spacy/glossary.py  \n",
    "https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/annotation/tiger_scheme-syntax.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1547842-c96e-4bc2-8ce2-679e21615730",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1 Retrieve Dependencies\n",
    "\n",
    "Lets retrieve dependency information with the `spaCy` library for our context data frames. \n",
    "\n",
    "The function `get_dependencies_recursive` retrieves all modifiers recursively to also check for cases such as found in: *\"Über den weltweit bekanntesten (und wohl aggressivsten) Klimaaktivisten Bill McKibben\"*\n",
    "\n",
    "We want *weltweit*, *bekanntesten*, *wohl* and *aggressivsten* to be found as well. Therefore the following function takes heads which are modifiers of the compound and looks recursively for their dependents too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1827,
   "id": "06dcafa9-dcca-470f-9f19-be7ca4271c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dependencies_recursive(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function recursively retrieves the dependencies of words being dependent of a compound word. \n",
    "    Arg: \n",
    "        df: a data frame containing the compound words for which we want to count the POS tags of the dependent words.\n",
    "    Returns: \n",
    "        A nested dictionary consisting of the compound words as keys and the dependency information, \n",
    "        i.e. the dependent word, the POS tag of the dependent word, the dependency tag, tag explanation, the sentence itself.\n",
    "    \"\"\"   \n",
    "    \n",
    "    deps = dict.fromkeys(set(df.pattern)) # initiate a dictionary with the compounds as keys\n",
    "    mods = [\"mo\", \"mnr\", \"nk\"] # the list of dependency tags we are interested in\n",
    "    pos = [\"ADJA\", \"ADJD\", \"PAV\", \"PROAV\", \"PDAT\", \"PIAT\", \"PIDAT\",\n",
    "          \"PPOSAT\", \"PRELAT\", \"PTKA\", \"PWAT\", \"PWAV\", \"ADJ\", \"ADP\", \"ADV\"] # the list of POS tags we are interested in\n",
    "    \n",
    "    # iterate over rows of data frame \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #deps[row[\"pattern\"]] = list() # initiate empty list for each key in dict\n",
    "        doc = nlp(row[\"keyword\"]) # retrieve sentence with keyword for according row\n",
    "\n",
    "        # for each token in the sentence \n",
    "        for token in doc:\n",
    "            \n",
    "            # check for words being dependent on our compound word, i.e. the compound is the head \n",
    "            if str(token.head.text).lower().startswith(row[\"pattern\"].lower()):\n",
    "                \n",
    "                # if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                if token.dep_ in mods and token.tag_ in pos:\n",
    "                    \n",
    "                    # if key has no values so far\n",
    "                    if deps[row[\"pattern\"]] == None:\n",
    "                        \n",
    "                        # initiate empty list for each key in dict\n",
    "                        deps[row[\"pattern\"]] = list()\n",
    "                \n",
    "                    else:\n",
    "                        pass\n",
    "                        \n",
    "                    # append information (lemma, token, pos tag, dependency tag, head word, tag explanation and sentence) to dependency list\n",
    "                    deps[row[\"pattern\"]].append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text, str(spacy.explain(token.dep_)), doc])\n",
    "                        \n",
    "                    # use the new found word as new head \n",
    "                    new_head = token.text\n",
    "                    \n",
    "                    # check if we have words that are dependent on our new head\n",
    "                    for token in doc:\n",
    "                        \n",
    "                        # if yes\n",
    "                        if token.head.text == new_head:\n",
    "                            \n",
    "                            # and if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                            if token.dep_ in mods and token.tag_ in pos:\n",
    "                                \n",
    "                                # append it to our list\n",
    "                                deps[row[\"pattern\"]].append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text, str(spacy.explain(token.dep_)), doc])\n",
    "\n",
    "            # if we have a conjunct of one of the modifiers and another modifier\n",
    "            if token.head.text == \"und\" and token.tag_ in pos and token.dep_ == \"cj\":\n",
    "                \n",
    "                # if key already has values \n",
    "                if deps[row[\"pattern\"]] != None:\n",
    "                    \n",
    "                    # append to list\n",
    "                    deps[row[\"pattern\"]].append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text, str(spacy.explain(token.dep_)), doc])\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                # use token as new head \n",
    "                next_head = token.text\n",
    "                \n",
    "                # check for dependent words \n",
    "                for token in doc:\n",
    "                    \n",
    "                    # if yes\n",
    "                    if token.head.text == next_head: \n",
    "                        \n",
    "                        # and if the token is a modifier (i.e. token has one of the modifier tags and one of the POS tags that we defined above)\n",
    "                        if token.dep_ in mods and token.tag_ in pos:\n",
    "                            \n",
    "                            # if key already has values\n",
    "                            if deps[row[\"pattern\"]] != None:\n",
    "                                \n",
    "                                # append information to list\n",
    "                                deps[row[\"pattern\"]].append([token.lemma_, token.text, token.tag_, token.dep_, token.head.text, str(spacy.explain(token.dep_)), doc])\n",
    "                                \n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "                \n",
    "    # return final dictionary with dependency information for each compound in the data frame\n",
    "    return deps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fe5cb-7f25-4a3f-b626-d242dbb0dad9",
   "metadata": {},
   "source": [
    "#### 4.2 Retrieve Modifiers from Dependencies\n",
    "\n",
    "Next, since the `get_dependencies_recursive` function outputs a dictionary full of dependency information for each compound, we use the function `get_mods` to retrieve the modifiers from the `get_dependencies_recursive` output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "id": "4cd40f7a-ebac-45cf-b0a0-5fbba8b05b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mods(column):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the modifiers of the column created via the \"get_dependencies_recursive\" above. \n",
    "    Arg: \n",
    "        df: a data frame column containing list of dependencies from which we want to retrieve the very first element.\n",
    "    Returns: \n",
    "        A list of modifier words for each compound word if possible, else 0.\n",
    "    \"\"\" \n",
    "    \n",
    "    mods = [] # initiate empty list\n",
    "    \n",
    "    # for each list of dependencies\n",
    "    for deps in column:\n",
    "        \n",
    "        # if list ist empty\n",
    "        if column == \"[]\":\n",
    "            return 0 # return 0\n",
    "           \n",
    "        else:\n",
    "            # get first element and save to new list\n",
    "            mods.append(deps[0])\n",
    "            \n",
    "    return mods # return list of modifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760b532-8c66-46cd-bff5-4fe89fefd04f",
   "metadata": {},
   "source": [
    "Let's apply both functions and save the dependency information to a new column `dependencies` and the list of modifiers to a new column `modifiers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "id": "c4ce9c36-b9db-41a5-9467-fe529bc6e7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply function to data frames \n",
    "con_deps = get_dependencies_recursive(con_context)\n",
    "pro_deps = get_dependencies_recursive(pro_context)\n",
    "\n",
    "# retrieve dependencies and save to new column in our data frame \n",
    "pro_context[\"dependencies\"] = pro_context['pattern'].map(pro_deps)\n",
    "con_context[\"dependencies\"] = con_context['pattern'].map(con_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "id": "94bf5419-234b-4568-9a04-e4e22273f55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn None into empty list \n",
    "pro_context.fillna(value=\"[]\", inplace=True)\n",
    "con_context.fillna(value=\"[]\", inplace=True)\n",
    "\n",
    "# apply modifier function to find all modifiers and save to new column \"modifiers\"\n",
    "pro_context[\"modifiers\"] = pro_context.dependencies.apply(get_mods)\n",
    "con_context[\"modifiers\"] = con_context.dependencies.apply(get_mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e206c-9133-4c05-9279-209868bf9723",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3 Visualization of the Process\n",
    "\n",
    "To visualize what just happened in the functions we applied before, let's have a look at the following code. We will use the previously mentioned example phrase from the C2022 corpus: \n",
    "*Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben*\n",
    "\n",
    "For this phrase, we will retrieve the *tokens*, *dependency labels*, the according *heads* and a brief *explanation* of the dependency tag. The ouput is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "id": "fb9c7055-ded7-4b37-86ab-0c7a6ec7a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           Dependence      Head Text             Dependency Explained \n",
      "Über            ROOT =>         Über                  root \n",
      "den             nk =>           Klimaaktivisten       noun kernel element \n",
      "weltweit        mo =>           bekanntesten          modifier \n",
      "bekanntesten    nk =>           Klimaaktivisten       noun kernel element \n",
      "und             cd =>           bekanntesten          coordinating conjunction \n",
      "wohl            mo =>           aggressivsten         modifier \n",
      "aggressivsten   cj =>           und                   conjunct \n",
      "Klimaaktivisten nk =>           Über                  noun kernel element \n",
      "Bill            pnc =>          McKibben              proper noun component \n",
      "McKibben        nk =>           Klimaaktivisten       noun kernel element \n"
     ]
    }
   ],
   "source": [
    "text = \"Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben\"\n",
    "\n",
    "print(f\"{'Token':{15}} {'Dependence':{15}} {'Head Text':{20}}  {'Dependency Explained'} \")\n",
    "for token in nlp(text):\n",
    "     print(f\"{token.text:{15}} {token.dep_+' =>':{13}}   {token.head.text:{20}}  {spacy.explain(token.dep_)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f5404-7445-444a-b826-6be2f2394eed",
   "metadata": {},
   "source": [
    "Heads are tagges as `ROOT`, a full list of the remaining dependency tags can be found here: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "\n",
    "Our function `get_dependencies_recursive` now seeks to retrieve all words that are being dependent on our compound word (and that are an adjective or adverb).\n",
    "\n",
    "Accordingly, for this example, our code retrieves the words (in this order):\n",
    "\n",
    "**bekanntesten** => **weltweit**  \n",
    "**aggressivsten** => **wohl** \n",
    "\n",
    "The syntactic structure of the phrase can also be visualized as a graph of dependencies as given here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "id": "9517ac56-0220-4dd8-8e39-1abfab8d459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"de\" id=\"aa90bc8723274acc985cd1363eb82bbb-0\" class=\"displacy\" width=\"1800\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Über</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">den</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">weltweit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">bekanntesten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">und</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">wohl</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">aggressivsten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Klimaaktivisten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Bill</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">McKibben</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-0\" stroke-width=\"2px\" d=\"M245,439.5 C245,89.5 1270.0,89.5 1270.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-1\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cd</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M730.0,441.5 L738.0,429.5 722.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-4\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mo</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-5\" stroke-width=\"2px\" d=\"M770,439.5 C770,264.5 1085.0,264.5 1085.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,441.5 L1093.0,429.5 1077.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-6\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 1275.0,2.0 1275.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,441.5 L1283.0,429.5 1267.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pnc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-aa90bc8723274acc985cd1363eb82bbb-0-8\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-aa90bc8723274acc985cd1363eb82bbb-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,441.5 L1618.0,429.5 1602.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex1 = \"Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben\"\n",
    "\n",
    "doc = nlp(ex1)\n",
    "displacy.render(doc, style=\"dep\")\n",
    "\n",
    "# to save the plot please un-comment the following lines\n",
    "\n",
    "#dep_plot = displacy.render(doc, style='dep', jupyter=False)\n",
    "#output_path = Path(\"../../plots/dependency_plot.svg\")\n",
    "#output_path.open(\"w\", encoding=\"utf-8\").write(dep_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1e7a0-ffdd-42bc-adfe-d9e703672bc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2 Save Information\n",
    "\n",
    "Save the information to the data frame `pro_full` and `con_full` (these data frames already contain the information from the Named-Entity Recognition part).\n",
    "\n",
    "- the new column `dependencies` contains the dependencies of words being dependent on the compound word in a list format.\n",
    "- the new column `modifiers` contains the modifiers retrieved from the `dependencies` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "1a9cdcb8-a724-44dc-94b5-fb6c753fb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new information to csv files\n",
    "pro_ner_df.to_csv(\"textmining/pro_full.csv\", index = False)\n",
    "con_ner_df.to_csv(\"textmining/con_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b768c-d85a-4c40-a9be-7a968cd48ee0",
   "metadata": {},
   "source": [
    "Write necessary information about the modifiers to the `compounds` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2624,
   "id": "4d42f200-d81a-4f34-a558-e62a76ec775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve dictionary with MODIFIERS with each compound as a key and entities as values\n",
    "pro_mods = pro_context.set_index('pattern').to_dict()['modifiers']\n",
    "con_mods = con_context.set_index('pattern').to_dict()['modifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "f2c92c2f-d781-43ef-a750-7316d913c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = \"Über den weltweit bekanntesten und wohl aggressivsten Klimaaktivisten Bill McKibben\"\n",
    "\n",
    "doc = nlp(ex1)\n",
    "dep_plot = displacy.render(doc, style='dep', jupyter=False)\n",
    "\n",
    "#output_path = Path(\"../../plots/dependency_plot.svg\")\n",
    "#output_path.open(\"w\", encoding=\"utf-8\").write(dep_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5ee4a-2e4e-487e-942f-e23cfae87b33",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Sentiment Analysis\n",
    "- [X] get tokens\n",
    "- [X] apply stop lists (optional)\n",
    "- [X] get sentiment of pre and post sentences\n",
    "- [X] save sentiment to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "id": "8f056f2a-690d-4d2a-92cc-da83b53321fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/anna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# load stop words from nltk\n",
    "#stop_words = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2449d-ae59-4f4c-a872-7309a56ed3c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.1 Apply Sentiment Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce9ca3-3dbd-4d73-975c-becad6bbc4fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1.1 German BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "id": "5b86c517-89bd-4fce-95fb-04fb3ec6a0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "model = SentimentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "id": "93ee3b45-502c-436d-ac7b-a4beef85e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the polarity of the German Bert model. \n",
    "    Arg: \n",
    "        text: a string for which we want to retrieve a polarity label.\n",
    "    Returns: \n",
    "        A polarity label, i.e. \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"   \n",
    "    \n",
    "    # retrieve polarity from German Bert model\n",
    "    pol = model.predict_sentiment([str(text)])\n",
    "        \n",
    "    return pol[0] # return polarity label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "6eea717d-f4e0-4f80-b03a-9965b279cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_bert(\"ich habe einen guten tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "id": "11124fd5-85b6-41b9-9439-6c4333237b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frames\n",
    "pro_context[\"bert\"] = pro_context.full.apply(get_bert)\n",
    "con_context[\"bert\"] = con_context.full.apply(get_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1841,
   "id": "12347655-8145-431f-8bb6-e63292dbe1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new information to csv files\n",
    "#pro_context.to_csv(\"../output/pro_ner.csv\", index = False)\n",
    "#con_context.to_csv(\"../output/con_ner.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e95e0-9d59-40de-ad05-b65f45661744",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1.2 TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1842,
   "id": "19c127a4-592d-4d76-be21-7cf2c3c3b134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "id": "dba7e3af-ecc2-4607-a8cf-732075dc0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the polarity of the TextBlob model. \n",
    "    Arg: \n",
    "        text: a string for which we want to retrieve a polarity label.\n",
    "    Returns: \n",
    "        A sentiment score, ranging from -1.0 to 1.0\n",
    "    \"\"\"   \n",
    "        \n",
    "    # retrieve sentiment from TextBlob\n",
    "    blob = TextBlob(str(text))\n",
    "    \n",
    "    return blob.sentiment[0] # return sentiment score \n",
    "        \n",
    "\n",
    "def convert_sentiment(value, threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function converts the output of get_blob into discrete polarity labels, depending on the threshold that is set. \n",
    "    Arg: \n",
    "        value: a sentiment score, ranging from -1.0 to 1.0\n",
    "        threshold: a threshold score, ranging from -1.0 to 1.0\n",
    "    Returns: \n",
    "        A polarity label, i.e. \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"   \n",
    "    \n",
    "    # if sentiment score is higher or equal to threshold\n",
    "    if value >= threshold:\n",
    "        label = \"positive\" # polarity = positive\n",
    "        \n",
    "    # if sentiment score is lower or equal to the negative version of the threshold\n",
    "    if value <= -(threshold):\n",
    "        label = \"negative\" # polarity = negative\n",
    "        \n",
    "    # if sentiment score is between the negative version and the positive version of the threshold\n",
    "    else:\n",
    "        label = \"neutral\" # polarity = neutral\n",
    "        \n",
    "    return label # return label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "id": "0043d2ad-66c5-42ab-b743-bd6f762d95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to data frame \n",
    "pro_context[\"blob\"] = pro_context.full.apply(get_blob)\n",
    "con_context[\"blob\"] = con_context.full.apply(get_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625c71b-84ab-482b-8600-8ae4e15962bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3 Evaluate Sentiment Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92c579-338c-4aed-8dfa-a8d43a739074",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3.1 Check Sentiment\n",
    "In the following we will retrieve the texts where our sentiment tools return different polarities. These entries will be checked manually for its sentiment. Afterwards we can potentially decide which sentiment tool is better for our text type. \n",
    "\n",
    "Firstly, we will convert the polarity scores we retrieved via the TextBlob tool into discrete labels: `negative`, `neutral`, `positive` to be able to compare them to the polarity labels we got via GermanBert. \n",
    "\n",
    "To see which threshold for TextBlob gives us the most similar polarity labels (compared to the GermanBert output), different threshold values (0.0, 0.1, 0.2, 0.3, 0.4) were tested for the conversion into the discrete label format. This is done in the `test_parameters` function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2167,
   "id": "f5710a80-1ca7-451d-bec8-ceae2db4dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameters(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function tests different threshold parameters and applies the convert_sentiment function to retrieve an output for each threshold value. \n",
    "    It counts for each threshold value how many polarity labels are different for the TextBlob and the GermanBert output. \n",
    "    Arg: \n",
    "        df: a data frame.\n",
    "    Returns: \n",
    "        An array consisting of threshold values and the according difference count.\n",
    "    \"\"\"   \n",
    "    \n",
    "    diff_scores = [] # initiate empty list\n",
    "    \n",
    "    # for different threshold values\n",
    "    for x in np.arange(0.0, 0.50, 0.10):\n",
    "        \n",
    "        # get discrete labels\n",
    "        df[\"blob_bin\"] = df.blob.apply(convert_sentiment, threshold = round(x,2))\n",
    "        \n",
    "        # compute difference of columns\n",
    "        diff = len(df[(df['bert'] == df['blob_bin']) == False])\n",
    "        \n",
    "        # save scores to list in format: threshold, difference count\n",
    "        diff_scores.append([round(x,2), diff])\n",
    "        \n",
    "    # return minimum of difference count and list of difference counts\n",
    "    return min(diff_scores, key=lambda x: x[1]), diff_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2168,
   "id": "52a09868-e078-4e94-a9a9-273e4d3a67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to data frames \n",
    "pro_diff = test_parameters(pro_context)\n",
    "con_diff = test_parameters(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d8972-9a84-403d-b7a9-f832d27d6714",
   "metadata": {},
   "source": [
    "Now, we can have a look at the difference scores that the range of parameters gives us (the first value is the threshold parameter, the second value is the difference count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2171,
   "id": "9980d53f-cd48-469a-973f-70fe35719819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contra data frame differences:\n",
      "\n",
      "   threshold    difference count\n",
      "-----------  ------------------\n",
      "        0                   753\n",
      "        0.1                 615\n",
      "        0.2                 550\n",
      "        0.3                 519\n",
      "        0.4                 515\n",
      "\n",
      "Pro data frame differences:\n",
      "\n",
      "   threshold    difference count\n",
      "-----------  ------------------\n",
      "        0                   279\n",
      "        0.1                 212\n",
      "        0.2                 188\n",
      "        0.3                 176\n",
      "        0.4                 173\n"
     ]
    }
   ],
   "source": [
    "print(\"Contra data frame differences:\\n\\n\", tabulate(con_diff[1], headers=[\"threshold\", \"difference count\"]))\n",
    "print(\"\\nPro data frame differences:\\n\\n\", tabulate(pro_diff[1], headers=[\"threshold\", \"difference count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd410e37-a806-431e-8a43-6cef984b1cc7",
   "metadata": {},
   "source": [
    "The tables clearly show that a threshold of 0.4 would give the least differences between both sentiment models. Accordingly, we apply the conversion of the `TextBlob` labels with a threshold of 0.4. This means that all values smaller than -0.4 will be considered *negative*, all value between -0.4 and + 0.4 will be considered *neutral* and all values higher than + 0.4 are indicated as *positive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2174,
   "id": "ec174f0b-c6fe-4977-9fc8-fb06c4992c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert continuous sentiment score to discrete score \n",
    "con_context[\"blob_labels\"] = con_context.blob.apply(convert_sentiment, threshold = 0.4)\n",
    "pro_context[\"blob_labels\"] = pro_context.blob.apply(convert_sentiment, threshold = 0.4)\n",
    "\n",
    "# retrieve columns where we have different sentiment scores \n",
    "con_sentiment_diff = con_context[(con_context['bert'] == con_context['blob_labels']) == False]\n",
    "pro_sentiment_diff = pro_context[(pro_context['bert'] == pro_context['blob_labels']) == False]\n",
    "\n",
    "# save new information to csv files\n",
    "con_sentiment_diff.to_csv(\"../evaluation/con_sentiment_diff.csv\", index = False)\n",
    "pro_sentiment_diff.to_csv(\"../evaluation/pro_sentiment_diff.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0291e-f7fc-43da-a766-10e396ccb9fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3.2 Retrieve main Polarity for each Klima Compound\n",
    "\n",
    "In the end, we want to retrieve the prevailing polarity for each compound. For the manual evaluation of the polarity labels we now transform the data frame into a different format and get the full text of concordances for each compound word. This is done is `convert_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2178,
   "id": "0ba52437-5cc6-488f-bf71-4895e4a952b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe(df):\n",
    "    \"\"\"\n",
    "    This function groups a data frame by their key word (pattern) and concordances (i.e. context texts). \n",
    "    Arg: \n",
    "        df: a data frame.\n",
    "    Returns: \n",
    "        A grouped data frame with a column for the key word (pattern) and a column containing all concordances for the according key word (text_by_compound)\n",
    "    \"\"\"   \n",
    "    \n",
    "    # group data frame by pattern and the full text column\n",
    "    df[\"text_by_compound\"] =  df[[\"pattern\", \"full\"]].groupby(['pattern'])['full'].transform(lambda x: '//'.join(x))\n",
    "    # drop duplicates\n",
    "    df = df[['pattern','text_by_compound']].drop_duplicates()\n",
    "    # reset index\n",
    "    df = df.reset_index()\n",
    "    # drop index column\n",
    "    df = df.drop(\"index\", axis = 1)\n",
    "    # split text \n",
    "    df[\"text_by_compound\"] = df[\"text_by_compound\"].apply(lambda x: x.split(\"//\"))\n",
    "\n",
    "    return df # return grouped data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2179,
   "id": "550615f5-3e1f-45f9-b6dd-7737befad523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to data frame\n",
    "pro_sentiment = convert_dataframe(pro_context)\n",
    "con_sentiment = convert_dataframe(con_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e332186-ce75-439d-91af-d2dd76daf294",
   "metadata": {},
   "source": [
    "This is how the grouped data frame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2181,
   "id": "081cb0fc-a218-491c-bb63-34d6747d8f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>text_by_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>[Werner Kirstein Universität Leipzig Kirstein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>[Die SojalatteAdabeis der grünen Stadtbiotope ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>[Von diesem Beitrag werden Sie bei LeitMedien ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>[Klimaaktivisten sind aus anderem Holz geschni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>[Es ist die marxistische WassermelonenAgenda g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pattern                                   text_by_compound\n",
       "0  klimaabzockerei  [Werner Kirstein Universität Leipzig Kirstein ...\n",
       "1  klimaaktivismus  [Die SojalatteAdabeis der grünen Stadtbiotope ...\n",
       "2    klimaaktivist  [Von diesem Beitrag werden Sie bei LeitMedien ...\n",
       "3  klimaaktivistin  [Klimaaktivisten sind aus anderem Holz geschni...\n",
       "4       klimaalarm  [Es ist die marxistische WassermelonenAgenda g..."
      ]
     },
     "execution_count": 2181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbf597-7257-4a75-ba5e-3f1cfe8ccaad",
   "metadata": {},
   "source": [
    "To get the most common polarity label for each compound, we now group the data frames by pattern and polarity label and save the information to the transformed data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2182,
   "id": "2642439f-f422-48ae-b1e1-f03539d0a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common polarity label for each compound\n",
    "pro_sentiment_bert = pro_context.groupby(['pattern'])['bert'].max().tolist()\n",
    "con_sentiment_bert = con_context.groupby(['pattern'])['bert'].max().tolist()\n",
    "pro_sentiment_blob = pro_context.groupby(['pattern'])['blob_bin'].max().tolist()\n",
    "con_sentiment_blob = con_context.groupby(['pattern'])['blob_bin'].max().tolist()\n",
    "\n",
    "# and save to new column in info data frame \n",
    "pro_sentiment[\"bert\"] = pro_sentiment_bert\n",
    "con_sentiment[\"bert\"] = con_sentiment_bert\n",
    "\n",
    "pro_sentiment[\"blob\"] = pro_sentiment_blob\n",
    "con_sentiment[\"blob\"] = con_sentiment_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558a5ad-dde6-45dc-99a7-8cc9065a9fe0",
   "metadata": {},
   "source": [
    "The polarity labels that we retrieved before are now given in our tranformed data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2183,
   "id": "b432ec42-24ae-4333-99a8-8ecdef90bfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>text_by_compound</th>\n",
       "      <th>bert</th>\n",
       "      <th>blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>[Aber das macht nichts . Denn Du kannst trotzd...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>[Why should I be studying for a future that so...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>[Why should I be studying for a future that so...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaasyl</td>\n",
       "      <td>[Sie haben im Januar eine Podiumsdiskussion or...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaaufschrei</td>\n",
       "      <td>[Beispielsweise mittels zivilem Ungehorsam und...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pattern                                   text_by_compound  \\\n",
       "0  klimaaktivismus  [Aber das macht nichts . Denn Du kannst trotzd...   \n",
       "1    klimaaktivist  [Why should I be studying for a future that so...   \n",
       "2  klimaaktivistin  [Why should I be studying for a future that so...   \n",
       "3        klimaasyl  [Sie haben im Januar eine Podiumsdiskussion or...   \n",
       "4   klimaaufschrei  [Beispielsweise mittels zivilem Ungehorsam und...   \n",
       "\n",
       "       bert     blob  \n",
       "0   neutral  neutral  \n",
       "1  positive  neutral  \n",
       "2  positive  neutral  \n",
       "3   neutral  neutral  \n",
       "4   neutral  neutral  "
      ]
     },
     "execution_count": 2183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2189,
   "id": "fc6d4de6-2f3e-4f14-8574-762262488e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new information to csv files\n",
    "#con_sentiment.to_csv(\"../output/con_sentiment.csv\", index = False)\n",
    "#pro_sentiment.to_csv(\"../output/pro_sentiment.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c2360-1068-4b5b-bf85-61ec21f6b120",
   "metadata": {},
   "source": [
    "Next, we want to retrieve for which compound words the polarity labels we obtained from `TextBlob` and `GermanBert` differ and save those rows to a csv file (`con_sentiment_diff_by_compound.csv` and `pro_sentiment_diff_by_compound.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2184,
   "id": "df9dccad-3b0a-4259-be5e-348ce579fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve columns where we have different sentiment scores \n",
    "con_sentiment_diff = con_sentiment[(con_sentiment['bert'] == con_sentiment['blob']) == False]\n",
    "pro_sentiment_diff = pro_sentiment[(pro_sentiment['bert'] == pro_sentiment['blob']) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d350-8aad-4c3d-a2e1-2d849a371523",
   "metadata": {},
   "source": [
    "This is the case for 13 compounds for the P2022 and 56 compound for the C2022 corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2188,
   "id": "81251fa6-af95-4de7-8cf0-a37af639e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds with different sentiment labels (P2022): 13\n",
      "Number of compounds with different sentiment labels (C2022): 56\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of compounds with different sentiment labels (P2022):\", len(pro_sentiment_diff))\n",
    "print(\"Number of compounds with different sentiment labels (C2022):\", len(con_sentiment_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2190,
   "id": "ecc1f047-9062-4196-b069-c596a1a88f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new information to csv files\n",
    "#con_sentiment_diff.to_csv(\"../evaluation/con_sentiment_diff_by_compound.csv\", index = False)\n",
    "#pro_sentiment_diff.to_csv(\"../evaluation/pro_sentiment_diff_by_compound.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885934d4-1c1d-4e75-ac5b-5836c2bcd872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.4 Derive Connotations\n",
    "To derive the connotation of each climate compound in the discourse of climate change we exploit the simple assumption that connotation of a compound may be directly derived by the sentiment that the second constituent can be associated with. Accordingly, in the following, we will obtain a polarity label for each of the second constituents by re-applying both models and look for differences again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5768af7-7904-47c4-aabe-ee3fc7be625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sentiment models to the second constituent of the compounds\n",
    "compounds[\"bert\"] = compounds.second_part.apply(get_bert)\n",
    "compounds[\"blob\"] = compounds.second_part.apply(get_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a9fa5-2f63-467f-88f6-bdf1c17d8d27",
   "metadata": {},
   "source": [
    "After the application of both sentiment models to the second constituent of the compounds, we figured that a lot of compounds did not receive the expected sentiment label/score.\n",
    "Accordingly, we repeat the procedure on the column containing the related words, since here we have more words that can contribute to find the prevailing sentiment of the second constituent. For some cases there are no related words available. Accordingly, for those cases we retrieve the sentiment label/score of the second constituent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2296,
   "id": "b5e6dce1-0bfe-4a30-bbc4-5668ce5f4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_bert(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the sentiment labels from the GermanBert model for the related words in the compounds data frame.\n",
    "    Arg: \n",
    "        row: a row with a list of related words.\n",
    "    Returns: \n",
    "        The most common polarity label of all words in the list (GermanBert), else None.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # initiate empty lists \n",
    "    bert = []\n",
    "    if row:\n",
    "        # for each string in the list of related words\n",
    "        for string in row:\n",
    "            bert.append(get_bert(string)) # retrieve sentiment label and append to list\n",
    "        \n",
    "        bert = Counter(bert).most_common(1)[0][0] # retrieve most common sentiment label for list of related words \n",
    "        \n",
    "        return bert\n",
    "    \n",
    "    # if there is no list of related words, return None       \n",
    "    else:        \n",
    "        return \n",
    "    \n",
    "    \n",
    "def get_common_blob(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the sentiment scores from the TextBlob model for the related words in the compounds data frame.\n",
    "    Arg: \n",
    "        row: a row with a list of related words.\n",
    "    Returns: \n",
    "        The average sentiment score of all words in the list (TextBlob), else None.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # initiate empty lists \n",
    "    blob = []\n",
    "    if row:\n",
    "        # for each string in the list of related words\n",
    "        for string in row:\n",
    "            blob.append(get_blob(string)) # retrieve sentiment score and append to list\n",
    "            \n",
    "        blob = sum(blob)/len(blob) # get average score for the list of words\n",
    "        \n",
    "        return blob \n",
    "    \n",
    "    # if there is no list of related words, return None\n",
    "    else:        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2285,
   "id": "841287cd-61ea-47e6-84c9-e24ae39fa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bert function to the column of related words \n",
    "compounds[\"bert_related\"] = compounds.related_words.apply(get_common_bert)\n",
    "compounds.bert_related.fillna(compounds.bert, inplace=True) # if no related words are available, use sentiment label of second constituent\n",
    "\n",
    "\n",
    "# apply blob function to the column of related words \n",
    "compounds[\"blob_related\"] = compounds.related_words.apply(get_common_blob)\n",
    "compounds.blob_related.fillna(compounds.blob, inplace=True) # if no related words are available, use sentiment scores of second constituent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242da4ab-8f0e-4fa8-a17f-48ac522b812d",
   "metadata": {},
   "source": [
    "Next, we manually evaluate the sentiment of the compounds. Since in most cases `TextBlob` and `GermanBert` gave different labels and due to the fact that none of the models correctly identifies all sentiments, we decide to manually assign a polarity label to the compounds. This is done in the file `../evaluation/compounds_sentiment.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2335,
   "id": "7f36967f-8770-48a8-8881-0dce37919f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_sentiment = pd.read_csv(\"../evaluation/compounds_sentiment.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead12410-f541-422f-9303-78df8210f2f0",
   "metadata": {},
   "source": [
    "Let's have a look at the sentiment scores and polarity labels that we obtained by the two models: The `GermanBert` model has an accuracy of 0.56 for both columns (`bert` and `bert_related`) and `TextBlob` has an accuracy of 0.46 for the `blob` column and 0.57 for `blob_related`. For `TextBlob` we use a threshold value of 0.001 here since the manual evaluation suggests that all values higher or lower than 0.0 should be *positive* or *negative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2347,
   "id": "246f87df-b6a7-493a-beac-f962ad72aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy bert:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5604838709677419"
      ]
     },
     "execution_count": 2347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy bert:\")\n",
    "len(compounds_sentiment[compounds_sentiment.bert == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2348,
   "id": "e4e3863d-e0f3-4925-9cf9-ae725bb753e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy bert_related:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5766129032258065"
      ]
     },
     "execution_count": 2348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy bert_related:\")\n",
    "len(compounds_sentiment[compounds_sentiment.bert_related == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2350,
   "id": "c214d7d5-c0eb-4095-81b7-a7558802e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy blob:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45564516129032256"
      ]
     },
     "execution_count": 2350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds_sentiment[\"blob_label\"] = compounds_sentiment.blob.apply(lambda x: convert_sentiment(x, 0.001))\n",
    "compounds_sentiment[\"blob_related_label\"] = compounds_sentiment.blob_related.apply(lambda x: convert_sentiment(x, 0.001))\n",
    "\n",
    "print(\"Accuracy blob:\")\n",
    "len(compounds_sentiment[compounds_sentiment.blob_label == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2349,
   "id": "eb916d47-28ba-4b0e-89c7-f0b0cdc0b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy blob_related:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5725806451612904"
      ]
     },
     "execution_count": 2349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy blob_related:\")\n",
    "len(compounds_sentiment[compounds_sentiment.blob_related_label == compounds_sentiment.manual_sentiment])/ len(compounds_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a379076-d753-482b-acb3-9403ce34bda3",
   "metadata": {},
   "source": [
    "Finally, we save the manual sentiment labels to the final `compounds` data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2683,
   "id": "54d460fa-f529-40d5-892a-218d13cbd5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/q0xtplm10vxb6sm32kbzrbh00000gn/T/ipykernel_4340/3252193463.py:2: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  compounds = pd.concat([compounds, compounds_sentiment.manual_sentiment], 1)\n"
     ]
    }
   ],
   "source": [
    "# append column with manual sentiment to compounds data frame\n",
    "compounds = pd.concat([compounds, compounds_sentiment.manual_sentiment], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2943,
   "id": "53a88a27-6efb-4552-b54e-7efb455c3e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>second_part</th>\n",
       "      <th>noun_forms</th>\n",
       "      <th>lemma</th>\n",
       "      <th>genus</th>\n",
       "      <th>compound_forms</th>\n",
       "      <th>related_words</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>roots</th>\n",
       "      <th>en_hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>similar_words</th>\n",
       "      <th>PERS_pro</th>\n",
       "      <th>ORG_pro</th>\n",
       "      <th>PERS_con</th>\n",
       "      <th>ORG_con</th>\n",
       "      <th>manual_sentiment</th>\n",
       "      <th>tf_pro</th>\n",
       "      <th>tf_con</th>\n",
       "      <th>tfidf_pro</th>\n",
       "      <th>tfidf_con</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>[abzockerei, abzockereien]</td>\n",
       "      <td>abzockerei</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaabzockerei, klimaabzockereien]</td>\n",
       "      <td>[Abzocke, Geldschneiderei, Profitmacherei, Beu...</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[Zinssatz, Zinsfuß]</td>\n",
       "      <td>[robbery, stealing, thieving, theft, larceny, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Werner Kirstein]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>[aktivismus]</td>\n",
       "      <td>aktivismus</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivismus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[aktivismus, aktivist]</td>\n",
       "      <td>[[Ken Jebsen, Wolf Dieter Storl]]</td>\n",
       "      <td>[[GLS Bank], [BUNDjugend]]</td>\n",
       "      <td>[[Greta Thunberg]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>280</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>[aktivisten, aktivist]</td>\n",
       "      <td>aktivist</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaaktivisten, klimaaktivist]</td>\n",
       "      <td>[aktiver Mitarbeiter, Aktivist, politisch akti...</td>\n",
       "      <td>[Volksvertreter, Politiker]</td>\n",
       "      <td>[jemand, irgendjemand, jeder beliebige]</td>\n",
       "      <td>[reformer, meliorist, crusader, social reforme...</td>\n",
       "      <td>...</td>\n",
       "      <td>[demagoge, macher, aktivist, aktivismus]</td>\n",
       "      <td>[[Greta Thunberg], [Greta Thunberg], [Gesa Kae...</td>\n",
       "      <td>[[FFF], [NABU, Industriegebiet AchimWest], [FF...</td>\n",
       "      <td>[[Horst-Joachim Lüdecke], [Karl Popper], [Geor...</td>\n",
       "      <td>[[EIKE, PIK], [FAZ], [FN, EIKE, Frankfurter Ru...</td>\n",
       "      <td>positive</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>[aktivistinnen, aktivistin]</td>\n",
       "      <td>aktivistin</td>\n",
       "      <td>f</td>\n",
       "      <td>[klimaaktivistinnen, klimaaktivistin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[aktivistin, aktivist]</td>\n",
       "      <td>[[Greta Thunberg], [Greta Thunberg], [Vanessa ...</td>\n",
       "      <td>[[FFF], [FFF, Siemens], [EU, PCS, Public Clima...</td>\n",
       "      <td>[[George Monbiot, Greta Thunberg], [Luisa Neub...</td>\n",
       "      <td>[[New York Post, EIKE], [Spiegel], [SPD, IPCC,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>0.512397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>[alarms, alarm, alarmen, alarme, alarmes]</td>\n",
       "      <td>alarm</td>\n",
       "      <td>m</td>\n",
       "      <td>[klimaalarms, klimaalarm, klimaalarmen, klimaa...</td>\n",
       "      <td>[Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...</td>\n",
       "      <td>[Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...</td>\n",
       "      <td>[Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...</td>\n",
       "      <td>[fear, fearfulness, fright, emotion, feeling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[alarm, alarmist, alarm, alarmist]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Sterling Burnett, Donald Trump], [Sterling B...</td>\n",
       "      <td>[[UN, Heartland Institute], [Heartland Institu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.066116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          original second_part                                 noun_forms  \\\n",
       "0  klimaabzockerei  abzockerei                 [abzockerei, abzockereien]   \n",
       "1  klimaaktivismus  aktivismus                               [aktivismus]   \n",
       "2    klimaaktivist    aktivist                     [aktivisten, aktivist]   \n",
       "3  klimaaktivistin  aktivistin                [aktivistinnen, aktivistin]   \n",
       "4       klimaalarm       alarm  [alarms, alarm, alarmen, alarme, alarmes]   \n",
       "\n",
       "        lemma genus                                     compound_forms  \\\n",
       "0  abzockerei     f               [klimaabzockerei, klimaabzockereien]   \n",
       "1  aktivismus     m                                  [klimaaktivismus]   \n",
       "2    aktivist     m                   [klimaaktivisten, klimaaktivist]   \n",
       "3  aktivistin     f              [klimaaktivistinnen, klimaaktivistin]   \n",
       "4       alarm     m  [klimaalarms, klimaalarm, klimaalarmen, klimaa...   \n",
       "\n",
       "                                       related_words  \\\n",
       "0  [Abzocke, Geldschneiderei, Profitmacherei, Beu...   \n",
       "1                                                 []   \n",
       "2  [aktiver Mitarbeiter, Aktivist, politisch akti...   \n",
       "3                                                 []   \n",
       "4  [Alarm, Notruf, Alarmruf, Warnton, Warnsignal,...   \n",
       "\n",
       "                                           hypernyms  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2                        [Volksvertreter, Politiker]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Geneigtheit, Wohlwollen, Gewogenheit, ...   \n",
       "\n",
       "                                               roots  \\\n",
       "0                                [Zinssatz, Zinsfuß]   \n",
       "1                                                 []   \n",
       "2            [jemand, irgendjemand, jeder beliebige]   \n",
       "3                                                 []   \n",
       "4  [Gunst, Wohlwollen, Geneigtheit, Zugewandtheit...   \n",
       "\n",
       "                                        en_hypernyms  ...  \\\n",
       "0  [robbery, stealing, thieving, theft, larceny, ...  ...   \n",
       "1                                                NaN  ...   \n",
       "2  [reformer, meliorist, crusader, social reforme...  ...   \n",
       "3                                                NaN  ...   \n",
       "4  [fear, fearfulness, fright, emotion, feeling, ...  ...   \n",
       "\n",
       "                              similar_words  \\\n",
       "0                                        []   \n",
       "1                    [aktivismus, aktivist]   \n",
       "2  [demagoge, macher, aktivist, aktivismus]   \n",
       "3                    [aktivistin, aktivist]   \n",
       "4        [alarm, alarmist, alarm, alarmist]   \n",
       "\n",
       "                                            PERS_pro  \\\n",
       "0                                                NaN   \n",
       "1                  [[Ken Jebsen, Wolf Dieter Storl]]   \n",
       "2  [[Greta Thunberg], [Greta Thunberg], [Gesa Kae...   \n",
       "3  [[Greta Thunberg], [Greta Thunberg], [Vanessa ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             ORG_pro  \\\n",
       "0                                                NaN   \n",
       "1                         [[GLS Bank], [BUNDjugend]]   \n",
       "2  [[FFF], [NABU, Industriegebiet AchimWest], [FF...   \n",
       "3  [[FFF], [FFF, Siemens], [EU, PCS, Public Clima...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            PERS_con  \\\n",
       "0                                [[Werner Kirstein]]   \n",
       "1                                 [[Greta Thunberg]]   \n",
       "2  [[Horst-Joachim Lüdecke], [Karl Popper], [Geor...   \n",
       "3  [[George Monbiot, Greta Thunberg], [Luisa Neub...   \n",
       "4  [[Sterling Burnett, Donald Trump], [Sterling B...   \n",
       "\n",
       "                                             ORG_con manual_sentiment tf_pro  \\\n",
       "0                                                NaN         negative     24   \n",
       "1                                                NaN         positive    280   \n",
       "2  [[EIKE, PIK], [FAZ], [FN, EIKE, Frankfurter Ru...         positive     61   \n",
       "3  [[New York Post, EIKE], [Spiegel], [SPD, IPCC,...         positive      9   \n",
       "4  [[UN, Heartland Institute], [Heartland Institu...         negative     13   \n",
       "\n",
       "  tf_con tfidf_pro tfidf_con  \n",
       "0      9       NaN       NaN  \n",
       "1     16       NaN  0.000000  \n",
       "2     66  0.007220  0.000000  \n",
       "3      0  0.093863  0.512397  \n",
       "4     48  0.083032  0.066116  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2943,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88b22e68-ee80-4127-a528-aec9366b2dd2",
   "metadata": {},
   "source": [
    "# 6. Term Frequencies\n",
    "In the following, we will add the term frequencies and the TF-IDF scores that we computed in R to our compounds data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2917,
   "id": "cc09c3a8-645a-4153-9c47-1738e890dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tf data frame \n",
    "tf = pd.read_csv(\"../../R/output/tf_complete.csv\", header=1, names=[\"original\", \"tf_pro\", \"tf_con\"])\n",
    "\n",
    "# load tfidf data frame\n",
    "tf_idf = pd.read_csv(\"../../R/output/tfidf_complete.csv\", header=0, index_col =0, names=[\"original\", \"tfidf_con\", \"tfidf_pro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2921,
   "id": "cb8e2824-ba37-4fec-8755-25665a5b2568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>tf_pro</th>\n",
       "      <th>tf_con</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimagerechtigkeit</td>\n",
       "      <td>280</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimapäckchen</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimarettung</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             original  tf_pro  tf_con\n",
       "0     klimaaktivistin      24       9\n",
       "1  klimagerechtigkeit     280      16\n",
       "2       klimaaktivist      61      66\n",
       "3       klimapäckchen       9       0\n",
       "4        klimarettung      13      48"
      ]
     },
     "execution_count": 2921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2922,
   "id": "3339975a-218e-4f88-b20c-247a17f38944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>tfidf_con</th>\n",
       "      <th>tfidf_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klimaabzockerei</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klimaaktivismus</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klimaaktivist</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.093863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klimaaktivistin</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.083032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>klimaalarm</td>\n",
       "      <td>0.305785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          original  tfidf_con  tfidf_pro\n",
       "1  klimaabzockerei   0.000000        NaN\n",
       "2  klimaaktivismus   0.000000   0.007220\n",
       "3    klimaaktivist   0.512397   0.093863\n",
       "4  klimaaktivistin   0.066116   0.083032\n",
       "5       klimaalarm   0.305785        NaN"
      ]
     },
     "execution_count": 2922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2968,
   "id": "671c5313-51b8-468b-839d-af9493d2dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index\n",
    "compounds = compounds.set_index('original')\n",
    "tf = tf.set_index('original')\n",
    "tf_idf = tf_idf.set_index('original')\n",
    "\n",
    "# append new columns\n",
    "compounds = pd.concat([compounds, tf.tf_pro, tf.tf_con, tf_idf.tfidf_pro, tf_idf.tfidf_con], 1)\n",
    "compounds.tf_pro = compounds.tf_pro.astype('Int64') # convert into integers\n",
    "compounds.tf_con = compounds.tf_con.astype('Int64') # convert into integers\n",
    "\n",
    "# reset index\n",
    "compounds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f75f32-110b-40ee-9351-8b52ec106ce2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Pattern Extraction\n",
    "- spacy rule-based matching\n",
    "\n",
    "https://predictivehacks.com/rule-based-matching-for-nlp-using-spacy/\n",
    "\n",
    "https://spacy.io/api/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "82c8ddae-dd1d-45db-b48b-884429a3df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "d345a054-2c40-4971-a0c6-46afda654584",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"Sie liegt mit etwa 0,6 bar aber gar nicht soweit weg von unserem Planeten mit 1 bar aber nur 0,04 % CO2. Schon daraus erkennt man Ungereimtheiten. \n",
    "          Wenn man den Mond mit einbezieht, der praktisch keine Atmosphäre hat, ergibt sich: CO2 Druck MAX MIN Differenz Venus >90% >90 bar 770 710 60 K Erde <1% 1 \n",
    "          bar 331 184 147 K Mars >90% 0,6 bar 300 140 160 K Mond 0 0 403 113 290 K also eine Bestätigung der Gasgesetze. In der Atmosphäre gelten auch die Höhenformeln\n",
    "          für die Abnahme von Druck und Temperatur mit der Höhe. Gleichfalls die Hauptsätze der Thermodynamik.Die Klimaaktivisten behaupten, das CO2 der Atmosphäre \n",
    "          würde Wärme zurückstrahlen.Jeglicher Beweis dafür fehlt. Der Versuch, mittels der Strahlungsgesetze dies zu erklären, scheitert am 2. Hauptsatz. \n",
    "          Es kann kälteres CO2 der Atmosphäre nicht auf die wärmere Erdoberfläche zurückstrahlen. Wenn CO2 so wirksam wäre, wie behauptet, müsste auch der Luftdruck gestiegen sein. \n",
    "          Aber Klimaaktivisten sind nette Menschen\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "7e51235e-622d-4c55-8d70-f9eafbd9cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"Klimaaktivisten sind nette Menschen und lachen viel. \n",
    "          Ich glaube aber auch, Klimaaktivisten wissen, dass Co2 ein Faktor ist.\n",
    "          Der bekennende Klimaaktivisten Gisbert Fanselow weiß besonders viel darüber.\n",
    "          U. a. waren gestern Abend die „Fridays for Future\" Klimaaktivisten Franziska Wessel und Jakob Blasel in der Sendung zu Gast.\n",
    "          Radikale Klimaaktivisten wollen nun einen sofortigen Ausstieg durchsetzen. von Heinz Horeis Peking und Smog\n",
    "          -  das gehörte in den vergangenen Jahren untrennbar zusammen.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ebcf5-e3d5-4f79-8489-c591b5596479",
   "metadata": {},
   "source": [
    "## 6.1 Initiate Patterns\n",
    "- [ ] Keyword + sein + Substantiv\n",
    "- [ ] Keyword + sein + Adjektiv + Substantiv\n",
    "- [ ] Substantiv + ist + Keyword\n",
    "- [ ] Keyword + Verb + ,dass (Version mit und ohne Komma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "6bfaadf7-b75e-449f-a064-3e41489ee311",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_verbs = [\"behaupten\", \"sagen\", \"meinen\", \"glauben\", \"wollen\", \"denken\", \"befürchten\", \"wissen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "351043d1-ddc5-433a-b243-555cabf9a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(294216856376502964, 0, 4),\n",
       " (14694915939493522981, 14, 16),\n",
       " (12927341502877082044, 14, 17),\n",
       " (2932447317883468553, 26, 28),\n",
       " (9538583902685184471, 26, 29),\n",
       " (2932447317883468553, 46, 48),\n",
       " (9538583902685184471, 46, 49),\n",
       " (14694915939493522981, 60, 62)]"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Keyword + sein + ADJ + NOUN\n",
    "pattern1 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}}, {\"LEMMA\": \"sein\"}, {\"POS\": \"ADJ\"},\n",
    "          {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"p1\", [pattern1])\n",
    "\n",
    "# Keyword + sein + NOUN\n",
    "pattern2 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}}, {\"LEMMA\": \"sein\"},{\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"p2\", [pattern2])\n",
    "\n",
    "# NOUN + sein + Keyword \n",
    "pattern3 = [{\"POS\": \"NOUN\"},{\"LEMMA\": \"sein\"},{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}}]\n",
    "matcher.add(\"p3\", [pattern3])\n",
    "\n",
    "# Keyword + verb + , + dass ? oder nur Komma????\n",
    "pattern4 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}},{\"LEMMA\": {\"IN\": opinion_verbs}} ,{\"IS_PUNCT\": True}]\n",
    "matcher.add(\"p4\", [pattern4])\n",
    "\n",
    "# Keyword + Eigenname \n",
    "pattern5 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}},{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"p5\", [pattern5])\n",
    "\n",
    "# Keyword + Eigenname mit Nachnamen\n",
    "pattern6 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}},{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"p6\", [pattern6])\n",
    "\n",
    "# Keyword + opinion verb \n",
    "pattern7 = [{\"LOWER\": {\"REGEX\": \"klimaaktivist*\"}},{\"LEMMA\": {\"IN\": opinion_verbs}}]\n",
    "matcher.add(\"p7\", [pattern7])\n",
    "\n",
    "\n",
    "\n",
    "#doc = nlp(\"hello world!\")\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "bd7e6576-886d-454b-92ba-6b0b63a62d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klimaaktivisten sind nette Menschen und lachen viel.\n",
      "Ich glaube aber auch, Klimaaktivisten wissen, dass Co2 ein Faktor ist.\n",
      "Ich glaube aber auch, Klimaaktivisten wissen, dass Co2 ein Faktor ist.\n",
      "Der bekennende Klimaaktivisten Gisbert Fanselow weiß besonders viel darüber.\n",
      "Der bekennende Klimaaktivisten Gisbert Fanselow weiß besonders viel darüber.\n",
      "Klimaaktivisten Franziska Wessel und Jakob Blasel in der Sendung zu Gast.\n",
      "Klimaaktivisten Franziska Wessel und Jakob Blasel in der Sendung zu Gast.\n",
      "\n",
      "          Radikale Klimaaktivisten wollen nun einen sofortigen Ausstieg durchsetzen.\n"
     ]
    }
   ],
   "source": [
    "# TO PRINT FULL SENTENCE AFTER PATTERN MATCH!\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = doc[start:end]  \n",
    "    sents = span.sent  #ADDED THIS LINE\n",
    "    print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "de49958a-1971-4ff4-adcf-da1f58a10c3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294216856376502964 p1 0 4 Klimaaktivisten sind nette Menschen\n",
      "14694915939493522981 p7 14 16 Klimaaktivisten wissen\n",
      "12927341502877082044 p4 14 17 Klimaaktivisten wissen,\n",
      "2932447317883468553 p5 26 28 Klimaaktivisten Gisbert\n",
      "9538583902685184471 p6 26 29 Klimaaktivisten Gisbert Fanselow\n",
      "2932447317883468553 p5 46 48 Klimaaktivisten Franziska\n",
      "9538583902685184471 p6 46 49 Klimaaktivisten Franziska Wessel\n",
      "14694915939493522981 p7 60 62 Klimaaktivisten wollen\n"
     ]
    }
   ],
   "source": [
    "# TO PRINT EXACT MATCH!\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get the string representation \n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "584ff075-98d8-4978-80cb-00e6a2b3e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: {'Klimaaktivisten Franziska', 'Klimaaktivisten wissen', 'Klimaaktivisten wissen,', 'Klimaaktivisten wollen', 'Klimaaktivisten Gisbert', 'Klimaaktivisten sind nette Menschen', 'Klimaaktivisten Gisbert Fanselow', 'Klimaaktivisten Franziska Wessel'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches][:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94049b6-470a-46cd-8704-00651cd71047",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FURTHER READING\n",
    "\n",
    "\n",
    "http://markneumann.xyz/blog/dependency_matcher/\n",
    "\n",
    "https://www.kaggle.com/code/randikaweerasinghe/harrypotter-data-extract-with-spacy-pattern-match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4088183-932b-4da6-b551-1d6b447ae269",
   "metadata": {},
   "source": [
    "## 7. Update Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdaa333-0cce-4175-9b34-66bbe3286e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pro_context.to_csv(\"../output/pro_ner.csv\", index = False)\n",
    "#con_context.to_csv(\"../output/con_ner.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2976,
   "id": "6f1e579d-87b7-4fcf-96af-a5e720ce543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compounds.to_csv(\"../output/compounds_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad956990-58f6-4efb-ac1c-b1580d2655eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9d0c80-e709-43a4-bb78-d73b173ecbf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition Phrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da232ce1-0451-4e13-95df-9923d651c138",
   "metadata": {},
   "source": [
    "## 0. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88444c5c-8e3a-438d-a373-d62d396326ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import definition strings script\n",
    "import definition_strings as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44903fa-bcd5-473d-8fab-a6057f1fafca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load Knowledge Base\n",
    "We load the knowledge base that was created in the `textmining.ipynb` notebook and drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c61979c8-3cdb-4613-bf8c-ded1d9d60695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load knowledge base\n",
    "knowledge_base = pd.read_csv(\"../output/knowledge_base.csv\", sep=\",\")\n",
    "\n",
    "# drop unnecessary columns\n",
    "to_drop = [\"noun_forms\", \"related_words\", \"hypernyms\", \"roots\", \"en_hypernyms\", \"path\", \"wup\", \"stem_cistem\", \"stem_porter\",\n",
    "       \"stem_lancaster\", \"stem_snowball\", \"share_cistem\", \"share_porter\", \"share_lancaster\", \"share_snowball\", \"dist_stemmer\"]\n",
    "knowledge_base = knowledge_base.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07c406-e8bb-4f78-aeaf-3edd282b8431",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Preprocessing of the information\n",
    "Loading the csv file via `pandas` oftentimes requires the re-evaluation of the literals contained in the data frame. Accordingly, we run the following code to make sure all cell types are evaluated correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab185e7-2976-4c9b-9ac9-0b3ca007bdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate literals\n",
    "knowledge_base[\"compound_forms\"] = knowledge_base.compound_forms.apply(lambda x: literal_eval(str(x)))\n",
    "knowledge_base[\"definition\"] = knowledge_base.definition.apply(lambda x: literal_eval(str(x)))\n",
    "knowledge_base[\"PERS_pro\"] = knowledge_base.PERS_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"PERS_con\"] = knowledge_base.PERS_con.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"ORG_pro\"] = knowledge_base.ORG_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"ORG_con\"] = knowledge_base.ORG_con.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"similar_words\"] = knowledge_base.similar_words.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"pro_mods\"] = knowledge_base.pro_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"con_mods\"] = knowledge_base.con_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"pro_attr\"] = knowledge_base.pro_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"con_attr\"] = knowledge_base.con_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"pro_colls\"] = knowledge_base.pro_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else list())\n",
    "knowledge_base[\"con_colls\"] = knowledge_base.con_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != \"nan\") else list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc686920-701a-4775-ae6f-ffad14423cf9",
   "metadata": {},
   "source": [
    "Next we apply some preprocessing steps to translate and reduce some of the pieces of information that are contained in the knowledge base:\n",
    "- replace empty entity values with nan\n",
    "- get counts of entities\n",
    "- translate polarity labels into German counterparts\n",
    "- translate attribution tags into German counterparts\n",
    "- replace unavailable sarcasm values with 0\n",
    "- count modifiers\n",
    "- remove potential duplicates from collocation lists\n",
    "- replace unavailable TF-IDF scores with 0\n",
    "- retrieve definite articles from genus column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "629865cb-db2d-4f38-9e40-0d83707e8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty values with nan\n",
    "knowledge_base[\"PERS_pro\"] = knowledge_base.PERS_pro.apply(lambda x: x if x else np.nan)\n",
    "knowledge_base[\"PERS_con\"] = knowledge_base.PERS_con.apply(lambda x: x if x else np.nan)\n",
    "knowledge_base[\"ORG_pro\"] = knowledge_base.ORG_pro.apply(lambda x: x if x else np.nan)\n",
    "knowledge_base[\"ORG_con\"] = knowledge_base.ORG_con.apply(lambda x: x if x else np.nan)\n",
    "\n",
    "# apply counter to count occurrences of entities\n",
    "knowledge_base[\"PERS_pro\"] = knowledge_base.PERS_pro.apply(lambda x: Counter(x) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"PERS_con\"] = knowledge_base.PERS_con.apply(lambda x: Counter(x) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"ORG_pro\"] = knowledge_base.ORG_pro.apply(lambda x: Counter(x) if(str(x) != \"nan\") else x)\n",
    "knowledge_base[\"ORG_con\"] = knowledge_base.ORG_con.apply(lambda x: Counter(x) if(str(x) != \"nan\") else x)\n",
    "\n",
    "# replace sentiment with the German counterparts\n",
    "knowledge_base[\"manual_sentiment\"] = knowledge_base.manual_sentiment.replace(\"negative\", \"negativ\")\n",
    "knowledge_base[\"manual_sentiment\"] = knowledge_base.manual_sentiment.replace(\"positive\", \"positiv\")\n",
    "\n",
    "# strip white spaces at right end of string (attribution columns)\n",
    "knowledge_base[\"pro_attr\"] = knowledge_base[\"pro_attr\"].str.rstrip()\n",
    "knowledge_base[\"con_attr\"] = knowledge_base[\"con_attr\"].str.rstrip()\n",
    "\n",
    "# replace attribution tag with German counterpart / nan label\n",
    "knowledge_base[\"pro_attr\"] = knowledge_base.pro_attr.replace(\"Self\", \"Selbstzuschreibung\")\n",
    "knowledge_base[\"pro_attr\"] = knowledge_base.pro_attr.replace(\"External\", \"Fremdzuschreibung\")\n",
    "knowledge_base[\"pro_attr\"] = knowledge_base.pro_attr.replace(\"None\", \"nan\")\n",
    "knowledge_base[\"con_attr\"] = knowledge_base.con_attr.replace(\"Self\", \"Selbstzuschreibung\")\n",
    "knowledge_base[\"con_attr\"] = knowledge_base.con_attr.replace(\"External\", \"Fremdzuschreibung\")\n",
    "knowledge_base[\"con_attr\"] = knowledge_base.con_attr.replace(\"None\", \"nan\")\n",
    "\n",
    "# replace unavailable sarcasm values with 0\n",
    "knowledge_base[\"pro_sarcasm\"] = knowledge_base.pro_sarcasm.apply(lambda x: 0.0 if(str(x) == 'nan') else x)\n",
    "knowledge_base[\"con_sarcasm\"] = knowledge_base.con_sarcasm.apply(lambda x: 0.0 if(str(x) == 'nan') else x)\n",
    "\n",
    "# count modifiers\n",
    "knowledge_base[\"pro_mods\"] = knowledge_base.pro_mods.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "knowledge_base[\"con_mods\"] = knowledge_base.con_mods.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "\n",
    "# remove duplicates from list of collocations\n",
    "knowledge_base[\"pro_colls\"] = knowledge_base.pro_colls.apply(set)\n",
    "knowledge_base[\"pro_colls\"] = knowledge_base.pro_colls.apply(list)\n",
    "knowledge_base[\"con_colls\"] = knowledge_base.con_colls.apply(set)\n",
    "knowledge_base[\"con_colls\"] = knowledge_base.con_colls.apply(list)\n",
    "\n",
    "# replace nan values in TF columnd with 0\n",
    "knowledge_base[\"pro_updated\"] = knowledge_base.pro_updated.replace(np.nan, 0)\n",
    "knowledge_base[\"con_updated\"] = knowledge_base.con_updated.replace(np.nan, 0)\n",
    "knowledge_base.pro_updated = knowledge_base.pro_updated.astype('Int64') # convert into integers\n",
    "knowledge_base.con_updated = knowledge_base.con_updated.astype('Int64')\n",
    "\n",
    "# replace nan values in TF-IDF columnd with 0\n",
    "knowledge_base[\"tfidf_pro\"] = knowledge_base.tfidf_pro.replace(np.nan, 0)\n",
    "knowledge_base[\"tfidf_con\"] = knowledge_base.tfidf_con.replace(np.nan, 0)\n",
    "\n",
    "# change genus to article information\n",
    "# create a list of our conditions\n",
    "conditions = [(knowledge_base[\"genus\"] == \"f\"),(knowledge_base[\"genus\"] == \"m\"), (knowledge_base[\"genus\"] == \"n\")]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [\"die\", \"der\", \"das\"]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "knowledge_base[\"article\"] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289cb1-aacf-45ea-be44-d80d8d7118c5",
   "metadata": {},
   "source": [
    "## 2. Generate Definitions\n",
    "Next, for each compound word, we will generate a final combination of strings (according to the unique information pieces that we have for this compound) and fill the place holders (denoted in swift brackets) with these information pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af3e13e3-f481-4901-b285-65328a3b8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new text file containing the definition texts to be able to manually check the output \n",
    "f = open(\"../evaluation/definition_texts.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edb48b66-900c-4023-8333-2d3be1d6e346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each compound word \n",
    "for word in knowledge_base.original:\n",
    "    \n",
    "    # set index to this compound word \n",
    "    idx = knowledge_base.index[knowledge_base[\"original\"] == word][0]\n",
    "        \n",
    "    ### BASE INFORMATION ####\n",
    "    \n",
    "    # initiate base info string (i.e. compound + genus)\n",
    "    text = ds.str_base_info\n",
    "        \n",
    "    # retrieve base information from knowledge base \n",
    "    compound = word.capitalize() # capitalized version of compound\n",
    "    article = knowledge_base[\"article\"].iloc[idx] # definite article\n",
    "    con_freq = knowledge_base[\"con_updated\"].iloc[idx] # term frequency C2022\n",
    "    pro_freq = knowledge_base[\"pro_updated\"].iloc[idx] # term frequency P2022\n",
    "    con_tfidf = round(knowledge_base[\"tfidf_con\"].iloc[idx],3) # TF-IDF C2022\n",
    "    pro_tfidf = round(knowledge_base[\"tfidf_pro\"].iloc[idx],3) # TF-IDF P2022\n",
    "    pro_sarc = knowledge_base[\"pro_sarcasm\"].iloc[idx] # sarcasm P2022\n",
    "    con_sarc = knowledge_base[\"con_sarcasm\"].iloc[idx] # sarcasm C2022\n",
    "    pro_attr = knowledge_base[\"pro_attr\"].iloc[idx] # attribution P2022\n",
    "    con_attr = knowledge_base[\"con_attr\"].iloc[idx] # attribution C2022\n",
    "    \n",
    "    # if category of compound is \"person\"\n",
    "    if knowledge_base[\"concept\"].iloc[idx] == \"person\":\n",
    "        text += ds.str_base_pers # add \"person\" string to base string\n",
    "            \n",
    "    # if category of compound is \"location\"\n",
    "    elif knowledge_base[\"concept\"].iloc[idx] == \"location\":\n",
    "        text += ds.str_base_loc # add \"location\" string to base string \n",
    "    \n",
    "    # if category of compound is \"group\"\n",
    "    elif knowledge_base[\"concept\"].iloc[idx] == \"group\":\n",
    "        text += ds.str_base_group # add \"group\" string to base string \n",
    "        \n",
    "    # if category of compound is \"abstraction\"        \n",
    "    elif knowledge_base[\"concept\"].iloc[idx] == \"abstraction\":\n",
    "        text += ds.str_base_abstract # add \"abstraction\" string to base string \n",
    "    \n",
    "    # if category of compound is \"action\"\n",
    "    elif knowledge_base[\"concept\"].iloc[idx] == \"action\":\n",
    "        text += ds.str_base_action # add \"action\" string to base string \n",
    "    \n",
    "    ### ATTRIBUTION ###\n",
    "    \n",
    "    # if we have attribution info for both corpora\n",
    "    if str(knowledge_base[\"con_attr\"].iloc[idx]) != \"nan\" and str(knowledge_base[\"pro_attr\"].iloc[idx]) != \"nan\":\n",
    "\n",
    "        text += ds.str_attr # add attribution string\n",
    "        \n",
    "        # compose attribution filler\n",
    "        attr = con_attr + \" von Seiten der Skeptiker und als \" + pro_attr + \" im Vertreter Korpus.\"\n",
    "    \n",
    "    # if we only have attribution info for C2022\n",
    "    elif str(knowledge_base[\"con_attr\"].iloc[idx]) != \"nan\" and str(knowledge_base[\"pro_attr\"].iloc[idx]) == \"nan\":\n",
    "\n",
    "        text += ds.str_attr # add attribution string\n",
    "        \n",
    "        # compose attribution filler\n",
    "        attr = con_attr + \" von Seiten der Skeptiker.\" \n",
    "\n",
    "    # if we only have attribution info for P2022\n",
    "    elif str(knowledge_base[\"con_attr\"].iloc[idx]) == \"nan\" and str(knowledge_base[\"pro_attr\"].iloc[idx]) != \"nan\":\n",
    "        \n",
    "        text += ds.str_attr # add attribution string    \n",
    "            \n",
    "        # compose attribution filler\n",
    "        attr = pro_attr + \" von Seiten der Vertreter.\" \n",
    "            \n",
    "    else:        \n",
    "        attr = \"\" # attribution not available \n",
    "        \n",
    "        \n",
    "    ### SARCASM ###\n",
    "    \n",
    "    # if we have sarcasm values for P2022 and C2022\n",
    "    if pro_sarc > 0 and con_sarc> 0:\n",
    "                \n",
    "        # add sarcasm string\n",
    "        text += ds.str_sarcasm\n",
    "        \n",
    "        # compose text for sarcasm string\n",
    "        sarcasm = \"in \" + str(int(pro_sarc*100)) + \"% der Fälle für den Vertreter Diskurs und in \" + str(int(con_sarc*100)) + \"% der Fälle für den Skeptiker Diskurs\"\n",
    "\n",
    "    # or if we only have sarcasm info for P2022\n",
    "    elif pro_sarc > 0: \n",
    "        \n",
    "        # add sarcasm string\n",
    "        text += ds.str_sarcasm\n",
    "\n",
    "        # compose sarcasm filler\n",
    "        sarcasm = \"in \" + str(int(pro_sarc*100)) + \"% der Fälle im Vertreter Diskurs\"\n",
    "        \n",
    "    # or if we only have sarcasm info for C2022\n",
    "    elif con_sarc > 0:\n",
    "        \n",
    "        # add sarcasm string\n",
    "        text += ds.str_sarcasm\n",
    "\n",
    "        # compose sarcasm filler\n",
    "        sarcasm = \"in \" +str(int(con_sarc*100)) + \"% der Fälle im Skeptiker Diskurs\"\n",
    "        \n",
    "    else:\n",
    "        # add sarcasm string\n",
    "        text += ds.str_sarcasm\n",
    "        sarcasm = \"nicht\" # sarcasm not available\n",
    "            \n",
    "    \n",
    "    ### CONNOTATION ### \n",
    "        \n",
    "    # add sentiment string\n",
    "    text += ds.str_sent \n",
    "    \n",
    "    sentiment = knowledge_base[\"manual_sentiment\"].iloc[idx] # retrieve connotation label \n",
    "        \n",
    "    ### MODIFIERS ###    \n",
    "    \n",
    "    # if we have at least one P2022 modifier for compound\n",
    "    if knowledge_base[\"pro_mods\"].isna().iloc[idx] == False:\n",
    "        \n",
    "        try:\n",
    "            # try to retrieve two most common modifiers and connect with conjunction\n",
    "            pro_mods =  \" und \".join([\"'\"+el[0]+\"'\" for el in knowledge_base[\"pro_mods\"].iloc[idx].most_common(2) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty \n",
    "            if pro_mods != \"\":\n",
    "                text += ds.str_mods_pro # add \"pro modifier\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one modifier available retrieve this one\n",
    "            pro_mods = \"\".join([\"'\"+el[0]+\"'\" for el in knowledge_base[\"pro_mods\"].iloc[idx].most_common(1) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty\n",
    "            if pro_mods != \"\":\n",
    "                text += ds.str_mods_pro # add \"pro modifier\" string to base string \n",
    "    else:\n",
    "        pro_mods = \"\" # else, no P2022 modifier available \n",
    "            \n",
    "    # if we have at least one C2022 modifier for compound\n",
    "    if knowledge_base[\"con_mods\"].isna().iloc[idx] == False:\n",
    "        \n",
    "        try:\n",
    "            # try to retrieve two most common modifiers and connect with conjunction\n",
    "            con_mods =  \" und \".join([\"'\"+el[0]+\"'\" for el in knowledge_base[\"con_mods\"].iloc[idx].most_common(2) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty\n",
    "            if con_mods != \"\":\n",
    "                text += ds.str_mods_con # add \"con modifier\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one modifier available retrieve this one\n",
    "            con_mods = \"\".join([\"'\"+el[0]+\"'\" for el in knowledge_base[\"con_mods\"].iloc[idx].most_common(1) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not emtpy \n",
    "            if con_mods != \"\": \n",
    "                text += ds.str_mods_con # add \"con modifier\" string to base string \n",
    "    else:\n",
    "        con_mods = \"\" # else, no C2022 modifier available \n",
    "            \n",
    "    \n",
    "    ### PERSON ENTITIES ###\n",
    "    \n",
    "    # if we have at least one person in C2022 AND P2022\n",
    "    if knowledge_base[\"PERS_con\"].isna().iloc[idx] == False and knowledge_base[\"PERS_pro\"].isna().iloc[idx] == False:\n",
    "        pers_both = True # set value to TRUE\n",
    "    \n",
    "    else:\n",
    "        pers_both = False # set value to FALSE \n",
    "            \n",
    "    # if we have at least one person in C2022\n",
    "    if knowledge_base[\"PERS_con\"].isna().iloc[idx] == False:\n",
    "        \n",
    "        try:\n",
    "            # try to retrieve two most common person and connect with \"und\"\n",
    "            con_pers = \" und \".join([el[0] for el in knowledge_base[\"PERS_con\"].iloc[idx].most_common(2)])\n",
    "            text += ds.str_pers # add \"person\" string to base string \n",
    "            text += ds.str_pers_con # add \"con person\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one person available retrieve this one\n",
    "            con_pers = knowledge_base[\"PERS_con\"].iloc[idx].most_common(1)[0]\n",
    "            text += ds.str_pers # add \"person\" string to base string \n",
    "            text += ds.str_pers_con # add \"con person\" string to base string  \n",
    "           \n",
    "        # if we only have C2022 \"person\"\n",
    "        if pers_both == False:\n",
    "            text += \".\" # add full stop to string\n",
    "    else:\n",
    "        con_pers = \"\" # no C2022 person available \n",
    "            \n",
    "    # if we have at least one person in P2022\n",
    "    if knowledge_base[\"PERS_pro\"].isna().iloc[idx] == False:\n",
    "        \n",
    "        # if we have \"person\" entities for C2022 AND P2022\n",
    "        if pers_both == True:\n",
    "            \n",
    "            try:\n",
    "                # try to retrieve two most common person and connect with \"und\"\n",
    "                pro_pers =  \" und \".join([el[0] for el in knowledge_base[\"PERS_pro\"].iloc[idx].most_common(2)])\n",
    "                text += \" und\" # add \"und\" to string \n",
    "                text += ds.str_pers_pro + \".\" # add \"pro person\" string and full stop to base string \n",
    "\n",
    "            except:\n",
    "                # if only one person available retrieve this one\n",
    "                pro_pers = knowledge_base[\"PERS_pro\"].iloc[idx].most_common(1)[0]\n",
    "                text += \" und\" # add \"und\" to string \n",
    "                text += ds.str_pers_pro + \".\"# add \"pro person\" string and full stop to base string\n",
    "                \n",
    "        else:\n",
    "            try:\n",
    "                # try to retrieve two most common person and connect with comma\n",
    "                pro_pers =  \", \".join([el[0] for el in knowledge_base[\"PERS_pro\"].iloc[idx].most_common(2)])\n",
    "                text += ds.str_pers # add \"person\" string to base string \n",
    "                text += ds.str_pers_pro + \".\" # add \"pro person\" string and full stop to base string \n",
    "\n",
    "            except:\n",
    "                # if only one person available retrieve this one\n",
    "                pro_pers = knowledge_base[\"PERS_pro\"].iloc[idx].most_common(1)[0]\n",
    "                text += ds.str_pers # add \"person\" string to base string \n",
    "                text += ds.str_pers_pro + \".\" # add \"pro person\" string and full stop to base string \n",
    "\n",
    "    else:\n",
    "        pro_pers = \"\" # no P2022 person available \n",
    "\n",
    "    ### ORGANISATION ENTITIES ### \n",
    "    \n",
    "    # if we have at least one organisation in C2022 AND P2022\n",
    "    if knowledge_base[\"ORG_con\"].isna().iloc[idx] == False and knowledge_base[\"ORG_pro\"].isna().iloc[idx] == False:\n",
    "        org_both = True # set value to TRUE\n",
    "    \n",
    "    else:\n",
    "        org_both = False # set value to FALSE \n",
    "            \n",
    "    # if we have at least one organisation in C2022\n",
    "    if knowledge_base[\"ORG_con\"].isna().iloc[idx] == False:\n",
    "            \n",
    "        try:\n",
    "            # try to retrieve two most common organisations and connect with comma\n",
    "            con_org =  \", \".join([el[0] for el in knowledge_base[\"ORG_con\"].iloc[idx].most_common(2)])\n",
    "            text += ds.str_org # add \"organisation\" string to base string \n",
    "            text += ds.str_org_con # add \"con organisation\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one organisation available retrieve this one\n",
    "            con_org = knowledge_base[\"ORG_con\"].iloc[idx].most_common(1)[0]\n",
    "            text += ds.str_org # add \"organisation\" string to base string \n",
    "            text += ds.str_org_con # add \"con organisation\" string to base string \n",
    "        \n",
    "        # if we only have C2022 \"organisation\"\n",
    "        if org_both == False:\n",
    "            text += \".\" # add full stop to string \n",
    "                \n",
    "    else:\n",
    "        con_org = \"\" # no C2022 \"organisation\" available \n",
    "            \n",
    "    # if we have at least one organisation in P2022\n",
    "    if knowledge_base[\"ORG_pro\"].isna().iloc[idx] == False:\n",
    "        \n",
    "        # if we have organisations for C2022 AND P2022\n",
    "        if org_both == True:\n",
    "            try:\n",
    "                # try to retrieve two most common organisations and connect with comma\n",
    "                pro_org =  \", \".join([el[0] for el in knowledge_base[\"ORG_pro\"].iloc[idx].most_common(2)])\n",
    "                text += \" und\" # add \"und\" string to base string \n",
    "                text += ds.str_org_pro + \".\" # add \"pro organisation\" string to base string \n",
    "\n",
    "            except:\n",
    "                # if only one organisation available retrieve this one\n",
    "                pro_org = knowledge_base[\"ORG_pro\"].iloc[idx].most_common(1)[0]\n",
    "                text += \" und\" # add \"und\" string to base string \n",
    "                text += ds.str_org_pro + \".\" # add \"pro organisation\" string to base string\n",
    "                \n",
    "        else:\n",
    "            try:\n",
    "                # try to retrieve two most common organisations and connect with comma\n",
    "                pro_org =  \", \".join([el[0] for el in knowledge_base[\"ORG_pro\"].iloc[idx].most_common(2)])\n",
    "                text += ds.str_org # add \"organisation\" string to base string \n",
    "                text += ds.str_org_pro + \".\"# add \"pro organisation\" string to base string \n",
    "\n",
    "            except:\n",
    "                # if only one organisation available retrieve this one\n",
    "                pro_org = knowledge_base[\"ORG_pro\"].iloc[idx].most_common(1)[0]\n",
    "                text += ds.str_org # add \"organisation\" string to base string \n",
    "                text += ds.str_org_pro + \".\" # add \"pro organisation\" string to base string\n",
    "            \n",
    "    else:\n",
    "        pro_org = \"\" # no P2022 \"organisation\" available \n",
    "        \n",
    "    ### COLLOCATIONS ###\n",
    "        \n",
    "    # if we have collocations for C2022 and P2022\n",
    "    if len(knowledge_base[\"con_colls\"].iloc[idx]) != 0 and len(knowledge_base[\"pro_colls\"].iloc[idx]) != 0:\n",
    "        try:\n",
    "            # try to retrieve two random collocations from C2022 and connect with comma and compose string\n",
    "            con_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"con_colls\"].iloc[idx], 2)]) + \" (Skeptiker Korpus)\"\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            # if only one collocation from C2022 available retrieve this one and compose string\n",
    "            con_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"con_colls\"].iloc[idx], 1)]) + \" (Skeptiker Korpus)\"\n",
    "\n",
    "        try:\n",
    "            # try to retrieve two random collocations from P2022 and connect with comma and compose string \n",
    "            pro_colls = \" und \" + \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"pro_colls\"].iloc[idx], 2)]) + \" (Vertreter Korpus)\"\n",
    "\n",
    "\n",
    "        except:\n",
    "            # if only one collocation from P2022 available retrieve this one\n",
    "            pro_colls = \" und \" + \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"pro_colls\"].iloc[idx], 1)]) + \" (Vertreter Korpus)\"\n",
    "\n",
    "\n",
    "\n",
    "        text += ds.str_colls # add \"collocations\" string to base string \n",
    "        \n",
    "    # if we only have collocations for C2022\n",
    "    elif len(knowledge_base[\"con_colls\"].iloc[idx]) != 0:\n",
    "        try:\n",
    "            # try to retrieve two random collocations from C2022 and connect with comma and compose string\n",
    "            con_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"con_colls\"].iloc[idx], 2)]) + \" (Skeptiker Korpus)\"\n",
    "\n",
    "        except:\n",
    "            # if only one collocation from C2022 available retrieve this one and compose string\n",
    "            con_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"con_colls\"].iloc[idx], 1)]) + \" (Skeptiker Korpus)\"\n",
    "\n",
    "        pro_colls = \"\" # no collocations for P2022 available \n",
    "        text += ds.str_colls # add \"collocations\" string to base string\n",
    "        \n",
    "    # if we only have collocations for P2022\n",
    "    elif len(knowledge_base[\"pro_colls\"].iloc[idx]) != 0:\n",
    "        try:\n",
    "            # try to retrieve two random collocations from P2022 and connect with comma and compose string \n",
    "            pro_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"pro_colls\"].iloc[idx], 2)]) + \" (Vertreter Korpus)\"\n",
    "\n",
    "\n",
    "        except:\n",
    "            # if only one collocation from P2022 available retrieve this one\n",
    "            pro_colls = \", \".join([\"'\"+x+\"'\" for x in random.sample(knowledge_base[\"pro_colls\"].iloc[idx], 1)]) + \" (Vertreter Korpus)\"\n",
    "\n",
    "        con_colls = \"\" # no collocations for C2022 available\n",
    "        text += ds.str_colls # add \"collocations\" string to base string \n",
    "        \n",
    "    else:\n",
    "        pro_colls = \"\" # no P2022 collocation available\n",
    "        con_colls = \"\" # no C2022 collocation available \n",
    "        \n",
    "        \n",
    "    ### SIMILAR WORDS ###\n",
    "    \n",
    "    # if we have at least one similar word \n",
    "    if len(knowledge_base[\"similar_words\"].iloc[idx]) != 0:\n",
    "        \n",
    "        # retrieve the words and re-append the prefix \"Klima\" to the words\n",
    "        similar_words = set([\"Klima\"+x for x in knowledge_base[\"similar_words\"].iloc[idx] if \"Klima\"+x != compound])\n",
    "        \n",
    "        # connect the words with a comma \n",
    "        similar_words = \", \".join(similar_words)\n",
    "        \n",
    "        text += ds.str_simwords # add \"similar words\" string to base string\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # if no similar words available, keep empty\n",
    "        similar_words = \"\"\n",
    "\n",
    "    ### FILL PLACE HOLDERS ###\n",
    "            \n",
    "    # assign the fillers to the according place holders in the final definition string    \n",
    "    full_definition = text.format(COMPOUND= compound, ARTICLE = article, CON_FREQ= con_freq, PRO_FREQ = pro_freq, \n",
    "                                  CON_TFIDF = con_tfidf, PRO_TFIDF = pro_tfidf, SENTIMENT= sentiment, CON_PERS = con_pers, \n",
    "                                  PRO_PERS = pro_pers, CON_ORG = con_org, PRO_ORG = pro_org, SIMILAR_WORDS= similar_words, \n",
    "                                  PRO_MODS = pro_mods, CON_MODS = con_mods, PRO_COLLS = pro_colls, CON_COLLS = con_colls, \n",
    "                                  ATTRIBUTION = attr, SARCASM = sarcasm)\n",
    "    \n",
    "\n",
    "    #print(full_definition)\n",
    "    #print(\"\\n\")\n",
    "    #print(\"_\"*50)\n",
    "    \n",
    "    # write full definition to text file (for evaluation purposes)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(full_definition)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"_\"*50)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # save to column \"full_definition\" in knowledge base \n",
    "    knowledge_base.at[idx, \"full_definition\"] = full_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d410ce3-e81d-406f-956b-b6173d698bfd",
   "metadata": {},
   "source": [
    "Let's have a look at an example definition text for the compound **Klimalüge**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14fb97ec-3f36-42ca-a441-d516dfc939d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klimalüge, die\n",
      "Der Begriff 'Klimalüge' wird in unserem Korpus 18 Mal von den Klimaforschungsskeptikern und 3 Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen entspricht das einer relativen Häufigkeit (TF-IDF) von 0.191 für die Skeptiker und 0.016 für die Vertreter. Die Verwendung wird in 33% der Fälle für den Vertreter Diskurs und in 11% der Fälle für den Skeptiker Diskurs als sarkastisch eingestuft. In unserem Korpus Sample ist der Begriff tendenziell negativ konnotiert. Im Zusammenhang mit dem Begriff erwähnt der Skeptiker Korpus die Person(en) Christian Möser und Hartmut Bachmann und der Vertreter Korpus die Person(en) Hartmut Bachmann und Donald Trump. Im Kontext von 'Klimalüge' erfolgt die Nennung folgender Organisation(en): FFF, EIKE (Skeptiker Korpus) und EU, IPCC (Vertreter Korpus).\n",
      "\n",
      "Kollokationen: 'aufzuschwätzen', 'Welt' (Skeptiker Korpus)\n",
      "\n",
      "Siehe auch: Klimaleugner, Klimaerzählung, Klimalügner\n"
     ]
    }
   ],
   "source": [
    "print(knowledge_base[knowledge_base.original == \"klimalüge\"].full_definition.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c626085-84cb-450c-83bf-228a1394912e",
   "metadata": {},
   "source": [
    "***\n",
    "To retrieve a definition for a random glossary term please run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0972e9bc-96aa-42e7-a593-b61c28062029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klimahype, der\n",
      "Der Begriff 'Klimahype' wird in unserem Korpus 15 Mal von den Klimaforschungsskeptikern und 0 Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen entspricht das einer relativen Häufigkeit (TF-IDF) von 0.178 für die Skeptiker und 0.0 für die Vertreter. Die Verwendung wird nicht als sarkastisch eingestuft. In unserem Korpus Sample ist der Begriff tendenziell neutral konnotiert. Im Zusammenhang mit dem Begriff erwähnt der Skeptiker Korpus die Person(en) Stefan Rahmstorf und Angela Merkel. Im Kontext von 'Klimahype' erfolgt die Nennung folgender Organisation(en): EIKE, AWI (Skeptiker Korpus).\n",
      "\n",
      "Kollokationen: 'Wirkmächtigkeit', 'kommen' (Skeptiker Korpus)\n",
      "\n",
      "Siehe auch: Klimahysterie\n"
     ]
    }
   ],
   "source": [
    "print(knowledge_base.sample().full_definition.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8824e-23c2-4bd9-ba5c-201ca895a017",
   "metadata": {},
   "source": [
    "### Save Definition Texts to Knowledge Base\n",
    "In the end, we will save the final definition texts to an updated knowledge base file `knowledge_base_updated.csv`. We won't overwrite thew old knowledge base (output of `textmining.ipynb`) to be able to keep track of the progress and the full documentation of the work within the complete project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15c2dede-08fc-44ad-bb12-02d9771646bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file \n",
    "knowledge_base.to_csv(\"../output/knowledge_base_updated.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

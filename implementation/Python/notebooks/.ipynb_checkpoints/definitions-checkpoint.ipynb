{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9d0c80-e709-43a4-bb78-d73b173ecbf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition Phrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da232ce1-0451-4e13-95df-9923d651c138",
   "metadata": {},
   "source": [
    "## 0. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ace2a931-9ed2-45e3-9fc4-7dad00c91813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88444c5c-8e3a-438d-a373-d62d396326ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import requests\n",
    "import definition_strings as ds\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44903fa-bcd5-473d-8fab-a6057f1fafca",
   "metadata": {},
   "source": [
    "## 1. Load Knowledge Base\n",
    "We load the knowledge base that was created in the `textmining.ipynb` notebook and drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c61979c8-3cdb-4613-bf8c-ded1d9d60695",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pd.read_csv(\"../output/knowledge_base.csv\")\n",
    "\n",
    "to_drop = [ 'noun_forms', 'related_words', 'hypernyms', 'roots', 'en_hypernyms', 'path', 'wup', 'stem_cistem', 'stem_porter',\n",
    "       'stem_lancaster', 'stem_snowball', 'share_cistem', 'share_porter', 'share_lancaster', 'share_snowball', 'dist_stemmer']\n",
    "\n",
    "compounds = compounds.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf4946f9-00f0-4c9a-94cd-c30d51c2bb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original', 'second_part', 'lemma', 'genus', 'compound_forms',\n",
       "       'concept', 'definition', 'similar_words', 'PERS_pro', 'ORG_pro',\n",
       "       'PERS_con', 'ORG_con', 'pro_mods', 'con_mods', 'pro_sarcasm',\n",
       "       'con_sarcasm', 'pro_attr', 'con_attr', 'tf_pro', 'tf_con', 'tfidf_pro',\n",
       "       'tfidf_con', 'pro_colls', 'con_colls', 'manual_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compounds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07c406-e8bb-4f78-aeaf-3edd282b8431",
   "metadata": {},
   "source": [
    "# 2. Preprocessing of the information\n",
    "When loading the csv file via `pandas` oftentimes requires the re-evaluation of the literals contained in the data frame. Accordingly, we run the following code to make sure all cell types are evaluated correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bab185e7-2976-4c9b-9ac9-0b3ca007bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds['compound_forms'] = compounds.compound_forms.apply(lambda x: literal_eval(str(x)))\n",
    "#compounds['related_words'] = compounds.related_words.apply(lambda x: literal_eval(str(x)))\n",
    "#compounds['hypernyms'] = compounds.hypernyms.apply(lambda x: literal_eval(str(x)))\n",
    "#compounds['en_hypernyms'] = compounds.en_hypernyms.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['definition'] = compounds.definition.apply(lambda x: literal_eval(str(x)))\n",
    "compounds['PERS_pro'] = compounds.PERS_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['PERS_con'] = compounds.PERS_con.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['ORG_pro'] = compounds.ORG_pro.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['ORG_con'] = compounds.ORG_con.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['similar_words'] = compounds.similar_words.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['pro_mods'] = compounds.pro_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds['con_mods'] = compounds.con_mods.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else x)\n",
    "compounds[\"PERS_pro\"] = compounds.PERS_pro.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"PERS_con\"] = compounds.PERS_con.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"ORG_pro\"] = compounds.ORG_pro.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"ORG_con\"] = compounds.ORG_con.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"pro_attr\"] = compounds.pro_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != 'nan') else x)\n",
    "compounds[\"con_attr\"] = compounds.con_attr.apply(lambda x: \"\".join(literal_eval(str(x))) if(str(x) != 'nan') else x)\n",
    "compounds[\"pro_colls\"] = compounds.pro_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else list())\n",
    "compounds[\"con_colls\"] = compounds.con_colls.apply(lambda x: literal_eval(str(x)) if(str(x) != 'nan') else list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc686920-701a-4775-ae6f-ffad14423cf9",
   "metadata": {},
   "source": [
    "Next we apply some preprocessing steps to translate and reduce some of the pieces of information that are contained in the knowledge base:\n",
    "- translate polarity labels into German counterparts\n",
    "- translate attribution tags into German counterparts\n",
    "- count modifiers\n",
    "- replace unavailable TF-IDF scores with 0\n",
    "- retrieve definite articles from genus column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "629865cb-db2d-4f38-9e40-0d83707e8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace sentiment with the German counterparts\n",
    "compounds[\"manual_sentiment\"] = compounds.manual_sentiment.replace(\"negative\", \"negativ\")\n",
    "compounds[\"manual_sentiment\"] = compounds.manual_sentiment.replace(\"positive\", \"positiv\")\n",
    "\n",
    "# replace attribution tag with German counterpart\n",
    "compounds[\"pro_attr\"] = compounds.pro_attr.replace(\"Self\", \"Selbstzuschreibung\")\n",
    "compounds[\"pro_attr\"] = compounds.pro_attr.replace(\"External\", \"Fremdzuschreibung\")\n",
    "compounds[\"con_attr\"] = compounds.con_attr.replace(\"Self\", \"Selbstzuschreibung\")\n",
    "compounds[\"con_attr\"] = compounds.con_attr.replace(\"External\", \"Fremdzuschreibung\")\n",
    "\n",
    "# count modifiers\n",
    "compounds[\"pro_mods\"] = compounds.pro_mods.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "compounds[\"con_mods\"] = compounds.con_mods.apply(lambda x: Counter(x) if(str(x) != 'nan') else x)\n",
    "\n",
    "# replace nan values in TF-IDF columnd with 0\n",
    "compounds[\"tfidf_pro\"] = compounds.tfidf_pro.replace(np.nan, 0)\n",
    "compounds[\"tfidf_con\"] = compounds.tfidf_con.replace(np.nan, 0)\n",
    "\n",
    "# change genus to article information\n",
    "# create a list of our conditions\n",
    "conditions = [(compounds['genus'] == \"f\"),(compounds['genus'] == \"m\"), (compounds['genus'] == \"n\")]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['die', 'der', 'das']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "compounds['article'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8fb0e872-84d2-4e19-992e-0911f1bfb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "61cd0609-9ffd-47c0-b771-cfbae1c40951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lemma': 'Haus', 'url': 'https://www.dwds.de/wb/Haus', 'wortart': 'Substantiv', 'input': 'Haus'}]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://www.dwds.de/api/wb/snippet/?q=Haus\") as url:\n",
    "    data = json.load(url)\n",
    "    print(data)\n",
    "\n",
    "#url = \"https://www.dwds.de/api/wb/snippet/?q=Haus\"\n",
    "#response = urllib.urlopen(url)\n",
    "#data = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "6b0e8b77-50e9-435f-9c15-a836d8806af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Haus'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"lemma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cfe3a-86aa-42fd-bea9-86183d34795e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create Placeholders\n",
    "Next we will load the definition strings and place holders into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c76ee7-b070-4c6d-b4e5-ebc7cf9b2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import definition_strings as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b044f49b-d6db-4574-a6fe-483ac30f4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIATE STRING BASES FOR EACH CATEGORY\n",
    "\n",
    "str_base_info = \"\"\"{COMPOUND}, {ARTICLE}\\n\"\"\"\n",
    "\n",
    "str_base_pers = \"\"\"Der Begriff {COMPOUND} bezeichnet eine Person, die in einer gewissen Beziehung zum Klimawandel steht. Der Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaforschungsskeptikern und {PRO_FREQ} Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von {CON_TFIDF} für die Skeptiker und {PRO_TFIDF} für die Vertreter.\"\"\" \n",
    "\n",
    "str_base_loc = \"\"\"Der Begriff {COMPOUND} bezeichnet eine Lokalität im Bezug auf den Klimawandel. Der Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaforschungsskeptikern und {PRO_FREQ} Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von {CON_TFIDF} für die Skeptiker und {PRO_TFIDF} für die Vertreter.\"\"\" \n",
    "\n",
    "str_base_action = \"\"\"Der Begriff {COMPOUND} bezeichnet eine Aktion im Bezug auf den Klimawandel. Der Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaforschungsskeptikern und {PRO_FREQ} Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von {CON_TFIDF} für die Skeptiker und {PRO_TFIDF} für die Vertreter.\"\"\" \n",
    "\n",
    "str_base_abstract = \"\"\"Der Begriff {COMPOUND} bezeichnet ein Konzept in Relation zum Klimawandel. Der Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaforschungsskeptikern und {PRO_FREQ} Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von {CON_TFIDF} für die Skeptiker und {PRO_TFIDF} für die Vertreter.\"\"\" \n",
    "\n",
    "str_base_group = \"\"\"Der Begriff {COMPOUND} bezeichnet einen Zusammenschluss von Personen im Bezug auf den Klimawandel. Der Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaforschungsskeptikern und {PRO_FREQ} Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von {CON_TFIDF} für die Skeptiker und {PRO_TFIDF} für die Vertreter.\"\"\" \n",
    "\n",
    "# INITIATE STRINGS THAT ARE EQUAL FOR ALL CATEGORIES\n",
    "\n",
    "str_sent =  \"\"\" In unserem Korpus Sample ist der Begriff meist {SENTIMENT} konnotiert.\"\"\"\n",
    "\n",
    "#str_attr_con = \"\"\" Hierbei wird „{COMPOUND}“ von Seiten der Skeptiker im Sinne einer {CON_ATTRIBUTION}\"\"\" #(verwendet)\n",
    "#str_attr_pro = \"\"\" und von Vertretern als {PRO_ATTRIBUTION} verwendet.\"\"\"\n",
    "\n",
    "str_attr = \"\"\"Verwendet wird \"{COMPOUND}\" hierbei im Sinne einer {ATTRIBUTION}\"\"\"\n",
    "str_sarcasm = \"\"\" In {SARCASM}\"\"\"\n",
    "\n",
    "str_mods_pro = \"\"\" Im Subdiskurs der Klimaforschungsvertreter wird der Begriff von Wörtern wie {PRO_MODS} modifiziert.\"\"\" \n",
    "str_mods_con = \"\"\" Modifizierer wie {CON_MODS} treten häufig auf, um den Begriff im Subdiskurs der Klimaforschungsskeptiker näher zu beschreiben.\"\"\" \n",
    "\n",
    "#str_pers_con = \"\"\" Personen, die im Zusammenhang mit dem Begriff erwähnt werden sind {CON_PERS} (Skeptiker)\"\"\"\n",
    "str_pers_con = \"\"\" Im Zusammenhang mit dem Begriff erwähnt der Skeptiker Korpus die Person(en) {CON_PERS}\"\"\"\n",
    "#str_pers_pro = \"\"\" und {PRO_PERS} (Vertreter).\"\"\"\n",
    "str_pers_pro = \"\"\" und der Vertreter Diskurs die Person(en) {PRO_PERS}.\"\"\"\n",
    "\n",
    "str_org_con = \"\"\" Außerdem werden im Kontext von \"{COMPOUND}\" folgende Organisationen genannt: {CON_ORG} (Skeptiker Korpus)\"\"\"\n",
    "str_org_pro = \"\"\" und {PRO_ORG} (Vertreter Korpus).\"\"\"  \n",
    "\n",
    "str_colls = \"\"\"\\nHäufige Kollokationen: {CON_COLLS} {PRO_COLLS}\"\"\"\n",
    "str_simwords = \"\"\"\\n\\nSiehe auch: {SIMILAR_WORDS}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1727261-d153-456c-bfa5-98d08dfb92a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'bekannt' und 'weltweit'\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mods =  \" und \".join([\"'\"+el[0]+\"'\" for el in compounds['con_mods'].iloc[2].most_common(2) if el[1] > 1])\n",
    "\n",
    "con_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289cb1-aacf-45ea-be44-d80d8d7118c5",
   "metadata": {},
   "source": [
    "## 3. Fill Placeholders\n",
    "Next, for each compound word, we will generate a final combination of strings (according to the unique information pieces that we have for this compound) and fill the place holders (denoted in swift brackets) with these information pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "852b50ee-82dc-4313-b87e-47e61f05782b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "COLLS FOR BOTH\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO SIMILAR WORDS\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO SIMILAR WORDS\n",
      "COLLS FOR BOTH\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PERSON FOUND\n",
      "NO CON MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n",
      "NO PRO MOD FOUND\n",
      "NO PERSON FOUND\n",
      "NO PERSON FOUND\n",
      "NO SIMILAR WORDS\n"
     ]
    }
   ],
   "source": [
    "# for each compound word \n",
    "for word in compounds.original:\n",
    "    \n",
    "    # set index to this compound word \n",
    "    idx = compounds.index[compounds['original'] == word][0]\n",
    "        \n",
    "    ### BASE INFORMATION ####\n",
    "    \n",
    "    # initiate base info string (i.e. compound + genus)\n",
    "    text = str_base_info\n",
    "    \n",
    "    # retrieve base information from knowledge base \n",
    "    compound = word.capitalize() # capitalized version of compound\n",
    "    article = compounds['article'].iloc[idx] # definite article\n",
    "    con_freq = compounds['tf_con'].iloc[idx] # term frequency C2022\n",
    "    pro_freq = compounds['tf_pro'].iloc[idx] # term frequency P2022\n",
    "    con_tfidf = round(compounds['tfidf_con'].iloc[idx],2) # TF-IDF C2022\n",
    "    pro_tfidf = round(compounds['tfidf_pro'].iloc[idx],2) # TF-IDF P2022        \n",
    "    \n",
    "    # if category of compound is \"person\"\n",
    "    if compounds['concept'].iloc[idx] == \"person\":\n",
    "        text += str_base_pers # add \"person\" string to base string\n",
    "        \n",
    "        ### ATTRIBUTION AND SARCASM ###\n",
    "        \n",
    "        text += str_attr # add \"attribution\" string to base string\n",
    "        text += str_sarcasm # add \"sarcasm\" string to base string \n",
    "        \n",
    "        # retrieve attribution information from knowledge base \n",
    "        pro_attr = compounds['pro_attr'].iloc[idx]\n",
    "        con_attr = compounds['con_attr'].iloc[idx]\n",
    "        \n",
    "        # retrieve sarcasm information from knowledge base\n",
    "        pro_sarc = compounds['pro_sarcasm'].iloc[idx]\n",
    "        con_sarc = compounds['con_sarcasm'].iloc[idx]\n",
    "        \n",
    "        # if we have attribution info for both corpora\n",
    "        if str(compounds['con_attr'].iloc[idx]) != \"nan\" and str(compounds['pro_attr'].iloc[idx]) != \"nan\":\n",
    "            \n",
    "            # compose attribution filler\n",
    "            attr = con_attr + \"von Seiten der Skeptiker und als \" + pro_attr + \" im Vetreter Korpus.\"\n",
    "                        \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(pro_sarc*100)) + \" % (Vertreter) und \" + str(int(con_sarc*100)) + \" % (Skeptiker) der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    "          \n",
    "        # if we only have attribution info for C2022\n",
    "        elif str(compounds['con_attr'].iloc[idx]) != \"nan\" and str(compounds['pro_attr'].iloc[idx]) == \"nan\":\n",
    "            \n",
    "            # compose attribution filler\n",
    "            attr = con_attr + \"von Seiten der Skeptiker.\"\n",
    "                        \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(con_sarc*100)) + \" % der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    "\n",
    "        # if we only have attribution info for P2022\n",
    "        elif str(compounds['con_attr'].iloc[idx]) == \"nan\" and str(compounds['pro_attr'].iloc[idx]) != \"nan\":\n",
    "            \n",
    "            # compose attribution filler\n",
    "            attr = pro_attr + \"von Seiten der Vertreter.\"\n",
    "            \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(pro_sarc*100)) + \" % der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    "            \n",
    "    # if category of compound is \"location\"\n",
    "    elif compounds['concept'].iloc[idx] == \"location\":\n",
    "        text += str_base_loc # add \"location\" string to base string \n",
    "        attr = \"\" # attribution not available\n",
    "        sarcasm = \"\" # sarcasm not available\n",
    "    \n",
    "    # if category of compound is \"group\"\n",
    "    elif compounds['concept'].iloc[idx] == \"group\":\n",
    "        text += str_base_group # add \"group\" string to base string \n",
    "        \n",
    "        ### ATTRIBUTION AND SARCASM ###\n",
    "        \n",
    "        text += str_attr # add \"attribution\" string to base string\n",
    "        text += str_sarcasm # add \"sarcasm\" string to base string\n",
    "        \n",
    "        # retrieve attribution information from knowledge base \n",
    "        pro_attr = compounds['pro_attr'].iloc[idx]\n",
    "        con_attr = compounds['con_attr'].iloc[idx]\n",
    "        \n",
    "        # retrieve sarcasm information from knowledge base \n",
    "        pro_sarc = compounds['pro_sarcasm'].iloc[idx]\n",
    "        con_sarc = compounds['con_sarcasm'].iloc[idx]\n",
    "  \n",
    "        \n",
    "        # if we have attribution info for both corpora\n",
    "        if str(compounds['con_attr'].iloc[idx]) != \"nan\" and str(compounds['pro_attr'].iloc[idx]) != \"nan\":\n",
    "\n",
    "            # compose attribution filler\n",
    "            attr = con_attr + \"von Seiten der Skeptiker und als \" + pro_attr + \" im Vetreter Korpus.\"\n",
    "  \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(pro_sarc*100)) + \" % (Vertreter) und \" + str(int(con_sarc*100)) + \" % (Skeptiker) der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    " \n",
    "        # if we only have attribution info for C2022\n",
    "        elif str(compounds['con_attr'].iloc[idx]) != \"nan\" and str(compounds['pro_attr'].iloc[idx]) == \"nan\":\n",
    "            \n",
    "            # compose attribution filler\n",
    "            attr = con_attr + \"von Seiten der Skeptiker.\" \n",
    "            \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(con_sarc*100)) + \" % der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    "\n",
    "        # if we only have attribution info for P2022\n",
    "        elif str(compounds['con_attr'].iloc[idx]) == \"nan\" and str(compounds['pro_attr'].iloc[idx]) != \"nan\":\n",
    "            \n",
    "            # compose attribution filler\n",
    "            attr = pro_attr + \"von Seiten der Vertreter.\" \n",
    "                        \n",
    "            # compose sarcasm filler\n",
    "            sarcasm += str(int(pro_sarc*100)) + \" % der Fälle wird die Verwendung als sarkatisch eingestuft.\"\n",
    "            \n",
    "    # if category of compound is \"abstraction\"        \n",
    "    elif compounds['concept'].iloc[idx] == \"abstraction\":\n",
    "        text += str_base_abstract # add \"abstraction\" string to base string \n",
    "        attr = \"\" # attribution not available\n",
    "        sarcasm = \"\" # sarcasm not available\n",
    "    \n",
    "    # if category of compound is \"action\"\n",
    "    elif compounds['concept'].iloc[idx] == \"action\":\n",
    "        text += str_base_action # add \"action\" string to base string \n",
    "        attr = \"\" # attribution not available\n",
    "        sarcasm = \"\" # sarcasm not available\n",
    "    \n",
    "    ### CONNOTATION ### \n",
    "        \n",
    "    # add sentiment string\n",
    "    text += str_sent \n",
    "    \n",
    "    sentiment = compounds['manual_sentiment'].iloc[idx] # retrieve connotation label \n",
    "        \n",
    "    ### MODIFIERS ###    \n",
    "    \n",
    "    # if we have at least one P2022 modifier for compound\n",
    "    if compounds['pro_mods'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common modifiers and connect with conjunction\n",
    "            pro_mods =  \" und \".join([\"'\"+el[0]+\"'\" for el in compounds['pro_mods'].iloc[idx].most_common(2) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty \n",
    "            if pro_mods != \"\":\n",
    "                text += str_mods_pro # add \"pro modifier\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one modifier available retrieve this one\n",
    "            pro_mods = \"\".join([\"'\"+el[0]+\"'\" for el in compounds['pro_mods'].iloc[idx].most_common(1) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty\n",
    "            if pro_mods != \"\":\n",
    "                text += str_mods_pro # add \"pro modifier\" string to base string \n",
    "    else:\n",
    "        print(\"NO PRO MOD FOUND\")\n",
    "        pro_mods = \"\" # else, no P2022 modifier available \n",
    "            \n",
    "    # if we have at least one C2022 modifier for compound\n",
    "    if compounds['con_mods'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common modifiers and connect with conjunction\n",
    "            con_mods =  \" und \".join([\"'\"+el[0]+\"'\" for el in compounds['con_mods'].iloc[idx].most_common(2) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not empty\n",
    "            if con_mods != \"\":\n",
    "                text += str_mods_con # add \"con modifier\" string to base string \n",
    "        \n",
    "        except:\n",
    "            # if only one modifier available retrieve this one\n",
    "            con_mods = \"\".join([\"'\"+el[0]+\"'\" for el in compounds['con_mods'].iloc[idx].most_common(1) if el[1] > 1])\n",
    "            \n",
    "            # if modifier string is not emtpy \n",
    "            if con_mods != \"\": \n",
    "                text += str_mods_con # add \"con modifier\" string to base string \n",
    "    else:\n",
    "        print(\"NO CON MOD FOUND\")\n",
    "        con_mods = \"\" # else, no C2022 modifier available \n",
    "            \n",
    "    \n",
    "    ### PERSON ENTITIES ###\n",
    "        \n",
    "    # if we have at least one person in C2022\n",
    "    if compounds['PERS_con'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common persons and connect with conjunction\n",
    "            con_pers = \" und \".join([el[0] for el in compounds['PERS_con'].iloc[idx].most_common(2)])\n",
    "            text += str_pers_con # add \"con person\" string to base string \n",
    "            \n",
    "        except:\n",
    "            # if only one person available retrieve this one\n",
    "            con_pers = compounds['PERS_con'].iloc[idx].most_common(1)[0]\n",
    "            text += str_pers_con # add \"con person\" string to base string \n",
    "    \n",
    "    else:\n",
    "        print(\"NO PERSON FOUND\")\n",
    "        con_pers = \"\" \n",
    "            \n",
    "    # if we have at least one person in P2022\n",
    "    if compounds['PERS_pro'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common persons and connect with conjunction\n",
    "            pro_pers = \" und \".join([el[0] for el in compounds['PERS_pro'].iloc[idx].most_common(2)])\n",
    "            text +=  str_pers_pro # add \"pro person\" string to base string \n",
    "\n",
    "\n",
    "        except:\n",
    "            # if only one person available retrieve this one\n",
    "            pro_pers = compounds['PERS_pro'].iloc[idx].most_common(1)[0]\n",
    "            text += str_pers_pro # add \"pro person\" string to base string \n",
    "                \n",
    "    else:\n",
    "        pro_pers = \"\"\n",
    "        print(\"NO PERSON FOUND\")\n",
    "        \n",
    "    ### ORGANISATION ENTITIES ### \n",
    "            \n",
    "    # if we have at least one organisation in C2022\n",
    "    if compounds['ORG_con'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common organisations and connect with comma\n",
    "            con_org =  \", \".join([el[0] for el in compounds['ORG_con'].iloc[idx].most_common(2)])\n",
    "            text += str_org_con # add \"con organisation\" string to base string \n",
    "        except:\n",
    "            # if only one organisation available retrieve this one\n",
    "            con_org = compounds['ORG_con'].iloc[idx].most_common(1)[0]\n",
    "            text += str_org_con # add \"con organisation\" string to base string  \n",
    "                \n",
    "    else:\n",
    "        con_org = \"\" \n",
    "            \n",
    "            \n",
    "    # if we have at least one organisation in P2022\n",
    "    if compounds['ORG_pro'].isna().iloc[idx] == False:\n",
    "        try:\n",
    "            # try to retrieve two most common organisations and connect with comma\n",
    "            pro_org =  \", \".join([el[0] for el in compounds['ORG_pro'].iloc[idx].most_common(2)])\n",
    "            text += str_org_pro # add \"pro organisation\" string to base string \n",
    "            \n",
    "        except:\n",
    "            \n",
    "            # if only one organisation available retrieve this one\n",
    "            pro_org = compounds['ORG_pro'].iloc[idx].most_common(1)[0]\n",
    "            text += str_org_pro # add \"pro organisation\" string to base string \n",
    "                \n",
    "    else:\n",
    "        pro_org = \"\"\n",
    "        text += \".\" # add final stop to definition text\n",
    "            \n",
    "            \n",
    "    ### SIMILAR COMPOUNDS ###\n",
    "    \n",
    "    # if we have at least one similar word \n",
    "    if len(compounds['similar_words'].iloc[idx]) != 0:\n",
    "        \n",
    "        # retrieve the words and re-append the prefix \"Klima\" to the words\n",
    "        similar_words = set([\"Klima\"+x for x in compounds['similar_words'].iloc[idx] if \"Klima\"+x != compound])\n",
    "        \n",
    "        # connect the words with a comma \n",
    "        similar_words = \", \".join(similar_words)\n",
    "        \n",
    "        text += str_simwords # add \"similar words\" string to base string\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(\"NO SIMILAR WORDS\")\n",
    "        similar_words = \"\"\n",
    "        \n",
    "    ### COLLOCATIONS ###\n",
    "        \n",
    "    # if we have collocations for C2022 and P2022\n",
    "    if len(compounds['con_colls'].iloc[idx]) != 0 and len(compounds['pro_colls'].iloc[idx]) != 0:\n",
    "        print(\"COLLS FOR BOTH\")\n",
    "        try:\n",
    "            # try to retrieve two random collocations from C2022 and connect with comma and compose string\n",
    "            con_colls = \", \".join(random.sample(compounds['con_colls'].iloc[idx], 2)) + \" (Skeptiker)\"\n",
    "        except:\n",
    "            # if only one collocation from C2022 available retrieve this one and compose string\n",
    "            con_colls = \", \".join(random.sample(compounds['con_colls'].iloc[idx], 1)) + \" (Skeptiker)\"\n",
    "\n",
    "        try:\n",
    "            # try to retrieve two random collocations from P2022 and connect with comma and compose string \n",
    "            pro_colls = \"und \" + \", \".join(random.sample(compounds['pro_colls'].iloc[idx], 2)) + \" (Vertreter)\"\n",
    "        except:\n",
    "            # if only one collocation from P2022 available retrieve this one\n",
    "            pro_colls = \"und \" + \", \".join(random.sample(compounds['pro_colls'].iloc[idx], 1)) + \" (Vertreter)\"\n",
    "\n",
    "        text += str_colls # add \"collocations\" string to base string \n",
    "        \n",
    "    # if we only have collocations for C2022\n",
    "    elif len(compounds['con_colls'].iloc[idx]) != 0:\n",
    "        try:\n",
    "            # try to retrieve two random collocations from C2022 and connect with comma and compose string\n",
    "            con_colls = \", \".join(random.sample(compounds['con_colls'].iloc[idx], 2)) + \" (Skeptiker)\"\n",
    "        except:\n",
    "            # if only one collocation from C2022 available retrieve this one and compose string\n",
    "            con_colls = \", \".join(random.sample(compounds['con_colls'].iloc[idx], 1)) + \" (Skeptiker)\"\n",
    "\n",
    "\n",
    "        pro_colls = \" \" # no collocations for P2022 available \n",
    "        text += str_colls # add \"collocations\" string to base string\n",
    "        \n",
    "    # if we only have collocations for P2022\n",
    "    elif len(compounds['pro_colls'].iloc[idx]) != 0:\n",
    "        try:\n",
    "            # try to retrieve two random collocations from P2022 and connect with comma and compose string \n",
    "            pro_colls = \", \".join(random.sample(compounds['pro_colls'].iloc[idx], 2)) + \" (Skeptiker)\"\n",
    "        except:\n",
    "            # if only one collocation from P2022 available retrieve this one\n",
    "            pro_colls = \", \".join(random.sample(compounds['pro_colls'].iloc[idx], 1)) + \" (Skeptiker)\"\n",
    "\n",
    "\n",
    "        con_colls = \" \" # no collocations for C2022 available\n",
    "        text += str_colls # add \"collocations\" string to base string \n",
    "        \n",
    "    else:\n",
    "        pro_colls = \"\"\n",
    "        con_colls = \"\"\n",
    "\n",
    "    ### FINAL DEFINITION ###\n",
    "            \n",
    "    # assign the fillers to the according place holders in the final definition string    \n",
    "    full_definition = text.format(COMPOUND= compound, ARTICLE = article, CON_FREQ= con_freq, PRO_FREQ = pro_freq, \n",
    "                                  CON_TFIDF = con_tfidf, PRO_TFIDF = pro_tfidf, SENTIMENT= sentiment, CON_PERS = con_pers, \n",
    "                                  PRO_PERS = pro_pers, CON_ORG = con_org, PRO_ORG = pro_org, SIMILAR_WORDS= similar_words, \n",
    "                                  PRO_MODS = pro_mods, CON_MODS = con_mods, PRO_COLLS = pro_colls, CON_COLLS = con_colls, \n",
    "                                  ATTRIBUTION = attr, SARCASM = sarcasm)\n",
    "        \n",
    "        \n",
    "    #print(full_definition)\n",
    "    #print(\"_\"*50)\n",
    "                \n",
    "    # save to column \"full_definition\"\n",
    "    compounds.at[idx, 'full_definition'] = full_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d749a6a5-ac46-471e-8089-e302d5e91c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klimaaktivist, der\n",
      "Der Begriff Klimaaktivist bezeichnet eine Person, die in einer gewissen Beziehung zum Klimawandel steht. Der Begriff wird in unserem Korpus 66 Mal von den Klimaforschungsskeptikern und 61 Mal von den Klimaforschungsvertretern verwendet. Auf den gesamten Korpus gesehen, entspricht das einer relativen Häufigkeit (TF-IDF) von 0.61 für die Skeptiker und 0.16 für die Vertreter.Verwendet wird \"Klimaaktivist\" hierbei im Sinne einer Fremdzuschreibungvon Seiten der Skeptiker und als Selbstzuschreibung im Vetreter Korpus. In 0 % (Vertreter) und 1 % (Skeptiker) der Fälle wird die Verwendung als sarkatisch eingestuft. In unserem Korpus Sample ist der Begriff meist neutral konnotiert. Im Subdiskurs der Klimaforschungsvertreter wird der Begriff von Wörtern wie 'lieb' und 'jung' modifiziert. Modifizierer wie 'bekannt' und 'weltweit' treten häufig auf, um den Begriff im Subdiskurs der Klimaforschungsskeptiker näher zu beschreiben. Im Zusammenhang mit dem Begriff erwähnt der Skeptiker Korpus die Person(en) Greta Thunberg und Bill McKibben und der Vertreter Diskurs die Person(en) Greta Thunberg und Vanessa Nakate. Außerdem werden im Kontext von \"Klimaaktivist\" folgende Organisationen genannt: EIKE, IPCC (Skeptiker Korpus) und FFF, EU (Vertreter Korpus).\n",
      "\n",
      "Siehe auch: Klimademagoge, Klimaaktivismus, Klimamacher\n",
      "Häufige Kollokationen: Jahr, bekannt (Skeptiker) und indigen, Klimaziel (Vertreter)\n"
     ]
    }
   ],
   "source": [
    "print(compounds.full_definition[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f7d45-2fb0-4dd2-9153-5d62c7f552af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#def fill_persons(df):\n",
    "    \n",
    "for word in compounds.original:\n",
    "    #print(compound)\n",
    "    idx = compounds.index[compounds['original'] == word][0]\n",
    "    \n",
    "    if compounds['concept'].iloc[idx] == \"person\":\n",
    "        #print(compound)\n",
    "        \n",
    "        #text = \"\"\"Der Begriff {COMPOUND} bezeichnet eine/n {DEFINITION} im Bezug auf den Klimawandel.\\nDer Begriff wird in unserem Korpus {CON_FREQ} Mal von den Klimaskeptikern und {PRO_FREQ} Mal von den Klimaaktivisten verwendet und ist {SENTIMENT} konnotiert. \"\"\"\n",
    "        text = str_base_pers\n",
    "        \n",
    "        # retrieve according information\n",
    "        compound = word.capitalize()\n",
    "        con_freq = compounds['tf_con'].iloc[idx]\n",
    "        pro_freq = compounds['tf_pro'].iloc[idx]\n",
    "        sentiment = compounds['manual_sentiment'].iloc[idx]\n",
    "        \n",
    "        try:\n",
    "            definition = compounds['definition'].iloc[idx][0] # try [0] for definition\n",
    "        except:\n",
    "            #print(\"NO DEFINITION AVAILABLE\")\n",
    "            definition = \" \"\n",
    "            \n",
    "        \n",
    "        # retrieve number of persons. if 0 then do nothing\n",
    "        \n",
    "        # if we have at least one person in PERS_con\n",
    "        if compounds['PERS_con'].isna().iloc[idx] == False:\n",
    "            try:\n",
    "                con_pers =  \" und \".join([el[0] for el in compounds['PERS_con'].iloc[idx].most_common(2)])\n",
    "                #text += \"\"\"\\nPersonen, die im Zusammenhang mit dem Begriff erwähnt werden sind {CON_PERS} (Con Corpus) \"\"\"\n",
    "                text += str_pers_con\n",
    "            except:\n",
    "                con_pers = compounds['PERS_con'].iloc[idx].most_common(1)[0]\n",
    "                #text += \"\"\"\\nPersonen, die im Zusammenhang mit dem Begriff erwähnt werden sind {CON_PERS} (Con Corpus) \"\"\"\n",
    "                text += str_pers_con\n",
    "        else:\n",
    "          #  print(\"NO PERSON FOUND\")\n",
    "            #con_pers = \" \" \n",
    "            \n",
    "        # if we have at least one person in PERS_con\n",
    "        if compounds['PERS_pro'].isna().iloc[idx] == False:\n",
    "            try:\n",
    "                #print(\"2 PERSONS FOUND\")\n",
    "                pro_pers = \" und \".join([el[0] for el in compounds['PERS_pro'].iloc[idx].most_common(2)])\n",
    "                #text +=  \"\"\"und {PRO_PERS} (Pro Corpus). \"\"\"\n",
    "                text += str_pers_pro\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"ONLY 1 PERSON FOUND\")\n",
    "                pro_pers = compounds['PERS_pro'].iloc[idx].most_common(1)[0]\n",
    "                #text +=  \"\"\"und {PRO_PERS} (Pro Corpus). \"\"\"\n",
    "                text += str_pers_pro\n",
    "                \n",
    "        else:\n",
    "            pro_pers = \" \"\n",
    "            #print(\"NO PERSON FOUND\")\n",
    "            \n",
    "        if compounds['ORG_con'].isna().iloc[idx] == False:\n",
    "            try:\n",
    "                con_org =  \", \".join([el[0] for el in compounds['ORG_con'].iloc[idx].most_common(2)])\n",
    "                #text +=  \"\"\"\\nAußerdem werden im Kontext von {COMPOUND} folgende Organisationen genannt: {CON_ORG} (C2022)\"\"\"\n",
    "                text += str_org_con\n",
    "            except:\n",
    "                con_org = compounds['ORG_con'].iloc[idx].most_common(1)[0]\n",
    "                #text +=  \"\"\"\\nAußerdem werden im Kontext von {COMPOUND} folgende Organisationen genannt: {CON_ORG} (C2022)\"\"\"\n",
    "                text += str_org_con \n",
    "                \n",
    "        else:\n",
    "            con_org = \" \" \n",
    "            \n",
    "        # if we have at least one person in PERS_con\n",
    "        if compounds['ORG_pro'].isna().iloc[idx] == False:\n",
    "            try:\n",
    "                pro_org =  \", \".join([el[0] for el in compounds['ORG_pro'].iloc[idx].most_common(2)])\n",
    "                #text +=  \"\"\"und {PRO_ORG} (P2022). \"\"\"\n",
    "                text += str_org_pro\n",
    "            except:\n",
    "                pro_org = compounds['ORG_pro'].iloc[idx].most_common(1)[0]\n",
    "                #text +=  \"\"\"und {PRO_ORG} (P2022). \"\"\"   \n",
    "                text += str_org_pro\n",
    "                \n",
    "        else:\n",
    "            pro_org = \" \"\n",
    "            #text += \".\"\n",
    "            \n",
    "        if len(compounds['similar_words'].iloc[idx]) != 0:\n",
    "            #print(len(compounds['similar_words'].iloc[idx]))\n",
    "            similar_words = set([\"Klima\"+x for x in compounds['similar_words'].iloc[idx] if \"Klima\"+x != compound])\n",
    "            similar_words = \", \".join(similar_words)\n",
    "           # similar_words = \", \".join(compounds['similar_words'].iloc[idx])\n",
    "            #text += \"\"\"\\nSiehe auch: {SIMILAR_WORDS}\"\"\"\n",
    "            text += str_simwords\n",
    "            \n",
    "            \n",
    "        else:\n",
    "           # print(\"NO SIMILAR WORDS\")\n",
    "            \n",
    "        full_definition = text.format(COMPOUND= compound, DEFINITION= definition, CON_FREQ= con_freq,\n",
    "                        PRO_FREQ = pro_freq, SENTIMENT= sentiment, CON_PERS = con_pers, PRO_PERS = pro_pers, \n",
    "                        CON_ORG = con_org, PRO_ORG = pro_org, SIMILAR_WORDS= similar_words)\n",
    "        \n",
    "        \n",
    "       # full_definition = persons_text.format(COMPOUND= compound, DEFINITION= definition, CON_FREQ= con_freq,\n",
    "        #                 PRO_FREQ = pro_freq, SENTIMENT= sentiment, CON_PERS = con_pers, PRO_PERS = pro_pers, \n",
    "         #                CON_ORG = con_org, PRO_ORG = pro_org, SIMILAR_WORDS= similar_words)\n",
    "        \n",
    "        #print(full_definition)\n",
    "        #print(\"_\"*50)\n",
    "                \n",
    "        # save to column \"full_definition\"\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
